# SE 350: Operating Systems

Professor: Sebastian Fischmeister.

I used to have nicely written notes, but since the course content is kind of just a collection of facts, I'm just summarizing likely-testable material from the textbook.

Lectures are good for seeing really cool demos and for gaining an appreciation for life. Not good for learning the things that will be on quizzes, which is unfortunately what matters.

### Cool Demos Man

### Ch 3

- 2017-01-24: Multithreading with pthread, passing pointer to local stack, race condition.
- 2017-01-24: pth_burner.c
  - User-level threads with pth.
  - Only one thread works, is never blocked so never hands off control.
- 2017-01-24: pthreads_burner.c: Kernel-level threads. All threads run at a time. I/O works.

### Ch 1: Computers

- Four parts of a computer
  - CPU
  - Main memory
  - I/O modules
  - System bus
- Four general types of instruction
  - Processor-memory
  - Processor-I/O
  - Data processing (math)
  - Control
- Four types of interrupt
  - Exceptions
  - Timer
  - I/O
  - Hardware failure
- Registers
  - Memory address register (MAR):
    - Specifies address in memory for next I/O operation
  - Memory buffer register (MBR):
    - Contains data that needs to be written to, or that was read from, memory
  - I/O address register (I/OAR), I/OBR:
    - Specifies the I/O device and contains data for I/O
- Reentrant procedure: Single copy of code can be shared by many users
- Interrupt processing
  - Processor saves PSW, PC onto a control stack
  - Interrupt handler saves all other registers onto the control stack
  - Interrupt handler restores saved register values
  - Processor restores PSW, PC
- Ways to handle multiple interrupts
  - Disable interrupt in interrupt handlers
  - Prioritize interrupts and push lower-priority handler contexts onto a stack
- Memory hierarchy
  - Register, cache, primary, secondary
  - Locality of reference: Memory references cluster due to loops
    - Spatial locality: Sequentially accessed memory locations tend to be clustered (or themselves sequential)
    - Temporal locality: Recently used memory locations tend to be used again
- Cache memory
  - Cache size
    - Main memory has 2^n words
    - Main memory broken up into 2^n / K blocks of size K
    - Cache has C slots of K words each, C << M
  - Block size
  - Mapping function
  - Replacement algorithm (LRU > FIFO > random)
  - Write policy
  - Hit ratio: Percent of memory accesses that hit the cache
- I/O
  - Programmed I/O: processor polls I/O module
  - Interrupt-driven I/O: I/O module interrupts when ready
  - Direct memory access: DMA module (not necessarily same as I/O module) interacts with memory directly
- Multiprocessor
  - SMP: Similar processors, share memory and I/O

### Ch 2: Operating systems

- Operating system goals (CEA)
  - Convenience -- make computer easier to use
  - Efficiency -- better utilize computer
  - Ability to evolve -- be a good abstraction
- History
  - Serial processing: Scheduling and setup
  - Batch systems
    - Monitor. Programs branch back to it when finishing, and it decides which program to run next.
      - Resident monitor: part that controls sequence of events.
      - Also has utils and functions
    - Job control language: instructions for the monitor
    - Hardware features:
      - Memory protection
      - Timer
      - Privileged instructions
        - One consequence: Monitor retains control of I/O devices
        - Introduces user mode and kernel mode
      - Interrupts
  - Multiprogrammed batch system
    - Multiprogramming / multitasking:
      - Multiple programs at once
      - Switch between them when waiting on I/O
    - Uniprogramming is not multiprogramming
  - Time sharing systems
    - For interactive things
    - Interleave user time in short bursts
    - Time slicing: Using clock interrupts to reassign CPU control
- Process: Program, execution context (process state), and resources
  - Four main causes of errors (FIND):
    - Failed mutual exclusion (of shared resources)
    - Improper synchronization (of I/O)
    - Nondeterminate program operation: programs screwing with other programs' memory space and affecting their operation
    - Deadlocks: programs waiting on each other in a loop
- Memory management:
  - Roles of the OS (PASPL):
    - Process isolation
    - Automatic allocation
    - Support modular programming
    - Protection and access control (for shared memory)
    - Long-term storage
  - Virtual address: Page number and offset
  - Real address / Physical address: Actual address
- Security and protection (ACDA)
  - Availability
  - Confidentiality
  - Data integrity
  - Authenticity
- Resource management factors: (FED)
  - Fairness (Proccesses of the same class have fair and equal access to resources)
  - Efficiency: Maximum throughput, minimize response time
  - Differential responsiveness (More important processes have priority)
- Round-robin: Each process in a queue takes turns
- Modern OS developments
  - Microkernel: Kernel only has essential functions, unlike monolithic kernel
  - Multithreading
  - Symmetric multiprocessing
    - Advantages over uniprocessor (PAIS):
      - Performance -- parallelizable work done faster
      - Availability -- failure of a single processor does not kill everything
      - Incremental growth -- add processors to make things faster
      - Scaling -- sell wider variety of computers by adding more processors
  - Distributed operating systems: Huh-doop like. Lol no.
  - Object-oriented design: Used in development
- Faults
  - Types
    - Permanent (e.g. failed disk)
    - Temporary
      - Transient (e.g. RAM bit flip)
      - Intermittent (e.g. loose connection)
  - Fault tolerance
    - Reliability R(t) is probability system is still up after t time.
    - MTTF: Mean time to failure. Average time system stays up. = Integral 0 to infinity of R(t)
    - MTTR: Mean time to repair. Average time system stays down.
    - Availability: Fraction of time the system is up. = MTTF / (MTTF + MTTR)
  - Types of redundancy (STI)
    - Spatial / physical (e.g. backup server)
    - Temporal (e.g. retransmission)
    - Information (e.g. RAID)
  - Fault tolerance in OSes (PCVC)
    - Process isolation
    - Concurrency controls
    - Virtual machines
    - Checkpoints and rollbacks
- Multiprocessor / Multicore
  - Considerations (SSSRM):
    - Simultaneous concurrent processes / threads
    - Scheduling
    - Synchronization
    - Reliability / fault tolerance
    - Memory management
  - Grand Central Dispatch: Thread pool of lambdas
- Windows
  - Windows NT:
    - Executive: Memory management, processes, threads, security, I/O, IPC. Threaded.
    - Kernel: Processs execution, scheduling, switching, exceptions and interrupts, multiprocessor. Not threaded.
    - HAL: Maps generic hardware commands to platform-specific ones
    - Device drivers: Extend Executive.
- Unix
- Modern Unix
- Linux
  - Loadable modules
  - Signals (SIGKILL, SIGTERM, SIGINT, SIGSEGV, SIGTRAP)
  - Syscalls
- Android
  - Activities
  - Alarms

### Ch 3: Processes

- Task

- Process control block: keeps track of a process.
  - Identifier
  - State
  - Priority
  - PC
  - Memory pointers
  - Execution context
  - I/O status
  - Accounting
- Scheduler picks which process to run next
- Dispatcher switches the processor from one process to another
- Process states
  - Two-state model: (Enter) -> Running -> (Dispatch) / <- (Pause) Not running (Exit) ->
  - Five-state model:
    - New (admit -> Ready)
      - Still in disk. Not yet committed to running in memory.
    - Ready (dispatch -> Running)
    - Running (time-out -> Ready, event wait -> Blocked, release -> Exit)
    - Blocked (event occurs -> Ready)
    - Exit
  - Seven-state model:
    - Split Ready and Blocked out to produce Ready / Suspend and Blocked / Suspend
    - Allows for better swapping processes back in
  - When one process spawns another, the spawned process is a child and the spawner is a parent
- Trace: Sequence of instructions
- Swapping: Use disk to store a process
- Process image: The collection of all data associated with a process, including its PCB, stack, program code, and data
- Modes:
  - User mode: Least privileged.
  - System mode / Control mode / Kernel mode: Most privileged.
  - Determined by bit in PSW.
- Reasons for process creation:
  - New batch job
  - Interactive log-on
  - Created by OS to service a user
  - Spawned
- Process creation process:
  - Assign PID.
  - Allocate space for stack, heap, PCB
  - Initialize PCB
  - Set up linkages
  - Set up other data structures
- When to switch processes:
  - Clock interrupt
  - I/O interrupt
  - Memory fault (addr outside of virtual memory range)
- Ways to interrupt process execution:
  - Interrupt
  - Trap: Using an error or exception to stop process at a certain point.
  - Supervisor call: system call
- Interrupt handling:
  - PC = interrupt handler (hardware)
  - Switch to kernel mode (hardware)
- Execution of the OS
  - Nonprocess kernel
  - Execute within user processes

### Ch 4: Threads

- Multithreading
  - Processes have multiple threads
  - Threads run independently
  - Threads share memory space
  - Advantages of threads:
    - Creating is faster
    - Terminating is faster
    - Switching is faster
    - Communication is faster: no IPC or kernel involved; just share RAM!
  - Uses of threads:
    - Foreground / Background work
    - Asynchronous processing
    - Making programs faster
    - Making programs more modular
  - Thread state operations:
    - Spawn: New thread put on ready queue
    - Block: Wait for event
    - Unblock: Event happens
    - Finish: Deallocate stuff
- Types of threads
  - User-level threads (ULT): Application manages threads
  - Kernel-level threads (KLT): Kernel manages threads
    - e.g. Windows
  - Advantages of ULT:
    - No mode switches
    - Application-specific scheduling
    - OS-agnostic
  - Advantages of KLT:
    - Blocking system calls don't block whole process
    - Allows for multiprocessing
  - Kernel manages threads
  - Jacketing: Makes application-level threads better by converting a blocking syscall to a nonblocking syscall using application-level "blocking polling"
  - Thread-process ratios:
    - 1:1: e.g. UNIX
    - M:1: Processes have multiple threads e.g. Windows, Solaris, Linux
    - 1:M: Threads move between processes e.g. Ra (Clouds), Emerald
    - M:N: TRIX
- Valve
  - Coarse threading: Modules are separate threads. e.g. AI/Physics
  - Fine-grained threading: Loops are threads. Parallel loops.
  - Hybrid threading: Bit of both.
- I skipped over a billion OS case studies

### Ch 5: Concurrency: Mutex and Sync

- Power words
  - busy waiting
  - coroutine
  - spin waiting

- Key concurrency words
  - Atomic: a sequence of instructions that must appear to happen all at once
  - Critical section: section of code that requires access to shared resources
  - Deadlock: multiple processes are blocked because each waits for the other
  - Livelock: multiple processes mutually do nothing while looking busy
  - Mutual exclusion: only one process can access a shared resource at a time
  - Race condition: result depends on order of thread execution
  - Starvation: process never scheduled due to greedy others even though it can progress

- Process interaction
  - Unaware of each other.
    - Not intended to work together..
    - Competition.
  - Indirectly aware of each other.
    - Know each other's PIDs.
    - Cooperation by sharing.
  - Directly aware of each other.
    - Communicate by PID and work jointly on an activity.
    - Cooperation by communication.
- Mutual exclusion
  - Software approach: Dekker's algorithm in some library or OS/language support.
  - Hardware support
    - Disable interrupts to make a critical section atomic
      - Does not work if there is multiprocessing, multicore or multi-processor.
    - Hardware-level instructions to achieve mutual exclusion:
      - compare&swap: replace a value only if it is equal to some test value
      - exchange: swap register and memory location atomically
- Semaphore (counting / general)
  - Shared variable which can be atomically initialized, decremented, or incremented
  - Initialize: inits with non-negative value
  - semWait: decrements. blocks the caller if value becomes negative
    - No way to determine if it will block!
  - semSignal: increments. unblocks a process if value is still at most 0
    - No way to determine if it will wake up another process!
  - Binary semaphore: values 0 or 1
  - Mutex: binary semaphore where the same process needs to take and release lock.
  - Strong and weak
    - **Strong** semaphore: FIFO blocked queue. Free from starvation.
    - **Weak** semaphore: No order to pop from blocked queue. May starve.
  - Producer / consumer problem
    - Semaphores are hard. Get used to them.
    - Use local variables to prevent preemption from fucking shit up
- Monitor
  - A module. Only the monitor can access local vars.
  - Controlled entry: Processes invoke one of the monitor procedures to enter
  - Mutex: Only one process at a time
  - e.g. in Java, synchronized makes your thing a sorta-monitor
  - cwait(c): Suspend until condition c is signalled
  - csignal(c): Signals condition c
  - Equivalent in power to semaphores. Can implement monitor with semaphores.
- Message passing
  - send(destination, message): 
  - receive(source, message): 
  - Possible blocking semantics:
    - Blocking send, blocking receive. (rendezvous, sendr/recvr must coordinator)
    - Nonblocking send, blocking receive. (most useful, usually)
    - Nonblocking send, nonblocking receive. (Kafka)
  - Possible addressing semantics:
    - Direct addressing
      - Sender: Destination is specific
      - Receiver: Source is either specific or unspecified
    - Indirect addressing
      - Messages sent/received to/from **mailboxes**
      - Allows for (one/many)-to-(one/many)
  - Messages contain:
    - Type
    - Destination ID
    - (Source ID)
    - Message length
    - Control information
    - Contents
- Reader/Writer problem
  - n readers can read, 1 writer can write
  - but when the writer writes, no readers can read
  - Readers have priority: Solve with 2 semaphores
  - Writers have priortiy: Solve with 5 semaphores, is harder
  - Either readers or writers can have priority
    - Can solve with semaphores

### Ch 6: Concurrency: Deadlock and Starvation

- Deadlock
  - When two processes mutually block forever due to a conflict in resources.
  - Resource types:
    - Reusable:
      - Processors, I/O channels, memory
      - Deadlock happens when two processes need two resources but each only gets one and waits on the other
    - Consumable:
      - Interrupts, signals, messages, I/O buffer informations
  - Main problem: hold-and-wait
- Resource allocation graph
  - Way to represent which processes are holding which resources
  - Resources are squares with dots = the number of resources available
  - Processes are circles that the dots point to
- Avoiding deadlocks
  - Is hard.
  - Prevention: Static.
  - Avoidance: At runtime.
  - Conditions for deadlocks:
    - Mutual exclusion: Only one thing can access at a time
      - Cannot remove for resources which require it
    - Hold and wait: Processes holding resources while waiting for others
      - Can remove by making processes request all needed resources at once
      - Not practical
    - No preemption: Processes cannot forcibly lose resources.
      - Can remove by making processes give up their other resources if one request fails
      - Requires resources whose state can be saved and restored quickly
    - Circular wait: Processes wait for resources in a cycle
      - Can remove by ordering resources.
      - Inefficient
  - The Banker's Algorithm:
    - By Edsger Dijkstra
    - Requires that:
      - State maximum resource requirements for each process upfront
      - Independent processes
      - Fixed number of resources
      - No process exits while holding resources
    - Define m-vectors R, V, n√óm-matrices C, A
      - R(i): Total amount of resource i
      - V(i): Available amount of resource i
      - C(i,j): Requirement of process i for resource j
      - A(i,j): Current allocation to process i of resource j
    - Safe state:
      - At least one seqence of resource allocations does not result in a deadlock
    - Unsafe state: not safe
- Handling deadlocks
  - Abort them all
  - Abort them one by one until the deadlock is fixed
  - Reverse time for deadlocked processes and restart them
  - Preempt resources
ERRATA p278: (0 0 0 0 1) should be (0 0 0 1 1).
- Dining philosopher's problem
  - Five plates, five forks. Philosophers need both forks to eat spaghetti.
  - Deadlock if each philosopher takes one fork.
  - Solutions:
    - Eating lock.
    - Limiting number of eating philosophers.
- UNIX concurrency
  - Pipes: producer/consumer, fifos
  - Messages
  - Shared memory
  - Semaphores
  - Signals (SIGKILL, SIGSEGV, SIGINT, SIGTERM etc)

### Ch 7: Memory management

Power Words:
- absolute loading
- buddy system
- compaction
- dynamic linking
- dynamic partitioning
- dynamic run-time loading
- external fragmentation
- fixed partitioning
- frame
- internal fragmentation
- linkage editor
- linking
- loading
- logical address
- logical organization
- memory management
- page
- page table
- paging
- partitioning
- physical address
- physical organization
- protection
- relative address
- relocatable loading
- relocationg
- segment
- segmentation
- sharing
- address randomization

- Memory management requirements
  - Relocation: Moving process data around.
  - Protection: Prevent segfaults.
  - Sharing: Common libraries like libc and such.
  - Logical organization: Independent memory modules with independent R/W protection.
  - Physical organization: Main memory and secondary memory.
- Memory partitioning
  - Fixed partitioning
    - Simple. Fixed-sized blocks. Can have multiple sizes.
    - Awkward sizes
    - Very inefficient.
    - Internal fragmentation: unused space within a block.
  - Dynamic partitioning
    - Allocate just enough blocks for each process.
    - Efficient at first
    - But when processes end, they leave behind an awkward-sized hole
    - Need to do compaction (inefficient)
    - External fragmentation: unused space outside of blocks.
  - Simple paging
  - Simple segmentation
  - Virtual memory paging
  - Virtual memory segmentation
