# SE 350: Operating Systems

Professor: Sebastian Fischmeister.

I used to have nicely written notes, but since the course content is kind of just a collection of facts, I'm just summarizing likely-testable material from the textbook.

Lectures are good for seeing cool demos with fsize 35 and emacs and for gaining an appreciation for life. Not good for learning the things that will be on quizzes, which is unfortunately what matters.

### Cool Demos Man

### Ch 3

- 2017-01-24: Multithreading with pthread, passing pointer to local stack, race condition.
- 2017-01-24: pth_burner.c
  - User-level threads with pth.
  - Only one thread works, is never blocked so never hands off control.
- 2017-01-24: pthreads_burner.c: Kernel-level threads. All threads run at a time. I/O works.

### Ch 1: Computers

- Four parts of a computer
  - CPU
  - Main memory
  - I/O modules
  - System bus
- Four general types of instruction
  - Processor-memory
  - Processor-I/O
  - Data processing (math)
  - Control
- Four types of interrupt
  - Exceptions
  - Timer
  - I/O
  - Hardware failure
- Registers
  - Memory address register (MAR):
    - Specifies address in memory for next I/O operation
  - Memory buffer register (MBR):
    - Contains data that needs to be written to, or that was read from, memory
  - I/O address register (I/OAR), I/OBR:
    - Specifies the I/O device and contains data for I/O
- Reentrant procedure: Single copy of code can be shared by many users
- Interrupt processing
  - Processor saves PSW, PC onto a control stack
  - Interrupt handler saves all other registers onto the control stack
  - Interrupt handler restores saved register values
  - Processor restores PSW, PC
- Ways to handle multiple interrupts
  - Disable interrupt in interrupt handlers
  - Prioritize interrupts and push lower-priority handler contexts onto a stack
- Memory hierarchy
  - Register, cache, primary, secondary
  - Locality of reference: Memory references cluster due to loops
    - Spatial locality: Sequentially accessed memory locations tend to be clustered (or themselves sequential)
    - Temporal locality: Recently used memory locations tend to be used again
- Cache memory
  - Cache size
    - Main memory has 2^n words
    - Main memory broken up into 2^n / K blocks of size K
    - Cache has C slots of K words each, C << M
  - Block size
  - Mapping function
  - Replacement algorithm (LRU > FIFO > random)
  - Write policy
  - Hit ratio: Percent of memory accesses that hit the cache
- I/O
  - Programmed I/O: processor polls I/O module
  - Interrupt-driven I/O: I/O module interrupts when ready
  - Direct memory access: DMA module (not necessarily same as I/O module) interacts with memory directly
- Multiprocessor
  - SMP: Similar processors, share memory and I/O

### Ch 2: Operating systems

- Operating system goals (ACE)
  - Ability to evolve -- be a good abstraction
  - Convenience -- make computer easier to use
  - Efficiency -- better utilize computer
- History
  - Serial processing: Scheduling and setup
  - Batch systems
    - Monitor. Programs branch back to it when finishing, and it decides which program to run next.
      - Resident monitor: part that controls sequence of events.
      - Also has utils and functions
    - Job control language: instructions for the monitor
    - Hardware features:
      - Memory protection
      - Timer
      - Privileged instructions
        - One consequence: Monitor retains control of I/O devices
        - Introduces user mode and kernel mode
      - Interrupts
  - Multiprogrammed batch system
    - Multiprogramming / multitasking:
      - Multiple programs at once
      - Switch between them when waiting on I/O
    - Uniprogramming is not multiprogramming
  - Time sharing systems
    - For interactive things
    - Interleave user time in short bursts
    - Time slicing: Using clock interrupts to reassign CPU control
- Process: Program, execution context (process state), and resources
  - Four main causes of errors (FIND):
    - Failed mutual exclusion (of shared resources)
    - Improper synchronization (of I/O)
    - Nondeterminate program operation: programs screwing with other programs' memory space and affecting their operation
    - Deadlocks: programs waiting on each other in a loop
  - Steps in context switch:
    - Save context
    - Update PCB of currently running process (incl. change state to not running)
    - Move PCB to appropriate queue
    - Schedule (select another process)
    - Update PCB of selected process (incl. change state to running)
    - Update memory management data structures
    - Restore context
- Memory management:
  - Roles of the OS (APPLS):
    - Automatic allocation
    - Process isolation
    - Protection and access control (for shared memory)
    - Long-term storage
    - Support modular programming
  - Virtual address: Page number and offset
  - Real address / Physical address: Actual address
- Security and protection (ACDA)
  - Availability
  - Confidentiality
  - Data integrity
  - Authenticity
- Resource management factors: (FED)
  - Fairness (Proccesses of the same class have fair and equal access to resources)
  - Efficiency: Maximum throughput, minimize response time
  - Differential responsiveness (More important processes have priority)
- Round-robin: Each process in a queue takes turns
- Modern OS developments
  - Microkernel: Kernel only has essential functions, unlike monolithic kernel
  - Multithreading
  - Symmetric multiprocessing
    - Advantages over uniprocessor (PAIS):
      - Performance -- parallelizable work done faster
      - Availability -- failure of a single processor does not kill everything
      - Incremental growth -- add processors to make things faster
      - Scaling -- sell wider variety of computers by adding more processors
  - Distributed operating systems: Huh-doop like. Lol no.
  - Object-oriented design: Used in development
- Faults
  - Types
    - Permanent (e.g. failed disk)
    - Temporary
      - Transient (e.g. RAM bit flip)
      - Intermittent (e.g. loose connection)
  - Fault tolerance
    - Reliability R(t) is probability system is still up after t time.
    - MTTF: Mean time to failure. Average time system stays up. = Integral 0 to infinity of R(t)
    - MTTR: Mean time to repair. Average time system stays down.
    - Availability: Fraction of time the system is up. = MTTF / (MTTF + MTTR)
  - Types of redundancy (STI)
    - Spatial / physical (e.g. backup server)
    - Temporal (e.g. retransmission)
    - Information (e.g. RAID)
  - Fault tolerance in OSes (PCVC)
    - Process isolation
    - Concurrency controls
    - Virtual machines
    - Checkpoints and rollbacks
- Multiprocessor / Multicore
  - Considerations (SSSRM):
    - Simultaneous concurrent processes / threads
    - Scheduling
    - Synchronization
    - Reliability / fault tolerance
    - Memory management
  - Grand Central Dispatch: Thread pool of lambdas
- Windows
  - Windows NT:
    - Executive: Memory management, processes, threads, security, I/O, IPC. Threaded.
    - Kernel: Processs execution, scheduling, switching, exceptions and interrupts, multiprocessor. Not threaded.
    - HAL: Maps generic hardware commands to platform-specific ones
    - Device drivers: Extend Executive.
- Unix
- Modern Unix
- Linux
  - Loadable modules
  - Signals (SIGKILL, SIGTERM, SIGINT, SIGSEGV, SIGTRAP)
  - Syscalls
- Android
  - Activities
  - Alarms

### Ch 3: Processes

- Control tables
  - **Process control block**: keeps track of a process.
    - Process identification
      - PID
    - Processor state
      - PC
    - Process control information
      - State
      - Priority
      - Memory pointers
      - Execution context
      - I/O status
    - Accounting?
  - **I/O tables** track I/O device status, source, destination
  - **File tables** track file location, status, etc.
- **Scheduler** picks which process to run next
- **Dispatcher** switches the processor from one process to another
- Process states
  - Two-state model: (Enter) -> Running -> (Dispatch) / <- (Pause) Not running (Exit) ->
  - Five-state model:
    - New (admit -> Ready)
      - Still in disk. Not yet committed to running in memory.
    - Ready (dispatch -> Running)
    - Running (time-out -> Ready, event wait -> Blocked, release -> Exit)
    - Blocked (event occurs -> Ready)
    - Exit
  - Seven-state model:
    - Split Ready and Blocked out to produce Ready / Suspend and Blocked / Suspend
    - Allows for better swapping processes back in
  - When one process spawns another, the spawned process is a child and the spawner is a parent
- Trace: Sequence of instructions
- Swapping: Use disk to store a process
- Process image: The collection of all data associated with a process, including its PCB, stack, program code, and data
- Modes:
  - User mode: Least privileged.
  - System mode / Control mode / Kernel mode: Most privileged.
  - Determined by bit in PSW.
- Reasons for process creation:
  - New batch job
  - Interactive log-on
  - Created by OS to service a user
  - Spawned
- Process creation process:
  - Assign PID.
  - Allocate space for stack, heap, PCB
  - Initialize PCB
  - Set up linkages
  - Set up other data structures
- When to switch processes:
  - Clock interrupt
  - I/O interrupt
  - Memory fault (addr outside of virtual memory range)
- Ways to interrupt process execution:
  - Interrupt
  - Trap: Using an error or exception to stop process at a certain point.
  - Supervisor call: system call
- Interrupt handling:
  - PC = interrupt handler (hardware)
  - Switch to kernel mode (hardware)
- Execution of the OS
  - Nonprocess kernel
  - Execute within user processes

### Ch 4: Threads

- Multithreading
  - Processes have multiple threads
  - Threads run independently
  - Threads share memory space
  - Advantages of threads:
    - Creating is faster
    - Terminating is faster
    - Switching is faster
    - Communication is faster: no IPC or kernel involved; just share RAM!
  - Uses of threads:
    - Foreground / Background work
    - Asynchronous processing
    - Making programs faster
    - Making programs more modular
  - Thread state operations:
    - Spawn: New thread put on ready queue
    - Block: Wait for event
    - Unblock: Event happens
    - Finish: Deallocate stuff
- Types of threads
  - User-level threads (ULT): Application manages threads
  - Kernel-level threads (KLT): Kernel manages threads
    - e.g. Windows
  - Advantages of ULT:
    - No mode switches
    - Application-specific scheduling
    - OS-agnostic
  - Advantages of KLT:
    - Blocking system calls don't block whole process
    - Allows for multiprocessing
- Jacketing: User-level wrapper around blocking syscall that makes it non-blocking by checking to see if resource is available first
- Thread-process ratios:
  - 1:1: e.g. UNIX
  - M:1: Processes have multiple threads e.g. Windows, Solaris, Linux
  - 1:M: Threads move between processes e.g. Ra (Clouds), Emerald
  - M:N: TRIX
- Multicore
  - Speedup is 1 / (1 - f + f/N)
    - f: amount of code that is parallel
    - N: number of cores
- Case stughdies
  - Valve
    - Coarse threading: Modules are separate threads. e.g. AI/Physics
    - Fine-grained threading: Loops are threads. Parallel loops.
    - Hybrid threading: Bit of both.
  - Windows
    - Job objects manage groups of processes
    - Fibers are user-scheduled units of execution
    - User-mode scheduling (UMS) allows apps to schedule their own threads
    - Metro: Suspending an app saves its state
    - Process object has:
      - Access token
      - Handle table for resources and I/O
      - Virtual address linked list
    - Process state model:
      - Ready: Can run
      - Standby: Chosen to run next
      - Running
      - Waiting: Blocked on event, or voluntarily waiting
      - Transition: Ready to run but resources not available
      - Terminated
  - Solaris
    - Processes
    - User-level threads
    - Lightweight processes: maps ULTs to kernel threads
    - Kernel threads
    - Process state model:
      - IDL
      - PINNED
      - RUN
      - ONPROC: preempted
      - SLEEP: blocked
      - STOP: stopped
      - ZOMBIE: thread has terminated
      - FREE: resources released
  - Linux
    - PCB is task_struct
    - Process state model:
      - Stopped
      - Running: Ready / Executing
      - Interruptible: Normal blocked, will handle signals
      - Uninterruptible: Special blocked, waiting on hardware and will not handle signals
      - Zombie
  - Android
    - Four types of components
      - Activities: foreground screens
      - Services: background tasks
      - Content providers: data
      - Broadcast receivers: warnings and stuff
    - Dedicated virtual machine for each application (sandboxing)
  - macOS
    - G r a n d   C e n t r a l   D i s p a t c h

### Ch 5: Concurrency: Mutual exclusion and synrhonization

- Key words
  - Atomic: a sequence of instructions that happens indivisibly
  - Critical section: code section that requires access to shared resources
  - Deadlock: multiple processes are blocked because each waits for the other
  - Livelock: multiple processes look busy but mutually do nothing useful
  - Mutual exclusion: only one process can access a shared resource at a time
  - Race condition: result changes depending on order of thread execution
  - Starvation: process never scheduled even though it can progress
- Process interaction
  - Competition: Unaware of each other. Not intended to work together.
  - Cooperation by sharing: Indirectly aware of each other by shared object.
  - Cooperation by communication: Directly aware of each other.
    - Communicate by PID and work jointly on an activity.
- Mutual exclusion
  - Software support
    - Dekker's algorithm in some library or OS/language support.
  - Hardware support
    - Disable interrupts to make a critical section atomic
      - Does not work if there is multiprocessing, multicore or multiprocessor.
    - Hardware-level atomic instructions to achieve mutual exclusion
      - compare&swap: replace a value iff it is equal to some test value
      - exchange: swap register and memory location atomically
  - Advantages of instructions:
    - Applicable to any number of processes
    - Simple, easy to verify
    - Multiple critical sections
  - Disadvantages of instructions:
    - Busy waiting
    - Starvation can happen
    - Deadlock can happen if higher priority process gets stuck in the busy loop
- Semaphore
  - Shared variable which can be atomically initialized, decremented, or incremented.
  - Operations
    - initialize: inits with non-negative value
    - semWait: decrements. blocks the caller if value becomes negative
    - semSignal: increments. unblocks a process if value is still now at most 0
  - No way to tell if semWait or semSignal will block/unblock a process!
  - Binary semaphore: values restricted to 0 or 1
    - Mutual exclusion lock (mutex):
      - Binary semaphore where the same process needs to take and release lock.
  - Strong semaphore: FIFO blocked queue. Free from starvation.
  - Weak semaphore: No order to pop from blocked queue. May starve.
  - Producer / consumer problem
    - Semaphores are hard. Get used to them.
    - Use local variables to prevent preemption from fucking shit up
- Monitor
  - A module with private local data. Only one process can be in the monitor at a time.
  - e.g. in Java, synchronized makes your thing a monitor
  - Condition variables within the monitor:
    - cwait(c): Suspend until condition c is signalled
    - csignal(c): Signals condition c
  - Equivalent in power to semaphores.
  - Hoare monitor: csignal blocks if there are no processes
  - Lampson/Redell monitor: use cnotify instead, which is like a non-blocking csignal
- Message passing
  - API:
    - send(destination, message)
    - receive(source, message)
  - Possible blocking semantics:
    - Blocking send, blocking receive.
      - aka rendezvous
      - Sender and receiver must coordinate
    - Nonblocking send, blocking receive.
      - Most useful, usually
    - Nonblocking send, nonblocking receive. (Kafka)
  - Possible addressing semantics:
    - Direct addressing
      - Sender: Destination is specific
      - Receiver: Source can be specific or unspecified
      - Explicit: Receiver must specify the sender
      - Implicit: Receiver is told about the sender
    - Indirect addressing
      - Messages sent/received to/from ports/mailboxes (queues)
      - Allows for (one/many)-to-(one/many)
        - Many-to-one: Port (e.g. 80)
        - Others: Mailbox
  - Messages contain:
    - Type
    - Destination ID
    - Source ID (optional)
    - Message length
    - Control information
    - Contents
  - Messages can be retrieved FIFO, by priority, or by receiver choice
- Reader/Writer problem
  - n readers can read, 1 writer can write
  - but when the writer writes, no readers can read
  - Readers have priority: Solve with 2 semaphores
  - Writers have priortiy: Solve with 5 semaphores, is harder
  - Either readers or writers can have priority
    - Can solve with semaphores

### Ch 6: Concurrency: Deadlock and Starvation

- Joint progress diagram
  - Way to visualize how deadlocks can form
  - Each axis is progress along one process
  - Resource get/release represented as lines
  - Fatal region: where deadlock is inevitable / has already happened
- Deadlock
  - When two processes mutually block forever due to a conflict in resources.
  - Resource types
    - Reusable
      - Processors, I/O channels, memory
      - Deadlock happens when processes wait on resources in a cycle
    - Consumable
      - Interrupts, signals, messages, I/O buffer information
      - Deadlock happens when multiple processes block on receive/send calls.
- Resource allocation graph
  - Way to represent which processes are holding which resources
  - Resources are squares with dots = the number of resources available
  - Processes are circles that the dots point to
- Conditions for deadlock:
  - Possibility:
    - Mutual exclusion: Only one thing can access at a time
    - Hold and wait: Processes holding resources while waiting for others
    - No preemption: Processes cannot forcibly lose resources.
  - "Actually Happening":
    - Circular wait: Processes wait for resources in a cycle
- Deadlock Prevention: Static. Prevent one of the three Conditions. Surefire, but usually not good.
  - Mutual exclusion: usually not an option
  - Hold and wait: Allocate everything upfront. Often not possible, inefficient.
  - No preemption: Allow resources to be forcibly removed (triggered by process itself asking for more or another process asking for more). Sometimes doable.
  - Circular wait: Order resources, and force allocation in that order.
- Deadlock Avoidance: At runtime. Can use The Banker's Algorithm.
- The Banker's Algorithm:
  - By Dijkstra
  - Requires that:
    - Processes state maximum resource requirements up front
    - Independent processes
    - Fixed number of resources
    - No process exits while holding resources
  - Define:
    - n processes, m resources
    - m-vectors R (Resource), V (Available)
    - nÃ—m-matrices C (Claim), A (Allocation)
    - C-A is amount of resources needed to ensure completion
    - R(i): Total amount of resource i
    - V(i): Available amount of resource i
    - C(i,j): Requirement of process i for resource j
    - A(i,j): Current allocation to process i of resource j
  - Safe state: At least one seqence of resource allocations does not result in a deadlock
  - To determine safety, repeatedly run a process to completion if possible and see if all processes can finish. Greedy works.
  - Practice this on paper!
- Deadlock Detection: 
  - Like the Banker's Algorithm but you're given C-A directly instead of having to compute it
  - Recovery
    - Abort them all. (most common solution)
    - Abort them one by one until the deadlock is fixed
    - Reverse time for deadlocked processes and restart them (retoast)
    - Preempt resources
- Dining philosopher's problem
  - Five plates, five forks. Philosophers need both forks to eat spaghetti.
  - Deadlock if each philosopher takes one fork.
  - Solutions:
    - Eating lock.
    - Limiting number of eating philosophers.
- UNIX concurrency
  - Pipes: FIFO circular buffer that allows reading and writing. Can be named or unnamed.
  - Message passing: msgsnd, msgrcv.
  - Shared memory: shm
  - Semaphores: generalized semaphores with four ops depending on sem_op
    - sem_op positive: like semSignal
    - sem_op == 0: block until semaphore == 0
    - sem_op negative until abs(semaphore value): adds sem_op to the semaphore value
    - sem_op really negative: block until semaphore increases
  - Signals (SIGKILL, SIGSEGV, SIGINT, SIGTERM etc)
- TODO(final): 6.8 through 6.13

### Ch 7: Memory management

Power Words:
- address randomization

- Memory management requirements
  - Relocation: Data can be moved around physically.
  - Protection: Prevent segfaults dynamically.
  - Sharing: Common data can be shared. (e.g. standard libraries)
  - Logical organization: There are independent memory "modules" with independent R/W protection.
  - Physical organization: Data can be moved between RAM and disk.
- Address types
  - Physical address: Actual absolute location on memory device.
  - Logical address: Physical address is some function of it.
  - Relative address: Physical address is offset from it.
    - Adder and comparator used to translate and bound check
    - Base register and bounds register are relevant
- Types of fragmentation
  - **Internal fragmentation**: unused space within a block.
  - **External fragmentation**: unused space outside of blocks.
- Fixed partitioning
  - Fixed-sized memory blocks. Can have multiple fixed sizes.
  - Much internal fragmentation.
- Dynamic partitioning
  - Memory blocks just the right size for the process.
  - Much external fragmentation. Use **compaction** to fix temporarily.
  - Placement algorithms:
    - Best-fit: Choose smallest block possible
    - First-fit: Choose first block that fits
    - Next-fit: Choose first block that fits starting from last placement
  - Buddy system: Memory is a binary space partition
- Paging
  - Divide memory into equal fixed-sized page frames.
  - Assign a number of pages to each process.
  - Address translation:
    - Logical address = page ++ offset
    - Physical address = frame ++ offset
    - Page table is used to obtain frame from page
  - No external fragmentation, a bit of internal fragmentation
- Simple segmentation
  - Program is divided into dynamic-length segments.
  - No internal fragmentation. A bit of external fragmentation.
  - Address translation:
    - Logical address = segment number ++ offset
    - Physical address = physical start ++ offset
    - Segment table is used to obtain physical start from segment number
- Loading
  - Absolute loading
    - Always loads module to the same physical address
  - Relocatable loading
    - Compiler produces relative addresses.
    - Loader translates them to physical addresses.
  - Dynamic run-time loading
    - Defer calculation of physical addresses until run time
    - Done by hardware for efficiency
- Linking
  - Linkage editor: linker that produces a relocatable load module
  - Dynamic linker:
    - Load time dynamic linking: addresses resolved at load time
    - Run-time dynamic linking: at run time

### Ch 8: Virtual memory

- Virtual memory: Use disk to store chunks of memory
  - Unused memory is written to disk.
  - Now-needed memory is read from disk.
- Requires:
  - Logical addresses
  - Run-time address translation
- Allows program to be running without entire program in memory!
- **Page faults** are triggered when CPU tries to access nonresident data that block the process until data is loaded into memory
- **Resident set**: How much a process is actually using
- **Thrashing**: When the computer uses too much time swapping rather than execution instructions
- **Principle of locality**: Stuff you need in the future is close to stuff you needed in the past.
- Virtual memory paging
  - Page table bits
    - Present (P): Whether page is actually in main memory
    - Modify (M): Whether page has been changed since last loaded into main memory
  - Hierarchial page table
    - 4KB root page table (in memory)
    - --> 4MB user page table (on disk)
    - --> 4GB user address space
  - Inverted page table: Hash map from page number to PID, with chaining
  - Translation lookaside buffer
    - Hardware cache from page number to physical address
    - Uses an associative mapping (lots of brute force circuits)
  - Page size
    - Very small: Large number of pages is available and page fault rate is low
    - Medium: More page faults
    - Very large: Page covers the entire process and page fault rate is low
- Virtual memory segmentation
  - Segment table also needs Present and Modify bits.
  - Possibly other bits too for protection or sharing.
- Combined paging and segmentation
  - Segments built on top of pages
- Operating system decisions
  - Fetch policy: When are pages loaded?
    - Demand paging: Page loaded only when reference is made
    - Prepaging: Pages loaded in advance
  - Placement policy: Best/first/next fit (discussed earlier)
  - Replacement policy: Which page to replace?
    - Frame locking: Page marked as cannot be replaced
    - Basic algorithms
      - Optimal: Replace page that won't be used for the longest (requries precognition)
      - LRU: Replace page that hasn't been used for the longest
      - FIFO: Replace page that was loaded earliest
      - Clock: Approximate LRU using a use bit
        - When page is replaced, OS scans until it finds a frame with use bit 0. Otherwise sets it to 0.
    - Page buffering
      - Have free page list, modified page list
  - Resident set management:
    - How many page frames to give to each process?
    - Which pages should we even consider for replacement?
    - Fixed allocation:
      - Each process has fixed number of frames
    - Variable allocation:
      - Process can gain or lose page frames
    - Replacement scope
      - Local: Only chooses among resident pages of faulting process
      - Global: Considers all pages in memory
    - Fixed allocation, global scope is not possible
    - Page fault frequency algorithm
  - Cleaning policy
    - Demand cleaning: Page written out only when selected for replacement
    - Precleaning: Pages written out in batches
  - Load control
    - Don't thrash
    - Process suspension criteria:
      - Lowest priority
      - Faulting
      - Last activated
      - Smallest resident set
      - Largest process
      - Largest remaining execution window
- Unix and Solaris
  - Page table: one per process
  - Disk block descriptor: one for each page of a process
  - Page frame data table: describes each page frame
  - Swap-use table: one for each swap device
  - Two-handed clock replacement algorithm. Scanrate and handspread
- Linux
  - Three level page table structure
    - Page directory -> Page middle directory -> Page table -> Page frame
  - Page replacement: Split LRU
  - Memory allocation is hard, uses slab allocation
- Windows
  - Virtual address space:
    - 0x00000000 to 0x0000ffff: reserved to help catch null pointers
    - 0x00010000 to 0x7ffeffff: user address space
    - 0x7fff0000 to 0x7fffffff: guard page to check out of bounds
    - 0x80000000 to 0xffffffff: system address space, used by executive, kernel, HAL, drivers
  - Paging:
    - Three types of regions
      - Available: addresses not currently used by the process
      - Reserved: set aside for a process
      - Committed: committed
- Android
  - ASHMem: anonymous shared memory
  - Pmem: physically contiguous virtual memory
  - Low Memory Killer

### Ch 9: Uniprocessor scheduling

- Types of scheduling
  - Long-term: Decision to add to pool of processes
  - Medium-term: Decision to put process in main memory
  - Short-term: Decision to execute process
  - I/O: Decision of which process's I/O should be handled
- Short term scheduling criteria
  - User oriented, performance related
    - Turnaround time: time between process submission and completion
    - Response time: time between request submission and receiving response
    - Deadlines
  - User oriented, other
    - Predictability: Running time should have low variance
  - System oriented, performance related
    - Throughput: Maximize processes completed / time
    - Processor utilization: Maximize processor usage
  - System oriented, other
    - Fairness: No process should starve
    - Enforce priorities
    - Balance resources
- Decision modes
  - Preemptive: Can override current process
  - Non-preemptive: Does not override current process
- Selection function: picks next process to execute in short term scheduling
  - FCFS (First come first serve)
  - Round robin
  - SPN (Shortest process next)
  - SRT (Shortest remaining time)
  - HRRN (Highest response ratio next)
    - R = (w + s) / s, where w is time spent waiting for processor and s is expected service time
  - Feedback: Penalize long-running jobs
    - Multi-level feedback
- Unix
  - One-second preemption

### Ch 10: Multiprocessor, multicore, and real-time scheduling

- Types of coupling
  - Loosely coupled - distributed systems
  - Functionally specialized - I/O module, GPUs
  - Tightly coupled - multiple cores

- The five types of synchronization granularity are:
  - Fine
  - Medium
  - Coarse
  - Very coarse
  - Independent

- The four types of thread scheduling are:
  - Load sharing
  - Gang scheduling
  - Dedicated processor assignment
  - Dynamic scheduling

- The three disadvantages of load sharing are:
  - Central queue needs mutual exclusion
  - Cache misses from "preemptive threads" switching processors
  - A single application's threads usually won't all be on the processor, which might matter

- The difference between soft real time and hard real time tasks are:
  - Soft real time - Response time is an average. Not too important.
  - Hard real time - Response time is a strict upper bound. Safety critical.
  - Periodic and aperiodic are also things

- The five characteristics of a real-time system are:
  - Determinism - Operations are performed at predetermined times
  - Responsiveness - (Low) response time to handle ISRs
  - User control - Over task priority
  - Reliability - Lack of failures
  - Fail-soft operation - Graceful degradation in case of failure

- The three kinds of real-time scheduling are:
  - Static table-driven
  - Static priority-driven
  - Dynamic planning-based

- TODO: rest of chapter 10

### Ch 11: I/O Management 

Power words
- block
- block-oriented device
- circular buffer
- device I/O
- DMA
- disk cache
- gap
- I/O buffer
- I/O channel
- I/O processor
- logical I/O
- programmed I/O
- read/write head
- RAID
- rotationald elay
- sector
- seek time
- stream-oriented device
- track
- transfer time

- Three types of I/O devices
  - Human readable (e.g. displays, keyboard)
  - Machine readable (e.g. disks, sensors)
  - Communication (e.g. modems)
- Techniques for performing I/O
  - Programmed I/O
  - Interrupt driven I/O
  - Direct memory access (D M A)
- Evolution of I/O function
  - Direct programmed control
  - Programmed I/O
  - Interrupt driven I/O
  - Direct memory access (I/O module <-> memory direct link)
  - I/O module has its own processor
  - I/O module has its own memory
- Design objectives
  - Efficiency: Interleave I/O and processing
  - Generality: Abstract away specifics of I/O device
- I/O organization model
  - Hardware
  - Scheduling and control
  - Device I/O
  - (Physical organization)
  - I/O thing (communication architecture, logical I/O, file system)
  - (Directory management)
  - User processes
- I/O buffering
  - Block-oriented device
    - Information stored and transferred in fixed-size blocks
    - e.g. disks, USB keys
  - Stream-oriented device
    - I/O
    - e.g. keyboard, network
  - Types of buffering
    - Single buffering
    - Double buffering / buffer swapping
    - Circular buffering: useful if there are bursts of I/O
- Disk scheduling
  - Disks are 4 orders of magnitude slower than memory
  - Terms:
    - Seek time: Time to position the head on the track (circle)
    - Rotational delay: Time for beginning of sector to reach the head
    - Access time: Seek time + rotational delay
    - Transfer time: Time to transfer all data
