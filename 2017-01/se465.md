# SE 465: Software Testing and Quality Assurance

Professor: Patrick Lam

## Faults, errors, and failures

- **Validation** is ensuring that the code does the right thing.
- **Verification** is ensuring that the code conforms to the specs.
- **Faults** are static defects present in the software.
- An **error** is bad internal state caused by some fault.
- **Failures** are bad external behavior.

## RIP fault model

- **Reachability**: The fault must be reachable.
- **Infection**: The program state must be wrong after reaching the fault.
- **Propagation**: The infected state must propagate to output.

## Dealing with faults

- **Avoidance**: Use Rust, don't allow buffer overflows, better system design
- **Detection**: Testing
- **Tolerance**: Let system keep working even when there are faults

## Static vs. dynamic

- **Static** testing: Find fault directly by analyzing the program.
  - Type checking, dead code analysis, null check, bounds check, code review, formal verification
- **Dynamic** testing: Find failures by running the program with inputs and comparing them to expected outputs.
  - Black-box: Don't look at code
  - White-box: Look at code
  - Pros: It is easy to run the program
  - Cons: We need to generate inputs, and test suites can grow unnecessarily large

## Words Patrick does not like

- "Complete testing", "exhaustive testing"
  - No way. Input space is too big.
- "Full coverage"
  - Of what? Lines? Branches?

## When to stop testing

- You run out of time (open-ended exploratory testing, automatic input generation)
- You've explored "enough" of the behaviors, use cases, program states, inputs, and statements/branches

## Controllability and observability

- **Observability**: How easy it is to observer the system's behavior
- **Controllability**: How easy it is to provide the system with inputs

## Test cases

- A **test case value** is an input
- An **expected result** is the expected output
- **Prefix values** are inputs to prepare software for the test cases
- **Postfix values** are inputs to tidy up software after test cases
  - Verification values: Show results of test cases
  - Exit commands: Terminate or return to initial state
- A **test case** is (test case values, expected results, prefix values, postfix values)
- A **test set** is a set of test cases
- A **test requirement** is a property of the program that a test case can satisfy.
  - e.g.
    - This branch is followed
    - This method is called
  - Can be infeasible if there is dead code
- A **criterion** generates test requirements. e.g.
  - All branches are followed
  - All methods are called
- **Coverage level** of a test set is fraction of test requirements satisfied by it

## Exploratory testing

- **Exploratory testing**: Testers do it to discover interesting behavior. It's good for
  - Realism.
  - Finding important bugs in the shortest time.
  - Evaluating risks and seeing if scripted tests are needed.
- Process:
  - Start with a **charter**. e.g. "Explore the product elements"
  - Decide which area of the software to test.
  - Design a test.
  - Execute a test and log bugs.
  - Repeat.
- Outputs:
  - Set of bug reports
  - Test notes
  - Artifacts (input/output pairs)

## Control flow graph

- **Control flow graph**: Graph of program execution.
  - Nodes are sequential statements
  - Edges mean "the program can follow this path during execution"
  - **Basic block**
    - A sequence of nodes with a single entry/exit which when entered will always run sequentially
    - Can be combined into a single node
- **Coverage criterion**: Generates of a set of test requirements given a CFG.
  - e.g. "statement coverage" is "given G, generate all nodes in G"
- **Test path**: The path in the CFG that the program follows given a particular **test input**.
- Test set T **satisfies** coverage criterion C on graph G...
  - iff there exists some test path in path(T) that satisfies each test requirement generated by C.
- **Nondeterministic** programs or inputs could result in multiple test paths for a single input.
- **Statement coverage** formally:
  - $TR$ contains a requirement to visit $n$ for each node $n \in reach_G(N_0)$.
- **Branch coverage** formally:
  - $TR$ contains each reachable path of length at most $1$ in $G$.
- **Complete path coverage**:
  - $TR$ contains all paths in $G$.
  - Impossible for graphs with loops.
- For real programs, 80% coverage is usually good enough

## Finite state machine

- Higher level graph to describe your program state.
  - Nodes are software states
  - Edges are transitions between them
  - Edges may be guarded by preconditions and postconditions
- Test requirements and coverage are analogous to those for CFGs
- **Simple round trip coverage**: $TR$ has at least one round-trip path for every reachable node in $G$ in a round-trip path.
- **Complete round-trip coverage** $TR$ has all round-trip paths for every reachable node in $G$.
- To obtain an FSM:
  - CFGs can be considered really bad FSMs
  - Look at the software structure or specs. This is effortful, subjective, and requires system expertise.
  - Model state using relevant state variables and prune using domain knowledge.
  - iComment, Daikon

## Syntax-based testing

- Grammars can be input-space (for test inputs) or grammar-space (for mutations).
- Grammars can be regexes or context free grammars.
- Obtaining invalid strings
  - Mutate the grammar by adding/removing/permuting terminals and nonterminals
  - Or just misderive rules the same way (on occasion)

## Fuzzing

- First of all, everything is bad. Please write to your MPs.
- Examples:
  - JavaScript created by Fuzzinator crashed WebKit
  - 1988 Prof. Barton Miller taught Advanced Operating Systems, students crashed 25-33% of UNIX utilities using fuzzer.
  - 1983 Apple MacPaint MacWrite "The Monkey", MonkeyLives: ignore quit button.
- Mutation-based:
  1. Randomly flip bytes
  2. Parse input and change terminals/nonterminals
- Generation-based:
  - Become increasingly sophisticated.
  - e.g. for a C compiler
    1. Random bitstring
    2. Random ASCII chars
    3. Sequence of words, separators, and whitespace
    4. Syntactically correct programs
    5. Type-correct programs
    6. Statically conforming programs
    7. Dynamically conforming programs
    8. Model-conforming programs
- Chaos Monkey by Netflix takes random servers down
- american fuzzy lop is a cool fuzzer

## Test coverage in reality

- Open source: 20-95% statement coverage
- Industry: 80% statement coverage
- JUnit: 85% statement coverage. 13000 lines of system code, 15000 lines of test code
  - 65% coverage on deprecated code
  - 93% coverage on non-deprecated code
- Reasons for not testing
  - Code is too simple (getters, setters, empty methods)
  - Dead by design (code that should never actually be run)
  - Hard to execute code (like OOM handlers)

## Mutation testing

- Change program so it's wrong and see if tests fail.
- **Ground string**: A program. ("a string belonging to a programming language's grammar")
- **Mutation operator**: A way to change a program. ("specifies syntactic variations of a string")
- **Mutant**: A changed program. ("the result of applying a mutation operator to a ground string")
- A test case **kills** a mutant if the test case distinguishes between the mutant and the original.
- **Mutation score**: % of mutants killed given a fixed set of mutants
- **Mutation testing**: Keep adding tests until the mutation scores reaches some target
- Uninteresting mutants include those which are:
  - Stillborn: Can't compile or immediate crash
  - Trivial: Killed by almost any test case
  - Equivalent: Same as original program
- Strong and weak
  - Strong mutation: The fault must propagate to output (RIP)
  - Weak mutation: The fault need only infect state (RI)
  - Strong killing: The mutation is killed by a mismatch in output
  - Weak killing: The mutation is killed by any internal error state
  - **Strong mutation coverage** (SMC): All mutants are strongly killed by some test in TR.
  - **Weak mutation coverage** (WMC): All mutants are weakly killed by some test in TR.
- General mutation test algorithm:
  - Create mutants
  - Eliminate known-equivalent mutants
  - While not enough mutants killed:
    - Generate test cases
    - Run test cases on program
    - Run test cases on mutants
    - Filter out bogus test cases (ones which kill no mutants)
  - Is program output on test cases correct?
    - Yes -> WÃ¼nderbar!
    - No -> Fix program, start from beginning
- **Mutation operators**: Ways to change the program
- Integration mutation testing
  - Change param values in caller
  - Change choice of callee
  - Change callee inputs and outputs
  - In Object Oriented:
    - Modify object of field accesses / method calls
- PIT mutation tool is cool

## Evaluating test suites

- Coverage vs. Mutation testing
  - Out of 300 real bugs:
    - 73% were found by mutation testing
    - 50% were found by branch coverage
    - 40% were found by statement coverage

## Test suite engineering

- Principles
  - Prefer smaller tests over one big test (A1Q1: 21/232 had <5 tests)
    - Easier to understand and fix failures
  - Name tests well
  - Make it easy to add new tests
    - Separate setup and teardown code
    - Make it easy to do the right thing
- Unit tests
  - Exercise a single module or function
  - Replace dependencies with stubs, mocks, or fakes
- Integration tests
  - Exercise an entire graph of dependencies
  - Slower, harder to write and run
  - Represents user interaction
- Test-driven development
  - Write tests first
- Flaky tests
  - Randomly fail. Are bad.
  - Causes
    - Concurrency
    - Non-hermetic tests
    - Network use
    - Timeouts
    - Random order iterators
    - Random numbers

## Selenium

- It automates browser tests
- IDE allows for record/replay
- WebDriver is an API for browsers
- **Page object**: Layer of abstraction between the test and WebDriver. (e.g. for Google, a page object might have .executeSearchQuery())

## How to make code review better

- Positive tone
- Explain why
- Don't be Dr. No
- Specific and actionable advice
  - Linked to places in the code
- Acknowledge effort, say things that are good
- Don't ask for unnecessary changes
- Provide shorter chunks for review
- Think beyond the superficial
- Make sure the code works
  - has tests
  - did you run the tests?
- Ask for clarifications on areas of confusion

Things to check:

- Formatting and naming
- Don't repeat yourself
- Does the code fail fast?
- Avoid magic numbers
- One purpose per variable

## Reporting bugs

- The goal is to get the most important bugs fixed, and to improve code quality.
- Important things:
  - Explain bug's consequences
  - Explain steps to reproduce
    - Specific steps
    - Minimal test case
  - Respond to follow-up
  - Correct tone
- Important bugs:
  - Affect many things
  - Have severe consequenes
    - Unavailability
    - Security
    - Data loss
- Anatomy of a bug report:
  - Bug ID
  - Reporter
  - Version
  - Severity: blocker, ..., trivial, enhancement
  - Assigned to
  - CC
  - Keywords
  - Depends on
  - Attachments
  - Summary: One-line recap of bug
  - Other fields:
    - Comments
    - Priority

## Testing tools

- You can verify your code using
  - Manual testing
  - Automated testing suite, manually generated
  - Automatically-generated suites
  - Static analysis tools

## FindBugs

- Static analysis of JVM bytecode
- Finds patterns, such as
  - off-by-one errors
  - null pointer derefs
  - ignored read() return values
  - some ignored return values
  - uninitialized reads in constructor

## Approaches to static analysis

- Generic approach
  - PMD, jshint, FindBugs
- Specification-driven approach
  - Korat
  - Devs express what code is supposed to do
  - More concise notation than imperative code

## False positives

- Correct answer depends on system requirementns
- Analysis tool not powerful enough
- e.g. FindBugs says "method ignores exception" -- sometimes that is the right thing to do

## Coverity

- Design goal: minimize false positives (even with millions of LOC)
- Infer beliefs about program behavior
- Middle ground between generic and spec-driven approaches

## Korat

- Exhaustive generate all inputs as defined in the spec
- Can use inputs in testing

## Facebook Infer

- Static analysis tool
- Also enforces generic program properties
- Open source and runs on industrial codebaces
- Works on C, Objective C, C++, Java
- Written in OCaml
- Infer Eradicate
  - References treated as non-null by default
  - @Nullable allows null
- Does inter-procedural analysis
- Leak detection
  - Memory leaks
  - Resource leaks (like files)
- Taint analysis
  - Data from untrusted source should not go to trusted sink
  - ... unless it passes through a sanitizer

## Dynamic analysis

- Generic properties
- Memory errors
  - Memory errors with Memcheck
  - Race conditions with Helgrind
- Memcheck
  - Illegal reads and writes
  - Reads of uninitialized memory
  - Bad frees: `free(n); free(n); // no`
  - Memory moves: `memcpy` must not overlap
  - Bad arg values: `malloc(-3)`
  - Memory leaks (threw away all pointers to allocated memory)
  - Performance penalty: 10x-50x
- Address Sanitizer
  - Alternative to memcheck
  - 2x slowdown
  - Use-after-free
  - Use-after-return/after-scope
  - Double-free, invalid free
  - Memory leaks
- Design decisions
  - Valgrind may return false positives
  - Address Sanitizer does NOT return false positives
    - terminates upon any error
- Implementation techniques
  - Valgrind: Virtual CPU and check relevant instructions
  - Address Sanitizer: Rewrite every memory access to check it, keep metadata about valid memory
- Helgrind
  - Detects race conditions
