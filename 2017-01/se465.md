# SE 465: Software Testing and Quality Assurance

Professor: Patrick Lam

## Code gone wild

Software usually goes wrong and there are ways to make it go wrong less and go less wrong.

- **Validation** is ensuring that the code does the right thing.
- **Verification** is ensuring that the code conforms to the specs.
- **Faults** are static defects present in the software.
- An **error** is bad internal state caused by some fault.
- **Failures** are bad external behavior.

RIP fault model:
- **Reachable**: The fault must be reachable.
- **Infection**: The program state must be wrong after reaching the fault.
- **Propagate**: The infected state must propagate to output.

Dealing with faults:
- Avoid (e.g. use Rust)
- Detect (unit testing)
- Tolerate (redundancy, isolation, checking input)

Static vs. dynamic:
- **Static** techniques: Find fault directly by analyzing the program.
  - Examples: type checking, dead code analysis, null check, bounds check, code review, formal verification.
- **Dynamic** techniques: Observe failures by running the program with particular inputs and comparing them to expected.
  - Pros: It is easy to run the program
  - Cons: We need to generate inputs, and test suites can grow unnecessarily large

Words Patrick Lam does not like:
  - "Complete testing", "exhaustive testing"
    - No way. Input space is too big.
  - "Full coverage"
    - Of what? Lines? Branches?

You should stop testing when:
  - You run out of time (open-ended exploratory testing, automatic input generation)
  - You've explored "enough" of the behaviors, use cases, program states, inputs, and statements/branches

Controllability and observability are also things.

A **test requirement** (TR) is an element of an artifact that some test case must satisfy. For example,

- "This branch is followed"
- "This method is called"

## Exploratory testing

**Exploratory testing**: Testers do it to discover interesting behavior. It's good for
- Realism.
- Finding important bugs in the shortest time.
- Evaluating risks and seeing if scripted tests are needed.

The process is as follows:
- Start with a **charter**. e.g. "Explore the product elements".
  - Decide which area of the software to test.
  - Design a test.
  - Execute a test and log bugs.
  - Repeat.

And has these outputs
- Set of bug reports
- Test notes
- Artifacts (input/output pairs)

See WaterlooWorks example.

## Control flow graph

- A **control flow graph** is a graph where nodes are sequential statements and edges mean "the program can follow this edge during execution". **Basic blocks** are sequences of nodes with a single entry and single exit where once entered will *always* run sequentially, and they can be combined into a single node. Practice drawing CFGs for programs.
- A **coverage criterion** generates of a set of test requirements given a CFG. For example, "statement coverage" means "given G, generate all nodes in G".
- A **test path** is the path in the CFG that the program follows given a particular **test input**. **Nondeterministic** programs or inputs could result in multiple test paths for a single input.
- A **test set** is a set of inputs.
- We say that a test set T **satisfies** coverage criterion C on graph G iff there exists some test path in path(T) that satisfies each test requirement generated by C.

## Finite state machine

- A higher level graph to describe your program state. Nodes are software states, and edges are transitions between them and may be guarded by preconditions and postconditions.
- You can also have test requirements and coverage for FSMs. **Simple round trip coverage** is when there is at least one round-trip path for every reachable node in G with a round-trip path. **Complete round-trip coverage** is when all round-trip paths are present.
- To obtain an FSM:
  - You can look at the software structure or specs. This is effortful, subjective, and requires system expertise.
  - Instead, you can obtain an FSM from shared state. Just identify key variables that summarize system state.

## Syntax-based testing

- You can generate test inputs using grammars, or by mutation-based testing.  
- Grammars can be regexes or CFGs.
- To obtain invalid strings, you can
  - Mutate the grammar, or
  - Misderive rules by adding/removing/permuting terminals and nonterminals
- Fuzzing:
  - Crashed WebKit using particularly bad JavaScript
  - A while back, UNIX utils crashed 1/4-1/3 of the time on random ASCII inputs
  - Mac Paint: Random inputs for testing. Monkey Lives to stop Mac Paint from exiting.

## Fuzzing

- First of all, everything is bad and please write to your MPs.
- You can modify existing test cases either by mutation or by generating the with a grammar.
- Generation:
  - E.g. become increasingly sophisticated to try to get a C compiler to crash.
    1. Random bitstring
    2. Random ASCII chars
    3. Sequence of words, separators, and whitespace
    4. Syntactically correct programs
    5. Type-correct programs
    6. Statically conforming programs
    7. Dynamically conforming programs
    8. Model-conforming programs
- Mutation-based:
  1. Randomly flip bytes
  2. Parse input and change terminals/nonterminals
- Chaos Monkey by Netflix takes random servers down

## Test coverage in reality

- Industry: 80% statement coverage.
- JUnit: 85% statement coverage. 13000 lines of system code, 15000 lines of test code
  - 65% coverage on deprecated code
  - 93% coverage on non-deprecated code
- Reasons for not testing
  - Code is too simple (getters, setters, empty methods)
  - Dead by design (code that should never be run)
  - Hard to execute code (like OOM handlers)

## Mutation testing

- Change program so it's wrong and see if tests fail.
- **Ground string**: A program. ("a string belonging to a programming language's grammar")
- **Mutation operator**: A way to change a program. ("specifies syntactic variations of a string")
- **Mutant**: A changed program. ("the result of applying a mutation operator to a ground string")
- A mutant is **killed** by a test case if the test case distinguishes between the mutant and the original.
  - Strong killing: mutation is killed by a mismatch in output
  - Weak killing: mutation is (also) killed by internal error state
- **Mutation score**: % of mutants killed given a fixed set of mutants
- **Mutation testing**: Keep adding tests until the mutation scores reaches some target
- Uninteresting mutants include those which are:
  - Stillborn (can't compile or immediate crash)
  - Trivial (killed by almost any test case)
  - Equivalent (same as original program)
- Reachability
- Infection
- Propagation
- General mutation test algorithm:
  - Generate mutants of program
  - Eliminate known-equivalent mutants
  - While not enough mutants killed:
    - Generate test cases
    - Run test cases on program and mutants
    - Filter out bogus test cases (ones which kill no mutants)
  - Is program output on test cases correct?
    - Yes -> Good!
    - No -> Fix program, start from beginning
- Mutation integration tests
  - Change params in caller
  - Change choice of callee
  - Change callee inputs and outputs
- Measuring quality of a test suite
  - Good measures: Coverage
  - Better measures: Mutation killing
