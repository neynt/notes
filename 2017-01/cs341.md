# CS 341: Algorithms
Professor: Semih Salihoglu

## Introduction

An algorithm is a well-defined procedure to solve a computational problem. Types include:

- **Serial** vs. **parallel**: whether parallel computation is used.
- **Deterministic** vs. **randomized**: whether the algorithm's execution is determined completely by its input, or if it makes use of an external source of randomness to improve certain characteristics of the algorithm.
- **Exact** vs. **approximate**: describes the kind of answer the algorithm gives.

In CS 341, we mainly focus on serial, deterministic, exact algorithms.

Coming up with algorithms is fun, but not nearly all there is to do. We also analyze algorithms to understand their costs in terms of time, memory, and I/O.

## Running time analysis

### Notes on CS 341

We will focus on worst case analysis. We will be sloppy with our counting. And we are mainly interested in the limiting behavior for large inputs.

## O-notation

$$
\begin{eqnarray}
T(n) \in O(f(n))      \iff \exists c,n_0 > 0 | \forall n \geq n_0 T(n) \leq cf(n) \\
T(n) \in \Omega(f(n)) \iff \exists c,n_0 > 0 | \forall n \geq n_0 T(n) \geq cf(n) \\
T(n) \in o(f(n))      \iff \forall c > 0, \exists n_0 | \forall n \geq n_0 T(n) \leq cf(n) \\
T(n) \in \omega(f(n)) \iff \forall c > 0, \exists n_0 | \forall n \geq n_0 T(n) \leq cf(n) \\
T(n) \in \Theta(f(n)) \iff T(n) \in O(f(n)) \cap \Omega(f(n))
\end{eqnarray}
$$

e.g. $$T(n)$$ is in $$O(f(n))$$ if some multiple of $$T(n)$$ eventually exceeds $$f(n)$$ forever.

For example, degree-k polynomials are in $$O(n^k)$$. Proof: Let $$c = \sum a_i$$, $$n_0 = 1$$. Then,

$$
\begin{eqnarray}
T(n) &=& \sum a_{i} n^i \\
T(n) &\leq& \sum a_{i} n^i \\
T(n) &\leq& cn^k
\end{eqnarray}
$$

for all $$ n \geq n_0 $$.

You can also use a limit.
$$
L = \lim_{n \to \infty} \frac{T(n)}{f(n)} = 
\begin{cases}
0 & \implies T(n) \in o(f(n)) \\
\infty & \implies T(n) \in \omega(f(n)) \\
a & \implies T(n) \in \Theta(f(n))
\end{cases}
$$

To get rid of logs, use l'Hôpital's rule.

## Useful math facts

1. Sum of arithmetic sequence = $$ \frac{n(n+1)}{2} $$
2. Sum of squares = $$ \frac{n(n+1)(2n+1)}{6} $$
3. Sum of cubes = $$ \left( \frac{n(n+1)}{2} \right) ^2 $$
4. Sum of $$ i^d \in \Theta(n^{d+1}) $$
5. $$ \sum_{i=0}^{n-1} c^i = \frac{c^n - 1}{c-1} $$, which is $$ \Theta(c^n) $$ if $$ c > 1 $$, $$ \Theta(n) $$ if $$ c = 1 $$, and $$ \Theta(1) $$ if $$ c < 1 $$.
6. $$ \sum_{i=1}^n \frac{1}{i} \in \Theta(\log n) $$
7. $$ \log(n!) = \sum_{i=1}^n \log n = n \log n - \Theta(n) \in \Theta(n \log n) $$

As an example of that last rule, consider the following program.

```
for i from n down to 1
  j = i
  while j ≤ n
    j = 2j
```

The total work done is

$$
\begin{eqnarray}
&& \sum_{i=1}^n \log \frac{n}{i} + 1 \\
&=& n + n \log n - (n \log n - \Theta(n)) \\
&=& n + \Theta(n)  \in \Theta(n)
\end{eqnarray}
$$

## Recurrences

When analyzing divide and conquer algorithms, you often run into recurrences like the following.

$$
T(n) \leq 2 T(n/2) + 7n
$$

Generally, there are three approaches to solve these recurrences.

### 1. Proof by induction

  - Use intuition to make a guess on an upper bound, and prove it holds for the recurrence by induction.
  - If your inductive step doesn't work out, you may need a stronger hypothesis, not a weaker one (as your gut may tell you).

### 2. Recursion tree method

  - Draw a recursion tree with nodes labelled as the amount of work done **outside of recursion**.
  - Write an expression for the total work at a given level, and sum over all levels.

### 3. Master method

This is a handy theorem. The solution to the recurrence

$$
T(n) = aT(\frac{n}{b}) + O(n^d)
$$

is given by

$$
T(n) \in
\begin{cases}
O(n^d \log n) & a=b^d \\
O(n^d) & a<b^d \\
O(n^{\log_b a}) & a>b^d
\end{cases}
$$

You can prove it by solving the recursion tree method for a general recurrence in the given form. (Use Useful Math Fact 5.)

## Appendix: Specific examples of algorithms

### Selection sort

```
selectionSort:
  input: X: array of n numbers
  for i = 1 ... n
    let minIndex = i
    for j = i+1 ... n
      if X[j] < X[minIndex]
        minIndex = j
    swap X[i], X[minIndex]
  return X
```

Running time analysis:
- Each line takes one op
- A single run of the inner loop takes 3 ops
- The inner loop takes $$3(n-1) + 3(n-2) + ... + 3 = \frac{3n(n-1)}{2}$$ ops
- The outer loop, initialization, and swap all run n times, so they each take n ops
- In total, we use $$\frac{3n^2 + 3n}{2}$$ ops

### Merge sort

```
mergeSort:
  input: X: array of n numbers
  L = mergeSort X[1 ... n/2]
  R = mergeSort X[n/2 + 1 ... n]
  return merge L, R

merge:
  input: L, R: sorted array of m/2 numbers
  i = j = 1
  for k = 1 ... m
    if L[i] < R[j]
      Out[k] = L[i]
      i++
    else
      Out[k] = R[j]
      j++
  return Out
```

Running time analysis:
- Trickier because of the recursion. Use the recursion tree method.
- The merge itself takes $$m + 4m + 2 ≤ 7m$$ operations.
- The recursive calls form a tree at most $$\log_{2}n + 1$$ levels, and at level j, the subproblems are of size at most $$7n/2^j$$. So the total ops at level j is at most 7n.
- So, mergeSort takes at most $$7n(\log_{2}n + 1) = 7n \log_{2}n + 7n$$ operations.

### 2D maxima problem

- Input: Set of 2D points.
- Output: List of all "maximal" points, i.e. points such that no other point has a greater x AND y coordinate.
- Naive solution: check each point to see if it is dominated. $$O(n^2)$$.
- Divide and conquer: Sort points and split into left and right halves along the median x-coord.
  - In the right half, maximal points are definitely globally maximal.
  - In the left half, maximal points are globally maximal iff their y-coord is higher than the highest y-coord in the right half.
  - Base case: A single point is maximal.
  - $$O(n \log n)$$
- Good solution: Sort points by x-coord and scan from right to left, keeping track of the highest x-coord encountered. $$O(n \log n)$$
- WTF: Apparently you can get the number of maximal points (but not the points themselves) in $$O(n \sqrt{\log n})$$ time.

### Closest pair problem

- Input: Set of 2D points.
- Output: Pair of points of least Euclidean distance.
- Naive solution: Check all pairs. $$O(n^2)$$
- Divide and conquer: Sort points by x and recurse into left and right halves. The globally closest pair either crosses the median line or it doesn't.
  - If it doesn't, one of the recursive results is correct and you're good.
  - If it does, the points must be at most $\delta$ apart where $\delta$ is the distance of the closer of the two subsolutions. Consider the $2\delta$-wide strip around the dividing line. Sort points by y, and for each point, check points with y at most $$\delta$$ greater. Notice that there are at the very most $$8$$ points in this $$2\delta \times \delta$$ rectangle for each point, so this takes constant time.
  - $$O(n\log^2 n)$$
- Divide and conquer improved: Sort by y at the beginning, and pass it into your recursive calls, filtering on x. This makes a y-sorted list available to each recursive call without the $$O(n \log n)$$ sort step. Improves runtime to $$O(n \log n)$$.

### Integer multiplication

- Input: $$n$$-digit integers $$X,Y$$.
- Output: $$XY$$
- Grade school multiplication: $$O(n^2)$$
- Divide and conquer 1: Let $$X = 10^{n/2} A + B$$, $$Y = 10^{n/2} C + D$$. Recursively compute $$AC, AD, BC, BD$$. Then the answer is $$10^n AC + 10^{n/2}(AD + BC) + BD$$. Time: $$T(n) = 4T(n/2) + O(n)$$, which is still $$O(n^2)$$.
- Karatsuba-Ofman: Recursively compute $$AC, BD, (A+B)(C+D)$$. Then $$AD+BC = (A+B)(C+D)-AC-BD$$. Time: $$T(n) = 3T(n/2) + O(n)$$, so $$T(n) \in \Theta(n^{\log_2 3}) = \Theta(n^1.59...)$$
- Toom-Cook: Generalized Karatsuba-Ofman. $$O(n^{\log_k 2k-1}) \in O(n^{1+\epsilon})$$ for any $$\epsilon > 0$$.
- Schönhage-Strassen: Magic with FFTs in rings. $$O(n \log n \log \log n)$$.
