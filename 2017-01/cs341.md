# CS 341: Algorithms
Professor: Semih Salihoglu

## Introduction

An algorithm is a well-defined procedure to solve a computational problem. Types include:

- **Serial** vs. **parallel**: whether parallel computation is used.
- **Deterministic** vs. **randomized**: whether the algorithm's execution is determined completely by its input, or if it makes use of an external source of randomness to improve certain characteristics of the algorithm.
- **Exact** vs. **approximate**: describes the kind of answer the algorithm gives.

In CS 341, we mainly focus on serial, deterministic, exact algorithms.

Coming up with algorithms is fun, but not nearly all there is to do. We also analyze algorithms to understand their costs in terms of time, memory, and I/O.

## Running time analysis

### Notes on CS 341

We will focus on worst case analysis. We will be sloppy with our counting. And we are mainly interested in the limiting behavior for large inputs.

### O-notation

$$
\begin{eqnarray}
T(n) \in O(f(n))      \iff \exists c,n_0 > 0 | \forall n \geq n_0 T(n) \leq cf(n) \\
T(n) \in \Omega(f(n)) \iff \exists c,n_0 > 0 | \forall n \geq n_0 T(n) \geq cf(n) \\
T(n) \in o(f(n))      \iff \forall c > 0, \exists n_0 | \forall n \geq n_0 T(n) \leq cf(n) \\
T(n) \in \omega(f(n)) \iff \forall c > 0, \exists n_0 | \forall n \geq n_0 T(n) \leq cf(n) \\
T(n) \in \Theta(f(n)) \iff T(n) \in O(f(n)) \cap \Omega(f(n))
\end{eqnarray}
$$

e.g. $$T(n)$$ is in $$O(f(n))$$ if some multiple of $$T(n)$$ eventually exceeds $$f(n)$$ forever.

For example, degree-k polynomials are in $$O(n^k)$$. Proof: Let $$c = \sum a_i$$, $$n_0 = 1$$. Then,

$$
\begin{eqnarray}
T(n) &=& \sum a_{i} n^i \\
T(n) &\leq& \sum a_{i} n^i \\
T(n) &\leq& cn^k
\end{eqnarray}
$$

for all $$ n \geq n_0 $$.

You can also use a limit.
$$
L = \lim_{n \to \infty} \frac{T(n)}{f(n)} = 
\begin{cases}
0 & \implies T(n) \in o(f(n)) \\
\infty & \implies T(n) \in \omega(f(n)) \\
a & \implies T(n) \in \Theta(f(n))
\end{cases}
$$

To get rid of logs, use l'Hôpital's rule.

### Useful math facts

1. Sum of arithmetic sequence = $$ \frac{n(n+1)}{2} $$
2. Sum of squares = $$ \frac{n(n+1)(2n+1)}{6} $$
3. Sum of cubes = $$ \left( \frac{n(n+1)}{2} \right) ^2 $$
4. Sum of $$ i^d \in \Theta(n^{d+1}) $$
5. $$ \sum_{i=0}^{n-1} c^i = \frac{c^n - 1}{c-1} $$, which is $$ \Theta(c^n) $$ if $$ c > 1 $$, $$ \Theta(n) $$ if $$ c = 1 $$, and $$ \Theta(1) $$ if $$ c < 1 $$.
6. $$ \sum_{i=1}^n \frac{1}{i} \in \Theta(\log n) $$
7. $$ \log(n!) = \sum_{i=1}^n \log n = n \log n - \Theta(n) \in \Theta(n \log n) $$

As an example of that last rule, consider the following program.

```
for i from n down to 1
  j = i
  while j ≤ n
    j = 2j
```

The total work done is

$$
\begin{eqnarray}
&& \sum_{i=1}^n \log \frac{n}{i} + 1 \\
&=& n + n \log n - (n \log n - \Theta(n)) \\
&=& n + \Theta(n)  \in \Theta(n)
\end{eqnarray}
$$

## Divide and conquer algorithms

When analyzing divide and conquer algorithms, you often run into recurrences like the following.

$$
T(n) \leq 2 T(n/2) + 7n
$$

Generally, there are three approaches to solve these recurrences.

### 1. Proof by induction

  - Use intuition to make a guess on an upper bound, and prove it holds for the recurrence by induction.
  - If your inductive step doesn't work out, you may need a stronger hypothesis, not a weaker one (as your gut may tell you).

### 2. Recursion tree method

  - Draw a recursion tree with nodes labelled as the amount of work done **outside of recursion**.
  - Write an expression for the total work at a given level, and sum over all levels.

### 3. Master method

This is a handy theorem. The solution to the recurrence

$$
T(n) = aT(\frac{n}{b}) + O(n^d)
$$

is given by

$$
T(n) \in
\begin{cases}
O(n^d \log n) & a=b^d \\
O(n^d) & a<b^d \\
O(n^{\log_b a}) & a>b^d
\end{cases}
$$

You can prove it by solving the recursion tree method for a general recurrence in the given form. (Use Useful Math Fact 5.)

## Greedy algorithms

Easy to design and analyze. Hard to show correctness. Generally, there are two approaches to show correctness:

### 1. Greedy-stays ahead

"Every step, Greedy's best!"

### 2. Exchange

Consider any solution and show that the greedy solution is just as good.

## Appendix: Specific examples of algorithms

### Selection sort

```
selectionSort:
  input: X: array of n numbers
  for i = 1 ... n
    let minIndex = i
    for j = i+1 ... n
      if X[j] < X[minIndex]
        minIndex = j
    swap X[i], X[minIndex]
  return X
```

Running time analysis:
- Each line takes one op
- A single run of the inner loop takes 3 ops
- The inner loop takes $$3(n-1) + 3(n-2) + ... + 3 = \frac{3n(n-1)}{2}$$ ops
- The outer loop, initialization, and swap all run n times, so they each take n ops
- In total, we use $$\frac{3n^2 + 3n}{2}$$ ops

### Merge sort

```
mergeSort:
  input: X: array of n numbers
  L = mergeSort X[1 ... n/2]
  R = mergeSort X[n/2 + 1 ... n]
  return merge L, R

merge:
  input: L, R: sorted array of m/2 numbers
  i = j = 1
  for k = 1 ... m
    if L[i] < R[j]
      Out[k] = L[i]
      i++
    else
      Out[k] = R[j]
      j++
  return Out
```

Running time analysis:
- Trickier because of the recursion. Use the recursion tree method.
- The merge itself takes $$m + 4m + 2 ≤ 7m$$ operations.
- The recursive calls form a tree at most $$\log_{2}n + 1$$ levels, and at level j, the subproblems are of size at most $$7n/2^j$$. So the total ops at level j is at most 7n.
- So, mergeSort takes at most $$7n(\log_{2}n + 1) = 7n \log_{2}n + 7n$$ operations.

### 2D maxima problem

- Input: Set of 2D points.
- Output: List of all "maximal" points, i.e. points such that no other point has a greater x AND y coordinate.
- Naive solution: check each point to see if it is dominated. $$O(n^2)$$.
- Divide and conquer: Sort points and split into left and right halves along the median x-coord.
  - In the right half, maximal points are definitely globally maximal.
  - In the left half, maximal points are globally maximal iff their y-coord is higher than the highest y-coord in the right half.
  - Base case: A single point is maximal.
  - $$O(n \log n)$$
- Good solution: Sort points by x-coord and scan from right to left, keeping track of the highest x-coord encountered. $$O(n \log n)$$
- WTF: Apparently you can get the number of maximal points (but not the points themselves) in $$O(n \sqrt{\log n})$$ time.

### Closest pair problem

- Input: Set of 2D points.
- Output: Pair of points of least Euclidean distance.
- Naive solution: Check all pairs. $$O(n^2)$$
- Divide and conquer: Sort points by x and recurse into left and right halves. The globally closest pair either crosses the median line or it doesn't.
  - If it doesn't, one of the recursive results is correct and you're good.
  - If it does, the points must be at most $$\delta$$ apart where $$\delta$$ is the distance of the closer of the two subsolutions. Consider the $$2\delta$$-wide strip around the dividing line. Sort points by y, and for each point, check points with y at most $$\delta$$ greater. Notice that there are at the very most $$8$$ points in this $$2\delta \times \delta$$ rectangle for each point, so this takes constant time.
  - $$O(n\log^2 n)$$
- Divide and conquer improved: Sort by y at the beginning, and pass it into your recursive calls, filtering on x. This makes a y-sorted list available to each recursive call without the $$O(n \log n)$$ sort step. Improves runtime to $$O(n \log n)$$.

### Integer multiplication

- Input: $$n$$-digit integers $$X,Y$$.
- Output: $$XY$$
- Grade school multiplication: $$O(n^2)$$
- Naive divide and conquer: Let $$X = 10^{n/2} A + B$$, $$Y = 10^{n/2} C + D$$. Recursively compute $$AC, AD, BC, BD$$. Then the answer is $$10^n AC + 10^{n/2}(AD + BC) + BD$$. Time: $$T(n) = 4T(n/2) + O(n)$$, which is still $$O(n^2)$$.
- Karatsuba-Ofman: Recursively compute $$AC, BD, (A+B)(C+D)$$. Then $$AD+BC = (A+B)(C+D)-AC-BD$$. Time: $$T(n) = 3T(n/2) + O(n)$$, so $$T(n) \in \Theta(n^{\log_2 3}) = \Theta(n^{1.59...})$$
- Toom-Cook: Generalized Karatsuba-Ofman. $$O(n^{\log_k 2k-1}) \in O(n^{1+\epsilon})$$ for any $$\epsilon > 0$$.
- Schönhage-Strassen: Magic with FFTs in rings. $$O(n \log(n)\log(\log(n)))$$.

### Matrix multiplication.

- Input: $$n\times n$$ matrices $$A, B$$.
- Output: $$C = A \times B$$, assuming multiplication is one operation.
- Naive: $$O(n^3)$$
- Bad D+C
  - Split matrices into four quadrants $$A_{1,1}, A_{1,2}, A_{2,1}, A_{2,2}$$. Compute each quadrant.
  - $$ T(n) = 8T(n/2) + O(n^2) $$ so $$ O(n^3) $$.
- Strassen's algorithm
  - Same four quadrants, but only compute 7 recursive multiplications and combine them with magic.
  - $$ T(n) = 7T(n/2) + O(n^2) $$ so $$ O(n^{2.81}) $$.
- 1981: Schoenhage. $$ O(n^{2.522}) $$.
- 1983: Coppersmith + Winograd. $$ O(n^{2.496}) $$, improved to $$ O(n^{2.372}) $$.
- Speculation: may be $$ O(n^2) $$ ???

### Selection.

- Input: $$n$$ distinct numbers $$A_i$$.
- Output: $$i^{th}$$ smallest number in $$A$$.
- Naive: Sort. $$ O(n \log n) $$
- Median-of-medians:
  - Pick a pivot by sorting in groups of $$5$$ (or larger) and recursively picking the median of the size $$n/5$$ array of medians.
  - Partition and recurse in the correct side.
  - Note that picking the median of medians guarantees that the largest side is at largest $$ 7n/10 $$. Draw the sorted size-5 arrays out in a rectangle to see this.
  - Time: $$ T(n) \leq T(n/5) + T(7n/10) + O(n) $$

### Activity scheduling.

- Input: $$n$$ activities with start and finish times.
- Output: Largest set of non-overlapping activities.
- Greedy (bad): Earliest start. Counterexample: long first event
- Greedy (bad): Shortest. Counterexample: one short overlaps two longs
- Greedy (bad): Least overlap. Counterexample: by construction
- Greedy (correct): Earliest finish. Correctness: For any time T, greedy stays ahead.

### Job scheduling.

- Input: $$n$$ jobs with length $$\ell_i$$.
- Output: Schedule of jobs that minimizes sum of completion times.
- Greedy: Sort jobs by ascending length.
  - Proof: Greedy stays ahead by induction.

### Job scheduling 2.

- Input: $$n$$ jobs with length $$\ell_i$$ and weight $$w_i$$.
- Output: Schedule of jobs with minimum weighted sum of completion times.
- Greedy: Sort by ascending $$\ell_i / w_i$$.
  - Proof: By exchange. Any arbitrary solution can be transformed into another solution that is at least as good by swapping adjacent elements which are out-of-order with respect to the greedy solution. Since there are $$ O(n^2) $$ possible swaps, such a procedure will eventually result in the greedy solution. So any arbitrary solution, including optimal solutions, can be transformed into the greedy solution.

### Stable Marriage.

- Watch out, it's getting awfully graphy in here
- Input: $n$ men, $n$ women, each with a ranking of the other set.
- Gale-Shapley: Iteratively have one unpaired man propose to his top remaining choice. Women take the higher ranked of $m$ or $m^\prime$.
  - Prove 1.) termination, 2.) that a matching is returned, 3.) that the matching is stable, 4.) that the matching is always the same, 5.) that the matching is the optimal matching for men

### Linear independent set.

- Input: List of weights.
- Output: Subset of elements in the list, no two adjacent elements, with maximum sum of weights.
- Naive: Brute force. $$ O(2^n) $$.
- D+C: Possible, but hard to resolve conflicts at the center.
- DP: Solution either contains or does not contain last element in list.
  - If it does not, it is the same as the solution for (list - last element).
  - If it does, it is the solution for (list - last two elements) + last element.
  - A[i] = solution for first i elements of list.
  - Time: $$O(n)$$. WOW!

### Sequence alignment.

- Input: Strings $$X=x_1 \cdots x_m$$ and $$Y=y_1 \cdots y_n$$, gap penalty $$\delta$$, and mismatch penalties $$\alpha(i,j)$$
- Output: Minimum total penalty of any alignment of $$X$$ and $$Y$$ made by adding gaps.
- DP: $$A[i][j]$$ is the minimum possible penalty of matching the first $$i$$ characters of $$X$$ and the first $$j$$ characters of $$Y$$. Can determine each cell by looking at top, left, and top-left neighbours.
  - Time: $$O(n^2)$$. FANTASTIC!
  - Trace back to get the actual alignment (note: you can ALWAYS do this for DP!)

### Optimal parenthesization.

- Input: Dimensions of $$n$$ matrices, where matrix $$A_i$$ is $$d_{i-1} \times d_{i}$$
- Output: Min-cost parenthesization to multiply $$A_1 \times ... \times A_n$$, where $$(p\times q) \times (q \times r)$$ costs $$pqr$$
- DP: TODO
