# CS 341: Algorithms
Professor: Semih Salihoglu

## Introduction

An algorithm is a well-defined procedure to solve a computational problem. Types include:

- **Serial** vs. **parallel**: whether parallel computation is used.
- **Deterministic** vs. **randomized**: whether the algorithm's execution is determined completely by its input, or if it makes use of an external source of randomness to improve certain characteristics of the algorithm.
- **Exact** vs. **approximate**: describes the kind of answer the algorithm gives.

In CS 341, we mainly focus on serial, deterministic, exact algorithms.

Coming up with algorithms is fun, but not nearly all there is to do. We also analyze algorithms to understand their costs in terms of time, memory, and I/O.

## Running time analysis

### Selection sort

```
selectionSort:
  input: X: array of n numbers
  for i = 1 ... n
    let minIndex = i
    for j = i+1 ... n
      if X[j] < X[minIndex]
        minIndex = j
    swap X[i], X[minIndex]
  return X
```

How many operations does this algorithm take on an input of size n? Each line takes one op, so
- A single run of the inner loop takes 3 ops
- The inner loop takes $$3(n-1) + 3(n-2) + ... + 3 = \frac{3n(n-1)}{2}$$ ops
- The outer loop, initialization, and swap all run n times, so they each take n ops
- In total, we use $$\frac{3n^2 + 3n}{2}$$ ops

### Merge sort

```
mergeSort:
  input: X: array of n numbers
  L = mergeSort X[1 ... n/2]
  R = mergeSort X[n/2 + 1 ... n]
  return merge L, R

merge:
  input: L, R: sorted array of m/2 numbers
  i = j = 1
  for k = 1 ... m
    if L[i] < R[j]
      Out[k] = L[i]
      i++
    else
      Out[k] = R[j]
      j++
  return Out
```

This is trickier because of the recursion. First, note that the merge itself takes $$m + 4m + 2 ≤ 7m$$ operations. Then, notice that the recursive calls form a tree at most $$\log_{2}n + 1$$ levels, and at level j, the subproblems are of size at most $$7n/2^j$$. So the total ops at level j is at most 7n, and mergeSort takes at most $$7n(\log_{2}n + 1) = 7n \log_{2}n + 7n$$ operations.

### Notes on CS 341

We will focus on worst case analysis. We will be sloppy with our counting. And we are mainly interested in the limiting behavior for large inputs.

## O-notation

$$
\begin{eqnarray}
T(n) \in O(f(n))      \iff \exists c,n_0 > 0 | \forall n \geq n_0 T(n) \leq cf(n) \\
T(n) \in \Omega(f(n)) \iff \exists c,n_0 > 0 | \forall n \geq n_0 T(n) \geq cf(n) \\
T(n) \in o(f(n))      \iff \forall c > 0, \exists n_0 | \forall n \geq n_0 T(n) \leq cf(n) \\
T(n) \in \omega(f(n)) \iff \forall c > 0, \exists n_0 | \forall n \geq n_0 T(n) \leq cf(n) \\
T(n) \in \Theta(f(n)) \iff T(n) \in O(f(n)) \cap \Omega(f(n))
\end{eqnarray}
$$

e.g. $$T(n)$$ is in $$O(f(n))$$ if some multiple of $$T(n)$$ eventually exceeds $$f(n)$$ forever.

For example, degree-k polynomials are in $$O(n^k)$$. Proof: Let $$c = \sum a_i$$, $$n_0 = 1$$. Then,

$$
\begin{eqnarray}
T(n) &=& \sum a_{i} n^i \\
T(n) &\leq& \sum a_{i} n^i \\
T(n) &\leq& cn^k
\end{eqnarray}
$$

for all $$ n \geq n_0 $$.

You can also use a limit.
$$
L = \lim_{n \to \infty} \frac{T(n)}{f(n)} = 
\begin{cases}
0 & \implies T(n) \in o(f(n)) \\
\infty & \implies T(n) \in \omega(f(n)) \\
a & \implies T(n) \in \Theta(f(n))
\end{cases}
$$

To get rid of logs, use l'Hôpital's rule.

## Useful math facts

1. Sum of arithmetic sequence = $$ \frac{n(n+1)}{2} $$
2. Sum of squares = $$ \frac{n(n+1)(2n+1)}{6} $$
3. Sum of cubes = $$ \left( \frac{n(n+1)}{2} \right) ^2 $$
4. Sum of $$ i^d \in \Theta(n^{d+1}) $$
5. $$ \sum_{i=1}^n c^i = \frac{c^{n+1} - 1}{c-1} $$, which is $$ \Theta(c^n) $$ if $$ c > 1 $$, $$ \Theta(n) $$ if $$ c = 1 $$, and $$ \Theta(1) $$ if $$ c < 1 $$.
6. $$ \sum_{i=1}^n \frac{1}{i} \in \Theta(\log n) $$
7. $$ \log(n!) = \sum_{i=1}^n \log n = n \log n - \Theta(n) \in \Theta(n \log n) $$

As an example of that last rule, consider the following program.

```
for i from n down to 1
  j = i
  while j ≤ n
    j = 2j
```

The total work done is

$$
\begin{eqnarray}
&& \sum_{i=1}^n \log \frac{n}{i} + 1 \\
&=& n + n \log n - (n \log n - \Theta(n)) \\
&=& n + \Theta(n)  \in \Theta(n)
\end{eqnarray}
$$

## Recurrences

When analyzing divide and conquer algorithms, you often run into recurrences like the following.

$$
T(n) \leq 2 T(n/2) + 7n
$$

Generally, there are three approaches to solve these recurrences.

### 1. Proof by induction

  - Use intuition to make a guess on an upper bound, and prove it holds for the recurrence by induction.
  - If your inductive step doesn't work out, you may need a stronger hypothesis, not a weaker one (as your gut may tell you).

### 2. Recursion tree method

  - Draw a recursion tree with nodes labelled as the amount of work done **outside of recursion**.
  - Write an expression for the total work at a given level, and sum over all levels.

### 3. Master method

This is a handy theorem. The solution to the recurrence

$$
T(n) = aT(\frac{n}{b}) + O(n^d)
$$

is given by

$$
T(n) \in
\begin{cases}
O(n^d \log n) & a=b^d \\
O(n^d) & a<b^d \\
O(n^{\log_b a}) & a>b^d
\end{cases}
$$
