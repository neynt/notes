# CS 341: Algorithms
Professor: Semih Salihoglu

## Introduction

An algorithm is a well-defined procedure to solve a computational problem. Types include:

- **Serial** vs. **parallel**: whether parallel computation is used.
- **Deterministic** vs. **randomized**: whether the algorithm's execution is determined completely by its input, or if it makes use of an external source of randomness to improve certain characteristics of the algorithm.
- **Exact** vs. **approximate**: describes the kind of answer the algorithm gives.

In CS 341, we mainly focus on serial, deterministic, exact algorithms.

Coming up with algorithms is fun, but not nearly all there is to do. We also analyze algorithms to understand their costs in terms of time, memory, and I/O.

## Running time analysis

### Notes on CS 341

We will focus on worst case analysis. We will be sloppy with our counting. And we are mainly interested in the limiting behavior for large inputs.

### O-notation

$$
\begin{eqnarray}
T(n) \in O(f(n))      \iff \exists c,n_0 > 0 | \forall n \geq n_0 T(n) \leq cf(n) \\
T(n) \in \Omega(f(n)) \iff \exists c,n_0 > 0 | \forall n \geq n_0 T(n) \geq cf(n) \\
T(n) \in o(f(n))      \iff \forall c > 0, \exists n_0 | \forall n \geq n_0 T(n) \leq cf(n) \\
T(n) \in \omega(f(n)) \iff \forall c > 0, \exists n_0 | \forall n \geq n_0 T(n) \leq cf(n) \\
T(n) \in \Theta(f(n)) \iff T(n) \in O(f(n)) \cap \Omega(f(n))
\end{eqnarray}
$$

e.g. $$T(n)$$ is in $$O(f(n))$$ if some multiple of $$T(n)$$ eventually exceeds $$f(n)$$ forever.

For example, degree-k polynomials are in $$O(n^k)$$. (Prove this)

You can also use a limit to find the O-relationship between two functions.

$$
L = \lim_{n \to \infty} \frac{T(n)}{f(n)} = 
\begin{cases}
0 & \implies T(n) \in o(f(n)) \\
\infty & \implies T(n) \in \omega(f(n)) \\
a & \implies T(n) \in \Theta(f(n))
\end{cases}
$$

To get rid of logs, use l'Hôpital's rule.

### Useful math facts

1. Sum of arithmetic sequence = $$ \frac{n(n+1)}{2} $$
2. Sum of squares = $$ \frac{n(n+1)(2n+1)}{6} $$
3. Sum of cubes = $$ \left( \frac{n(n+1)}{2} \right) ^2 $$
4. Sum of $$ i^d \in \Theta(n^{d+1}) $$
5. $$ \sum_{i=0}^{n-1} c^i = \frac{c^n - 1}{c-1} $$, which is $$ \Theta(c^n) $$ if $$ c > 1 $$, $$ \Theta(n) $$ if $$ c = 1 $$, and $$ \Theta(1) $$ if $$ c < 1 $$.
6. $$ \sum_{i=1}^n \frac{1}{i} \in \Theta(\log n) $$
7. $$ \log(n!) = \sum_{i=1}^n \log n = n \log n - \Theta(n) \in \Theta(n \log n) $$

## Divide and conquer algorithms

When analyzing divide and conquer algorithms, you often run into recurrences like the following.

$$
T(n) \leq a T(n/b) + n^d
$$

Generally, there are three ways to solve these recurrences.

### 1. Proof by induction

  - Use intuition to make a guess on an upper bound, and prove it holds for the recurrence by induction.
  - If your inductive step doesn't work out, you may need a stronger hypothesis, not a weaker one (as your gut may tell you).

### 2. Recursion tree method

  - Draw a recursion tree with nodes labelled as the amount of work done **outside of recursion**.
  - Write an expression for the total work at a given level, and sum over all levels.

### 3. Master method

This is a handy theorem. The solution to the recurrence

$$
T(n) = aT(\frac{n}{b}) + O(n^d)
$$

is given by

$$
T(n) \in
\begin{cases}
O(n^d \log n) & a=b^d \\
O(n^d) & a<b^d \\
O(n^{\log_b a}) & a>b^d
\end{cases}
$$

You can prove it by solving the recursion tree method for a general recurrence in the given form. (Use Useful Math Fact 5.)

## Greedy algorithms

These are easy to design and analyze, but it's hard to show their correctness. Generally, there are two approaches to show correctness:

### 1. Greedy-stays ahead

Show by induction that at the $$i^{th}$$ step, the greedy solution is at least as good as any other solution.

### 2. Exchange

Consider any optimal solution, and show that it can be transformed into the greedy solution by a sequence of steps, each of which doesn't make the solution any worse.

## Dynamic programming

- Express the solution in terms of solutions to smaller subproblems
- Build an array bottom-up to determine the optimal solution
- If necessary, backtrace to determine the solution itself

To show correctness, consider any optimal solution and try to express the optimal solutions to smaller subproblems in terms of the optimal solution. Then, invert the relationship to get the recurrence.

## Graph algorithms

A graph $$ G = (V, E) $$ is vertices and edges connecting them. Know what directed/undirected, simple/multi, cyclic/acyclic, connected/unconnected mean. Conventionally $$ n = |V| $$, $$ m = |E| $$.

### Storage formats

**Adjacency matrix**: Use an $$n \times n$$ array of booleans, $$A[i][j]$$ means there is an edge between $$i$$ and $$j$$. $$O(n^2)$$ storage, $$O(1)$$ existence check, $$O(n)$$ neighbour iteration. Good for dense graphs.
**Adjacency list**: Use an array of lists of neighbors. $$O(n+m)$$ storage, $$\deg(u)$$ existence, $$\deg(u)$$ neighbor iteration. Good for sparse graphs.

## Appendix: Specific examples of problems

### Sorting

- Input: List of numbers
- Output: Sorted list of the same numbers
- Selection sort: Repeatedly swap the $$i^th$$ smallest number into the right place (determine it by a linear scan) -- $$O(n^2)$$ time
- Merge sort: Sort the first half, sort the last half, and merge cleverly -- $$O(n \log n)$$ time

### 2D maxima

- Input: Set of 2D points.
- Output: List of all "maximal" points, i.e. points such that no other point has a greater x AND y coordinate.
- Naive: Check each point to see if it is dominated. $$O(n^2)$$.
- Divide and conquer: Sort points and split into left and right halves along the median x-coord.
  - In the right half, maximal points are definitely globally maximal.
  - In the left half, maximal points are globally maximal iff their y-coord is higher than the highest y-coord in the right half.
  - Base case: A single point is maximal.
  - $$O(n \log n)$$
- Good solution: Sort points by x-coord and scan from right to left, keeping track of the highest x-coord encountered. $$O(n \log n)$$
- WTF: Apparently you can get the number of maximal points (but not the points themselves) in $$O(n \sqrt{\log n})$$ time. #thingsresearchersdo

### Closest pair

- Input: Set of 2D points.
- Output: Pair of points of least Euclidean distance.
- Naive solution: Check all pairs. $$O(n^2)$$
- Divide and conquer: Sort points by x and recurse into left and right halves. The globally closest pair either crosses the median line or it doesn't.
  - If it doesn't, one of the recursive results is correct and you're good.
  - If it does, the points must be at most $$\delta$$ apart where $$\delta$$ is the distance of the closer of the two subsolutions. Consider the $$2\delta$$-wide strip around the dividing line. Sort points by y, and for each point, check points with y at most $$\delta$$ greater. Notice that there are at the very most $$8$$ points in this $$2\delta \times \delta$$ rectangle for each point, so this takes constant time.
  - $$O(n \log^2 n)$$
- Divide and conquer improved: Sort by y at the beginning, and pass it into your recursive calls, filtering on x. This makes a y-sorted list available to each recursive call without the $$O(n \log n)$$ sort step. Improves runtime to $$O(n \log n)$$.

### Integer multiplication

- Input: $$n$$-digit integers $$X,Y$$.
- Output: $$XY$$
- Grade school multiplication: $$O(n^2)$$
- Naive divide and conquer: Let $$X = 10^{n/2} A + B$$, $$Y = 10^{n/2} C + D$$. Recursively compute $$AC, AD, BC, BD$$. Then the answer is $$10^n AC + 10^{n/2}(AD + BC) + BD$$. Time: $$T(n) = 4T(n/2) + O(n)$$, which is still $$O(n^2)$$.
- Karatsuba-Ofman: Recursively compute $$AC, BD, (A+B)(C+D)$$. Then $$AD+BC = (A+B)(C+D)-AC-BD$$. Time: $$T(n) = 3T(n/2) + O(n)$$, so $$T(n) \in \Theta(n^{\log_2 3}) = \Theta(n^{1.59...})$$
- Toom-Cook: Generalized Karatsuba-Ofman. $$O(n^{\log_k 2k-1}) \in O(n^{1+\epsilon})$$ for any $$\epsilon > 0$$.
- Schönhage-Strassen: Magic with FFTs over rings. $$O(n \log(n)\log(\log(n)))$$.

### Matrix multiplication

- Input: $$n\times n$$ matrices $$A, B$$.
- Output: $$C = A \times B$$, assuming multiplication is one operation.
- Naive: $$O(n^3)$$
- Bad D+C
  - Split matrices into four quadrants $$A_{1,1}, A_{1,2}, A_{2,1}, A_{2,2}$$. Compute each quadrant.
  - $$ T(n) = 8T(n/2) + O(n^2) $$ so $$ O(n^3) $$.
- Strassen's algorithm
  - Same four quadrants, but only compute 7 recursive multiplications and combine them with magic.
  - $$ T(n) = 7T(n/2) + O(n^2) $$ so $$ O(n^{2.81}) $$.
- 1981: Schoenhage. $$ O(n^{2.522}) $$.
- 1983: Coppersmith + Winograd. $$ O(n^{2.496}) $$, improved to $$ O(n^{2.372}) $$.
- Speculation: could even be $$ O(n^2) $$ ???

### Selection

- Input: $$n$$ distinct numbers $$A_i$$.
- Output: $$i^{th}$$ smallest number in $$A$$.
- Naive: Sort. $$ O(n \log n) $$
- Median-of-medians:
  - Pick a pivot by sorting in groups of $$5$$ (or larger) and recursively picking the median of the size $$n/5$$ array of medians.
  - Partition and recurse in the correct side.
  - Note that picking the median of medians guarantees that the largest side is at largest $$ 7n/10 $$. Draw the sorted size-5 arrays out in a rectangle to see this.
  - Time: $$ T(n) \leq T(n/5) + T(7n/10) + O(n) $$

### Activity scheduling

- Input: $$n$$ activities with start and finish times.
- Output: Largest set of non-overlapping activities.
- Greedy (bad): Earliest start. Counterexample: long first event
- Greedy (bad): Shortest. Counterexample: one short overlaps two longs
- Greedy (bad): Least overlap. Counterexample: by construction
- Greedy (correct): Earliest finish. Correctness: For any time T, greedy stays ahead.

### Job scheduling

- Input: $$n$$ jobs with length $$\ell_i$$.
- Output: Schedule of jobs that minimizes sum of completion times.
- Greedy: Sort jobs by ascending length.
  - Proof: Greedy stays ahead by induction.

### Weighted job scheduling

- Input: $$n$$ jobs with length $$\ell_i$$ and weight $$w_i$$.
- Output: Schedule of jobs with minimum weighted sum of completion times.
- Greedy: Sort by ascending $$\ell_i / w_i$$.
  - Proof: By exchange. Any arbitrary solution can be transformed into another solution that is at least as good by swapping adjacent elements which are out-of-order with respect to the greedy solution. Since there are $$ O(n^2) $$ possible swaps, such a procedure will eventually result in the greedy solution. So any arbitrary solution, including optimal solutions, can be transformed into the greedy solution.

### Stable marriage

- Watch out, it's getting awfully graphy in here
- Input: $n$ men, $n$ women. Each person has a ranking of the other group.
- Gale-Shapley: Iteratively have one unpaired man propose to his top remaining choice. Women take the higher ranked of $$m$$ or $$m^\prime$$.
  - Terminates: since there are at most $$O(n^2)$$ proposals
  - A matching is returned: since if there is an unmatched man/woman pair at the end, the man had to have proposed to the woman at some point, and that can't be the case
  - The returned matching is stable: since if there are lovers $$(m,w)$$ not matched to each other, then at some point $$m$$ proposed to $$w$$ and was either rejected or accepted, neither of which can be the case
  - The returned matching is always the same
  - The returned matching is man-optimal and woman-pessimal

### Linear independent set

- Input: List of weights.
- Output: Subset of elements in the list, no two adjacent elements, with maximum sum of weights.
- Naive: Brute force. $$ O(2^n) $$.
- D+C: Possible, but hard to resolve conflicts at the center.
- DP: Solution either contains or does not contain last element in list.
  - If it does not, it is the same as the solution for (list - last element).
  - If it does, it is the solution for (list - last two elements) + last element.
  - A[i] = solution for first i elements of list.
  - Time: $$O(n)$$. WOW!

### Sequence alignment

- Input: Strings $$X=x_1 \cdots x_m$$ and $$Y=y_1 \cdots y_n$$, gap penalty $$\delta$$, and mismatch penalties $$\alpha(i,j)$$
- Output: Minimum total penalty of any alignment of $$X$$ and $$Y$$ made by adding gaps.
- DP: $$A[i][j]$$ is the minimum possible penalty of matching the first $$i$$ characters of $$X$$ and the first $$j$$ characters of $$Y$$. Can determine each cell by looking at top, left, and top-left neighbours.
  - Time: $$O(n^2)$$. FANTASTIC!
  - Trace back to get the actual alignment (note: you can ALWAYS do this for DP!)

### Optimal parenthesization

- Input: Dimensions of $$n$$ matrices, where matrix $$A_i$$ is $$d_{i-1} \times d_{i}$$
- Output: Min-cost parenthesization to multiply $$A_1 \times ... \times A_n$$, where $$(p\times q) \times (q \times r)$$ costs $$pqr$$
- DP: $$A[i][j]$$ is the cost of the optimal parenthesization for $$A_i, ..., A_j$$.
  - $$O(n)$$ to compute each cell, so $$O(n^3)$$ time.

### Weighted activity selection

- Input: $$n$$ activities with start time, end time, and weight.
- Output: Subset of non-conflicting activities with maximum weight.
- DP: $$A[i]$$ is the maximum for the first $$i$$ activities.
  - First compute a lookup table of the latest activity that starts before the $$i^{th}$$ activity and doesn't conflict with the $$i^{th}$$ activity
  - To compute each cell, use the lookup table
  - $$O(n)$$ if input is sorted, otherwise the sort is $$O(n \log n)$$
