# ECE 454: Distributed Computing.

Professor: Wojciech Golab.

## 2018 May 1

- Why build distributed systems?
  - [[Resource sharing saves money.]]
  - [[Simplifies business processes.]]
  - [[Scales beyond what a centralized system can.]]
  - [[Users may be distributed.]]

- Anecdote: Lord Kelvin vs. Edward Orange Wildman Whitehouse.
  - Entrepreneur Whitehouse fried transatlantic cable by pumping voltage,
    against engineer Kelvin's warning.

- Middleware: Offers a single-system view between applications and operating
  systems on different machines.
  - Common middleware services
    - [[Communication (e.g. add job to remote queue)]]
    - [[Transactions (e.g. access independent services atomically)]]
    - [[Service composition (e.g. Google map with weather forecast)]]
    - [[Reliability (e.g. replicated state machine)]]

- Goals of distributed systems:
  - Support resource sharing
  - Make distribution transparent (7 types... memorize them)
    - Access: representing and accessing resources
    - Location: putting resources somewhere
    - Migration: moving resources
    - Relocation: moving resources while in use
    - Replication: replicating resources
    - Concurrency: sharing resources
    - Failure: failing and recovering resources
  - Be open, that is:
    - [[Interopable]]
    - [[Composable]]
    - [[Extensible]]
    - [[Policy separate from mechanism]]
  - Be scalable
    - [[Size]]
    - [[Geography]]
    - [[Administration]]
    - Avoid common mistakes that limit scalability:
      - Centralized services (e.g. a single server)
      - Centralized data (e.g. a single telephone book database)
      - Centralized algorithms (e.g. routing based on complete information)

## 2018 May 2

- Types of distribution transparency:
  - Access
  - Location
  - Relocation
  - Migration
  - Replication
  - Concurrency
  - Failure

- (2/3)c is the fastest light goes

- Scaling techniques:
  - Partitioning and distribution: Splitting stuff up
    - Example: Original DNS, which is a tree with zones
  - Replication: Having multiple copies of a thing
    - Example: memcached to speed up web apps
    - Can also have look-aside caching (look at cache and storage in parallel)

- Falsehoods programmers believe about distro
  - The network is reliable
  - The network is secure
  - The network is homogeneous
  - The topology does not change
  - Latency is zero
  - Bandwidth is infinite
  - Transport cost is zero
  - There is one administrator
- "Recall n of these" was on a previous exam

- Types of distributed systems
  - Website, web services
  - High performance computing (HPC)
    - Multiprocessor vs. multicomputer
    - Cluster computing
      - There is a master node that coordinates
    - Grid computing
    - Cloud computing
  - Transaction processing
  - Enterprise application integration
  - Internet of things (distributed pervasive systems)
  - Sensor networks
    - In-network data processing: When the sensors calculate things themselves
      rather than shunting data directly to the operator
    - Example: smart irrigation

- Shared memory vs. message passing
  - Shared memory is parallel computing
  - Message passing is distributed computing
  - Some theorists say they're both distributed computing

- Big Data: In one minute, many many things are happening.

## 2018 May 3

- Architecture (triggered)
  - ... is Components connected by Connectors

- Architectural styles (fuck)
  - layered
  - object-based (e.g. Enterprise Java Beans)
  - data-centred
  - event-based (e.g. Kafka)

- Multi-tiered architectures
  - Example: Client and Server are the two tiers. Then UI, App, and DB can be
    split across them in five ways (between the divisions, possibly with some
    software layer split between the two)

- Peer-to-peer systems
  - e.g. Chord: DHT with a ring overlay network (coool)
  - Resistant to **churn** (nodes entering and leaving)
  - Finger table: Kinda like skiplist. Allows lookup in log(n)

## 2018 May 8

- Hybrid architectures
  - Example: Bittorrent
  - Client gets .torrent file from website
  - Tracker tells you where nodes are
  - Nodes have actual data (this is the P2P part)
  - To deal with this, some police upload garbage to poison the data.

- Self-management: Use a feedback control loop to adjust system based on
  sensors

- Module 3: Processes

- Lightweight processes
  - For A1: OOM errors can actually mean that you ran out of processes.

- Multi-threaded servers
  - Often: one dispatcher thread, multiple worker threads

- Virtualization
  - Can be in a runtime environment (like JRE)
  - Can be on an OS running on top of a virtual machine monitor
  - In A2, whene prof spins up tha doop

- Types of message passing
  - Application-specific: Like Skype.
  - Application-independent: Like X Windows.

- Clusters
  - Three tiers
    - Load balancer(s)
    - Application/compute server
    - Distributed filesystem or database

## 2018 May 9

- Netcat examples.
  - nc www.uwaterloo.ca 80
  - GET / HTTP/1.0
  - GET / HTTP/1.1
    - Keeps connection open, as an HTTP/1.1 feature!
  - nc -l [-p] 10000
  - nc localhost 10000
- Kafka examples.

- Remote procedure calls.
  - Client stub: Bit of code on the client that sends the call
  - Server stub: Bit of code on the server that receives the call
  - Marshalling: Packing parameters into a message
    - Does endianness transformations
  - IDL: Interface Definition Language
  - Synchronous: Client waits for return value.
    - Like usual call center holds.
  - Asynchronous: Client doesn't wait. Instead, server calls client.
    - Like if a call center just takes your number and calls you back.
  - One-way: Client doesn't wait or care.

- Message queuing model.
  - Alternative to RPCs that use a shared scalable message queue.
  - Primitives:
    - Put: Append message to queue.
    - Get: Blocking get next message from queue.
    - Poll: Non-blocking get (option) next message from queue.
    - Notify: Install handler for when message is put into queue.

- Message-oriented middleware: Characterized by async message passing.
  - JMS: Java Message Service. Supports message queues and pubsub.

- Coupling among processes
  - Referential coupling: One process explicitly references another.
  - Temporal coupling: Communication processes must simply by running.

- RPC vs. MOM
  - RPC:
    - for two-way communication requiring a response
    - middleware linked into the client and server, no additional software
      required
    - tighter coupling
  - MOM:
    - response not required; just publishing information (like Trump's tweets)
    - middleware is a separate component between producer and consumer
    - looser coupling

- RPC demo

## 2018 May 10

- Apache Thrift.
  - Protobuf by Facebook for the entire world.
  - Software stack:
    - Server
    - Processor (compiler generated)
    - Protocol (JSON, compact, ...)
    - Transport (TCP, HTTP, ...)
  - Learn the IDL syntax
    - Types: bool, byte, i16, i32, i64, double, binary, string, void
    - Containers: list, set, map
    - Other types: const, typdef, enum, struct, exception
    - Field modifiers: required, optional, default values
    - Services and procedures: service, extends, oneway
    - Namespace: Java package (lel)
  - Field numbers can never be reused

## 2018 May 15

- Asynchronous RPC.
  - Must use non-blocking socket and transport.
  - Must provide TAsyncClientManager instance to handle callbacks.
  - Must provide a callback via interface AsyncMethodCallback.
  - May need synchronization between callback and caller, e.g. with Condition
    or CountDownLatch.

- Multi-threaded server.
  - THsHaServer: "Thrift Half Synchronous Half Asynchronous" monstrosity.
  - For assignment: Limit your worker thread pool to AT MOST 64 worker threads.
  - Server implementations:
    - `TSimpleServer`: Single thread, blocking IO
    - `TNonblockingServer`: Single thread, nonblocking IO
    - `THsHaServer`: One thread for IO, worker thread pool
    - `TThreadedSelectorServer`: IO thread pool, worker thread pool
    - `TThreadPoolServer`: One thread accepts connections, each connection has
      a dedicated thread from a pool
    - Assignment hint: Use either THsHaServer or TThreadPoolServer.
      - You probably want the fastest one... as long as it doesn't crash.

- Thrift protocols.
  - TBinaryProtocol: Straightforward binary format.
  - TCompactProtocol: Compact binary format with varints and stuff.
    - Usually the fastest, since network is slower than CPU.
  - TJSONProtocol: Uses bloated JSON format.

## 2018 May 16

- On the assignment.
  - When you're processing requests, two main strategies you can use to process
    batches.
  - Strategy #1: Send entire batch to one backend.
    - Faster/"fairer" with lots of parallel clients.
  - Strategy #2: Break up the batch.
    - Faster response time with a single threaded client.

- Quiz questions.
  - Turing Award winner for 2013: Leslie Lamport
  - Google doesn't depend on: global clock
  - Layered architecture is: more coupled than object-based
  - Vertical distribution is: adding more power to a machine
  - Hash partitioning is popular for: horizontal distribution
  - IPC is expensive because: it requires a context switch <!-- 2018 May 17 -->
  - In an RPC framework, parameters are typically: passed by value.
  - The purpose of the service handler in an RPC server is: not to call the
    server stub; the server stub calls the service handler.
  - The Intel IA-64 architecture is: Bi-endian
  - RPC is an example of: transient communication
  - What is NOT a message-oriented middleware: RPC
  - Not a keyword in Thrift IDL: synchronous
  - Java thread invoking async Thrift RPC will have callback executed in:
    different thread
  - Thrift Java server that does not use a single thread to accept network
    connections: TThreadedSelectorServer
  - TCompactProtocol uses compression to reduce the size of Thrift messages:
    False; is done by variable length encoding
  - How Thrift treats the "optional" keyword: Ignores keyword, allows caller to
    either provide or not provide the argument.
  - Non-blocking sockets are required in: Just the asynchronous client.
  - Vertical distribution is spreading different layers across multiple
    physical tiers: true.

## 5b. Thrift additional notes

- Bunch of examples

## 6. Distributed file systems

- ecelinux servers use a distributed file system. Proof: Log in and type
  `mount`. NFS is a distributed file system.

- Access models:
  - Remote access model: "reading a book at the library"
  - Upload/download model: "checking a book out from the library"
  - Often, upload/download model is preferred

- NFS
  - Network File System
  - Developed at Sun in 1984
  - e.g. ecelinux home folders are NFSv4
  - Uses client-side caching
  - Changes are flushed on file close
  - Consistency handling is implementation-dependent
  - NFSv4 lets the server delegate authority
  - Uses RPCs internally
  - Problem: Not that great at large files
    - Striping: splitting files into multiple chunks

<!-- 2018 May 23 -->

- Semantics of file sharing
  - UNIX semantics:
    - Every operation immediately visible to everyone.
  - Session semantics:
    - Changes visible to others only after file is closed.
  - Immutable files:
    - No updates are possible.
  - Transactions:
    - ATOMICITY

<!-- 2018 May 24 -->

- GFS
  - Google File System
  - Master:
    - Coordinates several Chunk Servers
    - Stores metadata about files and chunks
    - Polls chunk servers to keep metadata consistent
  - To read, the Master redirects the client to the appropriate chunk server
  - To write, there's cool pipelining involving primary and secondary replicas

- Quiz!
  - The advantage of upload/download model is: Lower latency in write-intensive
    workloads.
  - "Striping" is: Placing chunks of the same file on different servers
  - In GFS, a client reads data by: reading metadata from the master and data
    from the chunk servers
  - In GFS, a client writes data by: writing to one replica, which chains it to
    other replicas
  - GFS vs HDFS: Files are mutable in GFS, append-only in HDFS

## 7. MapReduce (part a)

- History: Lambda calculus
- Google: GFS: '03, MapReduce: '04
- Hadoop: 2005 open source version of MapReduce created by Doug Cutting (mostly
  implemented by Doug Cafarella)

<!-- 2018 May 29 --> <!-- Missed first 15m -->

- Stages of MapReduce:
  - Map: Takes input, typically from a distributed file system, and emits keyed
    output values.
  - Shuffle
  - Sort
  - Reduce: Takes mapper's output values with a common key and produces a
    single output value.

- Example: Word count
  - mapper(filename, file-contents): for word in file-contents: emit (word, 1)
  - reducer(word, values): emit (word, sum(value for value in nvalues))

<!-- 2018 May 30 --> <!-- Missed first 3m -->

- Mapper example code
  - Hadoop MapReduce string tokenizer may behave differently by default from
    that of Spark! BE WARNED.

- Reducer example code
  - If you print to stdout in your mapper, you don't actually see it in your
    console! It get sent to logs, since your mapper code isn't actually running
    on the master node.

- Driver example code

- How to run on ecelinux
  - Use `hadoop classpath` to get the classpath
  - Hint: To complete assignment 2, you need to understand
    `job.setNumReduceTasks()`

- Demo
  - `part-r-00000`: r is for reducer. m is for mapper.
  - Cool little exercise:
    - Take the driver, and experiment with things like
      - What if we don't use a combiner?
      - What if I don't use a reducer?
        - Hadoop uses the identity reducer by default
  - We will get a DADOOP cluster. Will cost the school about $300/day
    - And if you start working on the second assignment two days before the
      deadline, you're going to die.
    - There will be no extensions because we will run out of money at some
      point and we'll have to share the cluster down.

## 7. MapReduce b: Programming patterns

- Counting terms
  - Simplest solution:
    - Mapper: emit (term, 1) for all terms
    - Reducer: emit (term, sum(counts))
  - Can also do a bit of counting in the Mapper:
    - Mapper: accumulate counts by words, then emit (term, counts[term]) for
      all terms
      - Can run out of memory
  - Alternatively, use combiner that's identical to the reducer

- Selection (i.e. Filter)
  - Return input elements that satisfy some predicate
  - Mapper: key -> t -> if t satisfies predicate then emit(t, null)

- Projection
  - Return subset of fields
  - Mapper: key -> t -> emit(project t, null)
  - Reducer: t -> n -> emit(t, null) # n is an array of nulls
    - The reducer eliminates duplicates

<!-- 2018 May 31 -->

- Inverted index
  - Aside: I have no idea why they call this "inverted"; it's literally just an
    index
  - Produce a mapping from term to document ID
  - Mapper :: id: docid -> d: doc -> emit(t, id) for t: term in d
  - Reducer :: t: term -> docids: docid list -> emit(t, docids)

- Thunderstorm: Stay safe. Bad things happen to good people.

- Cross-correlation
  - Set of tuples. For each pair of items, calculate the number of tuples where
    these items co-occur.
  - Pairs approach:
    - Slower, simpler
    - Mapper :: () -> items -> emit((i, j), count 1) for i in items for j in
      items if j > i
    - Reducer :: (i, j) -> counts -> emit((i, j), sum(counts))
  - Stripes approach:
    - Faster, more complex, more memory on mapper
    - Mapper :: () -> items -> for i in items: H = map from item to count for j
      in items if j > i: h[j] += 1 emit(i, H)
    - Reducer :: i -> stripes -> H = mconcat stripes for (k,v) in H:
      emit((i,k), v)
  - TODO: Exercise: do cross-correlation assuming a combiner is used.
  - Stripes is better because it permits more combining.

<!-- 2018 Jun 5 -->

- MapReduce is kinda bad:
  - Extraordinarily verbose

- (talking about Hadoop jobs) Other students at other universities aren't so
  fortunate; they don't have J-O-Bs

## 8a. Apache Spark

- Spark Paper: Cluster computing frameworks like MapReduce and Dryad are shit
  because it's hard to reuse intermediate results.
- You can use Spark with Scala, which is baller.

- RDD lineage
  - Spark keeps track of how an RDD was derived from other RDDs in case a job
    or machine fails.

- Narrow vs. wide dependencies
  - Narrow: One output looks at O(1) inputs.
    - e.g. map, filter, union, zip (join with inputs co-partitioned)
  - Wide: Every output looks at every input.
    - e.g. groupByKey, join with inputs not co-partitioned)

- Demo:
  - THERE IS A SPARK SHELL HOLY SHIT

<!-- 2018 Jun 6 -->

- PageRank
  - Column-stochastic matrix
  - Random surfer interpretation
  - One way: many iterations
  - Another way: quantum random surfer
  - This just computes the eigenvector
  - Frobenius theorem: Yes eigenvalue with eigenvector exists
- Scala example code

<!-- 2018 Jun 7 -->

- In A2, one of the tasks only requires a mapper. Careful not to introduce a
  shuffle here.
- Ways to partition your RDD
  - Arbitrarily. e.g. when loading raw data from files.
  - By key. After grouping or reducing by key.
- `RDD1.join(RDD2)` may be a narrow dependency (if inputs are co-partitioned)
  or a wide dependency (if inputs are not)

- Hadoop has only one advantage over Spark. You will discover that once you're
  done A2.

## 8b. Big graph processing

- PageRank damping
  - Stops oscillations.
  - Ensure convergence to a meaningful PageRank vector by ensuring that the
    transition matrix has all non-zero elements.
  - Give every transition an α chance to jump to a uniform random site.

<!-- 2018 Jun 12 -->

- Google: Pregel.
  - Vertex partitions, with vertex-centric computations
  - Computation divided into supersteps driven by the master.
  - Workers communicate only betwen supersteps.
  - Superstep consists of:
    - Workers run user-defined function
    - Vertices receive messages sent in the previous superstep
    - Vertices send messages to be received in the next superstep
    - Vertices modfiy their value, modify their edges' values, add/remove edges
    - Vertices deactivate itself (vote to halt)
    - Vertices reactive inactive vertices by sending messages to it
  - Example: find max vertex label
    - How many supersteps are required? (max length path from max node)

- In A2: Don't use job.setJarByClass, use job.setJar.

- Dirty joke: Big Data is like teenage sex

<!-- 2018 Jun 13 -->

- The framework tradeoff: Using a distributed computing framework can make your
  code scalable and easier to understand, but it can limit its efficiency
- Example of Pregel code for PageRank (C++)

```cpp
class PageRankVertex : public Vertex<double, void, double> {
  public:
    virtual void Compute(MessageIterator *msgs) {
      if (superstep() >= 1) {
        double sum = 0;
        for (; !msgs->Done(); msgs->Next()) {
          sum += msgs->Value();
        }
        *MutableValue() = 0.15 / NumVertices() + 0.85 * sum;
      }
      if (superstep() < 30) {
        const int64 n = GetOutEdgeIterator().size();
        SendMessageToAllNeighbors(GetValue() / n);
      } else {
        VoteToHalt();
      }
    }
};
```

- Example: Single source shortest paths.
  - Is label propagation.
  - Initialize start node to 0, other nodes to infinity.
  - Propagated values are current value + cost of edge.
  - Update value with min of received values.
  - Propagate only if received value is lower than your current value.

```cpp
class ShortestPathVertex : public Vertex<int, int, int> {
  void Compute(MessageIterator* msgs) {
    int mindist = IsSource(vertex_id()) ? 0 : INF;
    for (; !msgs->Done(); msgs->Next()) {
      mindist = min(mindist, msgs->Value());
    }
    if (mindist < GetValue()) {
      *MutableValue() = mindist;
      OutEdgeIterator iter = GetOutEdgeIterator();
      for (; !iter.Done(); iter.Next()) {
        SendMessageTo(iter.Target(), mindist + iter.GetValue());
      }
    }
    VoteToHalt();
  }
};
```

- Pregel Combiners
  - Must be commutative and associative.
  - They combine messages sent from one partition to another, exchanging
    network I/O for CPU.

- Pregel Aggregators
  - Computes aggregate stats from vertex-reported values.
  - Can be used to evaluate convergence, e.g. in PageRank.

```cpp
class MinIntCombiner : public Combiner<int> {
  virtual void Combine(MessageIterator *msgs) {
    int mindist = INF;
    for (; !msgs->Done(); msgs->Next()) {
      mindist = min(mindist, msgs->Value());
    }
    Output("combined_source", mindist);
  }
};
```

<!-- 2018 Jun 14 -->

- Quiz!
  - MapReduce emphasizes: Components can't share data arbitrarily, and data
    elements are immutable.
  - The MapReduce combiner is run: Between the mapper and the shuffle.
  - There is one map task per: Input split.
  - Hadoop restarts task when: JobTracker loses contact with a task's
    TaskTracker.
  - Problem that does not benefit from use of combiners: Selection.
  - Cross-correlation can be solved more efficiently using stripes than pairs
    because: Mapper output can be combined more effectively. (Shorter key)
  - Spark: Difference between cache() and persist() is: persist() uses one of
    several storage levels, while cache() only uses the default storage level
    (memory).
  - Spark: Recording lineage of RDDs allows: Reconstruction of RDD following
    failure, AND analysis to optimize execution.
  - Spark: Most useful function for word counting: reduceByKey
  - Scala lambdas are things
  - Performance difference between Spark and MapReduce is in: iterative
    computations.
    - Spark is actually slower for very simple things.
    - But Spark is faster for iterative computations.
  - ["A B C", "B C D"] to ["A", "B", "C", "B", "C", "D"] via: flatMap
  - [("A", (1,2)), ("B", (2,3))] to [("A",1), ("B",2), ...] via: flatMapValues

- Use the cluster to test.
  - Grading will run on the cluster.
  - Some things may work differently on the cluster.
  - NO JOBS require multiple mapreduce jobs.
  - For Task 4, make the ENTIRE FILE available to ALL tasks. Use distributed
    cache in Hadoop, broadcast in Spark.
    - Google: Map-side join.

<!-- 2018 Jun 19 -->

## 9a. Consistency and replication

- Consistency models
  - Sequential consistency
  - Causal consistency
  - Linearizability
  - Eventual consistency
  - Session guarantees

- Reasons to replicate
  - Increase reliability
  - Increase throughput
  - Decrease latency

- Examples of replicators
  - GFS, HDFS: Replicate data (harder)
  - MapReduce: Replicate math (easier)
  - MySQL: Replicate data (very hard! shared mutable state)

- Why replicating data is hard
  - Hard to define correctness (lol I troll u)

- Sequential consistency
  - Operations: read, write
  - "The result is the same as if all operations were executed in some order,
    and the operations of an individual process aren't reordered."

- Causal consistency
  - Weaker than sequential consistency.
  - "A write that causally precedes another must be seen by all processes in the
    same order. Concurrent writes may be seen in a different order on different
    machines."
    - Op1 causally precedes Op2 if Op1 occurs before Op2 in a process.
    - Op2 causally precedes Op2 if Op2 reads a value written by Op1.
    - Op1 precedes Op2 precedes Op3 -> Op1 precedes Op3.

<!-- 2018 Jun 20 -->

- Causal-but-not-sequential consistency is possible if the data is stored
  across multiple machines, any of which can handle a query.

- Linearizability
  - Stronger than sequential consistency.
  - Originally a correctness property for concurrent data structures
  - Every operation additionally has a well-defined start and finish time
  - "The result is the same as if all operations were executed in some order
    that extends Lamport's 'happens before' relation."
  - Happens before: "If Op1 finishes before Op2 begins, then Op1 must precede
    Op2 in the sequential order."
  - Linearizability algorithm:
    - (will be used to test the correctness of our A3 solution!!)
    - Something something make sure there are no cycles (TODO)

- Eventual consistency
  - Textbook:   With no updates,    all replicas will gradually  become consistent.
  - Doug Terry: With no NEW WRITES, all SERVERS  will EVENTUALLY HOLD THE SAME DATA.

<!-- 2018 Jun 21 -->

- Session guarantees
  - Used to augment eventual consistency
  - Monotonic reads: Successive reads never return an older value.

## 9b. Consistency and replication (part 2)

- Primary-based replication protocols
  - All updates are executed by a designated primary replica, and pushed to
    backup replicas
  - Classifications:
    - Remote-write: Primary replica is stationary, must be updated remotely by other servers
    - Local-write: Primary replica migrates from server to server, allowing local updates
    - TODO: Review examples
  - Failure detection needed to prevent split brain

- How do you tell if a server is down?
  - You can't! Any absence of communication could be due to the network.
  - However, you can typically assume that the network is reliable enough.

- Quorum-based replication protocols
  - Primary replica is a perfomrance bottleneck
  - Assume that e.g. 3/5 of the servers will always be up
  - Write quorum and read quorum
  - Partial quorums:
    - e.g. Dynamo, Cassandra, Voldemort, Riak
    - NR + NW > N: Strong consistency
    - NR + NW <= N: Weak consistency

---

- Eventually-consistent replication protocols

- Anti-entropy: MERKLE TREES, or HASH TREES
