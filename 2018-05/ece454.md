# ECE 454: Distributed Computing.

Professor: Wojciech Golab.

## 2018 May 1

- Why build distributed systems?
  - [[Resource sharing saves money.]]
  - [[Simplifies business processes.]]
  - [[Scales beyond what a centralized system can.]]
  - [[Users may be distributed.]]

- Anecdote: Lord Kelvin vs. Edward Orange Wildman Whitehouse.
  - Entrepreneur Whitehouse fried transatlantic cable by pumping voltage,
    against engineer Kelvin's warning.

- Middleware: Offers a single-system view between applications and operating
  systems on different machines.
  - Common middleware services
    - [[Communication (e.g. add job to remote queue)]]
    - [[Transactions (e.g. access independent services atomically)]]
    - [[Service composition (e.g. Google map with weather forecast)]]
    - [[Reliability (e.g. replicated state machine)]]

- Goals of distributed systems:
  - Support resource sharing
  - Make distribution transparent (7 types... memorize them)
    - Access: representing and accessing resources
    - Location: putting resources somewhere
    - Migration: moving resources
    - Relocation: moving resources while in use
    - Replication: replicating resources
    - Concurrency: sharing resources
    - Failure: failing and recovering resources
  - Be open, that is:
    - [[Interopable]]
    - [[Composable]]
    - [[Extensible]]
    - [[Policy separate from mechanism]]
  - Be scalable
    - [[Size]]
    - [[Geography]]
    - [[Administration]]
    - Avoid common mistakes that limit scalability:
      - Centralized services (e.g. a single server)
      - Centralized data (e.g. a single telephone book database)
      - Centralized algorithms (e.g. routing based on complete information)

## 2018 May 2

- Types of distribution transparency:
  - Access
  - Location
  - Relocation
  - Migration
  - Replication
  - Concurrency
  - Failure

- (2/3)c is the fastest light goes

- Scaling techniques:
  - Partitioning and distribution: Splitting stuff up
    - Example: Original DNS, which is a tree with zones
  - Replication: Having multiple copies of a thing
    - Example: memcached to speed up web apps
    - Can also have look-aside caching (look at cache and storage in parallel)

- Falsehoods programmers believe about distro
  - The network is reliable
  - The network is secure
  - The network is homogeneous
  - The topology does not change
  - Latency is zero
  - Bandwidth is infinite
  - Transport cost is zero
  - There is one administrator
- "Recall n of these" was on a previous exam

- Types of distributed systems
  - Website, web services
  - High performance computing (HPC)
    - Multiprocessor vs. multicomputer
    - Cluster computing
      - There is a master node that coordinates
    - Grid computing
    - Cloud computing
  - Transaction processing
  - Enterprise application integration
  - Internet of things (distributed pervasive systems)
  - Sensor networks
    - In-network data processing: When the sensors calculate things themselves
      rather than shunting data directly to the operator
    - Example: smart irrigation

- Shared memory vs. message passing
  - Shared memory is parallel computing
  - Message passing is distributed computing
  - Some theorists say they're both distributed computing

- Big Data: In one minute, many many things are happening.

## 2018 May 3

- Architecture (triggered)
  - ... is Components connected by Connectors

- Architectural styles (fuck)
  - layered
  - object-based (e.g. Enterprise Java Beans)
  - data-centred
  - event-based (e.g. Kafka)

- Multi-tiered architectures
  - Example: Client and Server are the two tiers. Then UI, App, and DB can be
    split across them in five ways (between the divisions, possibly with some
    software layer split between the two)

- Peer-to-peer systems
  - e.g. Chord: DHT with a ring overlay network (coool)
  - Resistant to **churn** (nodes entering and leaving)
  - Finger table: Kinda like skiplist. Allows lookup in log(n)

## 2018 May 8

- Hybrid architectures
  - Example: Bittorrent
  - Client gets .torrent file from website
  - Tracker tells you where nodes are
  - Nodes have actual data (this is the P2P part)
  - To deal with this, some police upload garbage to poison the data.

- Self-management: Use a feedback control loop to adjust system based on
  sensors

- Module 3: Processes

- Lightweight processes
  - For A1: OOM errors can actually mean that you ran out of processes.

- Multi-threaded servers
  - Often: one dispatcher thread, multiple worker threads

- Virtualization
  - Can be in a runtime environment (like JRE)
  - Can be on an OS running on top of a virtual machine monitor
  - In A2, whene prof spins up tha doop

- Types of message passing
  - Application-specific: Like Skype.
  - Application-independent: Like X Windows.

- Clusters
  - Three tiers
    - Load balancer(s)
    - Application/compute server
    - Distributed filesystem or database

## 2018 May 9

- Netcat examples.
  - nc www.uwaterloo.ca 80
  - GET / HTTP/1.0
  - GET / HTTP/1.1
    - Keeps connection open, as an HTTP/1.1 feature!
  - nc -l [-p] 10000
  - nc localhost 10000
- Kafka examples.

- Remote procedure calls.
  - Client stub: Bit of code on the client that sends the call
  - Server stub: Bit of code on the server that receives the call
  - Marshalling: Packing parameters into a message
    - Does endianness transformations
  - IDL: Interface Definition Language
  - Synchronous: Client waits for return value.
    - Like usual call center holds.
  - Asynchronous: Client doesn't wait. Instead, server calls client.
    - Like if a call center just takes your number and calls you back.
  - One-way: Client doesn't wait or care.

- Message queuing model.
  - Alternative to RPCs that use a shared scalable message queue.
  - Primitives:
    - Put: Append message to queue.
    - Get: Blocking get next message from queue.
    - Poll: Non-blocking get (option) next message from queue.
    - Notify: Install handler for when message is put into queue.

- Message-oriented middleware: Characterized by async message passing.
  - JMS: Java Message Service. Supports message queues and pubsub.

- Coupling among processes
  - Referential coupling: One process explicitly references another.
  - Temporal coupling: Communication processes must simply by running.

- RPC vs. MOM
  - RPC:
    - for two-way communication requiring a response
    - middleware linked into the client and server, no additional software
      required
    - tighter coupling
  - MOM:
    - response not required; just publishing information (like Trump's tweets)
    - middleware is a separate component between producer and consumer
    - looser coupling

- RPC demo

## 2018 May 10

- Apache Thrift.
  - Protobuf by Facebook for the entire world.
  - Software stack:
    - Server
    - Processor (compiler generated)
    - Protocol (JSON, compact, ...)
    - Transport (TCP, HTTP, ...)
  - Learn the IDL syntax
    - Types: bool, byte, i16, i32, i64, double, binary, string, void
    - Containers: list, set, map
    - Other types: const, typdef, enum, struct, exception
    - Field modifiers: required, optional, default values
    - Services and procedures: service, extends, oneway
    - Namespace: Java package (lel)
  - Field numbers can never be reused

## 2018 May 15

- Asynchronous RPC.
  - Must use non-blocking socket and transport.
  - Must provide TAsyncClientManager instance to handle callbacks.
  - Must provide a callback via interface AsyncMethodCallback.
  - May need synchronization between callback and caller, e.g. with Condition
    or CountDownLatch.

- Multi-threaded server.
  - THsHaServer: "Thrift Half Synchronous Half Asynchronous" monstrosity.
  - For assignment: Limit your worker thread pool to AT MOST 64 worker threads.
  - Server implementations:
    - `TSimpleServer`: Single thread, blocking IO
    - `TNonblockingServer`: Single thread, nonblocking IO
    - `THsHaServer`: One thread for IO, worker thread pool
    - `TThreadedSelectorServer`: IO thread pool, worker thread pool
    - `TThreadPoolServer`: One thread accepts connections, each connection has
      a dedicated thread from a pool
    - Assignment hint: Use either THsHaServer or TThreadPoolServer.
      - You probably want the fastest one... as long as it doesn't crash.

- Thrift protocols.
  - TBinaryProtocol: Straightforward binary format.
  - TCompactProtocol: Compact binary format with varints and stuff.
    - Usually the fastest, since network is slower than CPU.
  - TJSONProtocol: Uses bloated JSON format.

## 2018 May 16

- On the assignment.
  - When you're processing requests, two main strategies you can use to process
    batches.
  - Strategy #1: Send entire batch to one backend.
    - Faster/"fairer" with lots of parallel clients.
  - Strategy #2: Break up the batch.
    - Faster response time with a single threaded client.

- Quiz questions.
  - Turing Award winner for 2013: Leslie Lamport
  - Google doesn't depend on: global clock
  - Layered architecture is: more coupled than object-based
  - Vertical distribution is: adding more power to a machine
  - Hash partitioning is popular for: horizontal distribution
  - IPC is expensive because: it requires a context switch <!-- 2018 May 17 -->
  - In an RPC framework, parameters are typically: passed by value.
  - The purpose of the service handler in an RPC server is: not to call the
    server stub; the server stub calls the service handler.
  - The Intel IA-64 architecture is: Bi-endian
  - RPC is an example of: transient communication
  - What is NOT a message-oriented middleware: RPC
  - Not a keyword in Thrift IDL: synchronous
  - Java thread invoking async Thrift RPC will have callback executed in:
    different thread
  - Thrift Java server that does not use a single thread to accept network
    connections: TThreadedSelectorServer
  - TCompactProtocol uses compression to reduce the size of Thrift messages:
    False; is done by variable length encoding
  - How Thrift treats the "optional" keyword: Ignores keyword, allows caller to
    either provide or not provide the argument.
  - Non-blocking sockets are required in: Just the asynchronous client.
  - Vertical distribution is spreading different layers across multiple
    physical tiers: true.

## 5b. Thrift additional notes

- Bunch of examples

## 6. Distributed file systems

- ecelinux servers use a distributed file system. Proof: Log in and type
  `mount`. NFS is a distributed file system.

- Access models:
  - Remote access model: "reading a book at the library"
  - Upload/download model: "checking a book out from the library"
  - Often, upload/download model is preferred

- NFS
  - Network File System
  - Developed at Sun in 1984
  - e.g. ecelinux home folders are NFSv4
  - Uses client-side caching
  - Changes are flushed on file close
  - Consistency handling is implementation-dependent
  - NFSv4 lets the server delegate authority
  - Uses RPCs internally
  - Problem: Not that great at large files
    - Striping: splitting files into multiple chunks

<!-- 2018 May 23 -->

- Semantics of file sharing
  - UNIX semantics:
    - Every operation immediately visible to everyone.
  - Session semantics:
    - Changes visible to others only after file is closed.
  - Immutable files:
    - No updates are possible.
  - Transactions:
    - ATOMICITY

<!-- 2018 May 24 -->

- GFS
  - Google File System
  - Master:
    - Coordinates several Chunk Servers
    - Stores metadata about files and chunks
    - Polls chunk servers to keep metadata consistent
  - To read, the Master redirects the client to the appropriate chunk server
  - To write, there's cool pipelining involving primary and secondary replicas

- Quiz!
  - The advantage of upload/download model is: Lower latency in write-intensive
    workloads.
  - "Striping" is: Placing chunks of the same file on different servers
  - In GFS, a client reads data by: reading metadata from the master and data
    from the chunk servers
  - In GFS, a client writes data by: writing to one replica, which chains it to
    other replicas
  - GFS vs HDFS: Files are mutable in GFS, append-only in HDFS

## 7. MapReduce (part a)

- History: Lambda calculus
- Google: GFS: '03, MapReduce: '04
- Hadoop: 2005 open source version of MapReduce created by Doug Cutting (mostly
  implemented by Doug Cafarella)

<!-- 2018 May 29 --> <!-- Missed first 15m -->

- Stages of MapReduce:
  - Map: Takes input, typically from a distributed file system, and emits keyed
    output values.
  - Shuffle
  - Sort
  - Reduce: Takes mapper's output values with a common key and produces a
    single output value.

- Example: Word count
  - mapper(filename, file-contents): for word in file-contents: emit (word, 1)
  - reducer(word, values): emit (word, sum(value for value in nvalues))

<!-- 2018 May 30 --> <!-- Missed first 3m -->

- Mapper example code
  - Hadoop MapReduce string tokenizer may behave differently by default from
    that of Spark! BE WARNED.

- Reducer example code
  - If you print to stdout in your mapper, you don't actually see it in your
    console! It get sent to logs, since your mapper code isn't actually running
    on the master node.

- Driver example code

- How to run on ecelinux
  - Use `hadoop classpath` to get the classpath
  - Hint: To complete assignment 2, you need to understand
    `job.setNumReduceTasks()`

- Demo
  - `part-r-00000`: r is for reducer. m is for mapper.
  - Cool little exercise:
    - Take the driver, and experiment with things like
      - What if we don't use a combiner?
      - What if I don't use a reducer?
        - Hadoop uses the identity reducer by default
  - We will get a DADOOP cluster. Will cost the school about $300/day
    - And if you start working on the second assignment two days before the
      deadline, you're going to die.
    - There will be no extensions because we will run out of money at some
      point and we'll have to share the cluster down.

## 7. MapReduce b: Programming patterns

- Counting terms
  - Simplest solution:
    - Mapper: emit (term, 1) for all terms
    - Reducer: emit (term, sum(counts))
  - Can also do a bit of counting in the Mapper:
    - Mapper: accumulate counts by words, then emit (term, counts[term]) for
      all terms
      - Can run out of memory
  - Alternatively, use combiner that's identical to the reducer

- Selection (i.e. Filter)
  - Return input elements that satisfy some predicate
  - Mapper: key -> t -> if t satisfies predicate then emit(t, null)

- Projection
  - Return subset of fields
  - Mapper: key -> t -> emit(project t, null)
  - Reducer: t -> n -> emit(t, null) # n is an array of nulls
    - The reducer eliminates duplicates

<!-- 2018 May 31 -->

- Inverted index
  - Aside: I have no idea why they call this "inverted"; it's literally just an
    index
  - Produce a mapping from term to document ID
  - Mapper :: id: docid -> d: doc -> emit(t, id) for t: term in d
  - Reducer :: t: term -> docids: docid list -> emit(t, docids)

- Thunderstorm: Stay safe. Bad things happen to good people.

- Cross-correlation
  - Set of tuples. For each pair of items, calculate the number of tuples where
    these items co-occur.
  - Pairs approach:
    - Slower, simpler
    - Mapper :: () -> items -> emit((i, j), count 1) for i in items for j in
      items if j > i
    - Reducer :: (i, j) -> counts -> emit((i, j), sum(counts))
  - Stripes approach:
    - Faster, more complex, more memory on mapper
    - Mapper :: () -> items -> for i in items: H = map from item to count for j
      in items if j > i: h[j] += 1 emit(i, H)
    - Reducer :: i -> stripes -> H = mconcat stripes for (k,v) in H:
      emit((i,k), v)
  - TODO: Exercise: do cross-correlation assuming a combiner is used.
  - Stripes is better because it permits more combining.

<!-- 2018 Jun 5 -->

- MapReduce is kinda bad:
  - Extraordinarily verbose

- (talking about Hadoop jobs) Other students at other universities aren't so
  fortunate; they don't have J-O-Bs

## 8a. Apache Spark

- Spark Paper: Cluster computing frameworks like MapReduce and Dryad are shit
  because it's hard to reuse intermediate results.
- You can use Spark with Scala, which is baller.

- RDD lineage
  - Spark keeps track of how an RDD was derived from other RDDs in case a job
    or machine fails.

- Narrow vs. wide dependencies
  - Narrow: One output looks at O(1) inputs.
    - e.g. map, filter, union, zip (join with inputs co-partitioned)
  - Wide: Every output looks at every input.
    - e.g. groupByKey, join with inputs not co-partitioned)

- Demo:
  - THERE IS A SPARK SHELL HOLY SHIT

<!-- 2018 Jun 6 -->

- PageRank
  - Column-stochastic matrix
  - Random surfer interpretation
  - One way: many iterations
  - Another way: quantum random surfer
  - This just computes the eigenvector
  - Frobenius theorem: Yes eigenvalue with eigenvector exists
- Scala example code

<!-- 2018 Jun 7 -->

- In A2, one of the tasks only requires a mapper. Careful not to introduce a
  shuffle here.
- Ways to partition your RDD
  - Arbitrarily. e.g. when loading raw data from files.
  - By key. After grouping or reducing by key.
- `RDD1.join(RDD2)` may be a narrow dependency (if inputs are co-partitioned)
  or a wide dependency (if inputs are not)

- Hadoop has only one advantage over Spark. You will discover that once you're
  done A2.

## 8b. Big graph processing

- PageRank damping
  - Stops oscillations.
  - Ensure convergence to a meaningful PageRank vector by ensuring that the
    transition matrix has all non-zero elements.
  - Give every transition an α chance to jump to a uniform random site.

<!-- 2018 Jun 12 -->

- Google: Pregel.
  - Vertex partitions, with vertex-centric computations
  - Computation divided into supersteps driven by the master.
  - Workers communicate only betwen supersteps.
  - Superstep consists of:
    - Workers run user-defined function
    - Vertices receive messages sent in the previous superstep
    - Vertices send messages to be received in the next superstep
    - Vertices modfiy their value, modify their edges' values, add/remove edges
    - Vertices deactivate itself (vote to halt)
    - Vertices reactive inactive vertices by sending messages to it
  - Example: find max vertex label
    - How many supersteps are required? (max length path from max node)

- In A2: Don't use job.setJarByClass, use job.setJar.

- Dirty joke: Big Data is like teenage sex

<!-- 2018 Jun 13 -->

- The framework tradeoff: Using a distributed computing framework can make your
  code scalable and easier to understand, but it can limit its efficiency
- Example of Pregel code for PageRank (C++)

```cpp
class PageRankVertex : public Vertex<double, void, double> {
  public:
    virtual void Compute(MessageIterator *msgs) {
      if (superstep() >= 1) {
        double sum = 0;
        for (; !msgs->Done(); msgs->Next()) {
          sum += msgs->Value();
        }
        *MutableValue() = 0.15 / NumVertices() + 0.85 * sum;
      }
      if (superstep() < 30) {
        const int64 n = GetOutEdgeIterator().size();
        SendMessageToAllNeighbors(GetValue() / n);
      } else {
        VoteToHalt();
      }
    }
};
```

- Example: Single source shortest paths.
  - Is label propagation.
  - Initialize start node to 0, other nodes to infinity.
  - Propagated values are current value + cost of edge.
  - Update value with min of received values.
  - Propagate only if received value is lower than your current value.

```cpp
class ShortestPathVertex : public Vertex<int, int, int> {
  void Compute(MessageIterator* msgs) {
    int mindist = IsSource(vertex_id()) ? 0 : INF;
    for (; !msgs->Done(); msgs->Next()) {
      mindist = min(mindist, msgs->Value());
    }
    if (mindist < GetValue()) {
      *MutableValue() = mindist;
      OutEdgeIterator iter = GetOutEdgeIterator();
      for (; !iter.Done(); iter.Next()) {
        SendMessageTo(iter.Target(), mindist + iter.GetValue());
      }
    }
    VoteToHalt();
  }
};
```

- Pregel Combiners
  - Must be commutative and associative.
  - They combine messages sent from one partition to another, exchanging
    network I/O for CPU.

- Pregel Aggregators
  - Computes aggregate stats from vertex-reported values.
  - Can be used to evaluate convergence, e.g. in PageRank.

```cpp
class MinIntCombiner : public Combiner<int> {
  virtual void Combine(MessageIterator *msgs) {
    int mindist = INF;
    for (; !msgs->Done(); msgs->Next()) {
      mindist = min(mindist, msgs->Value());
    }
    Output("combined_source", mindist);
  }
};
```

<!-- 2018 Jun 14 -->

- Quiz!
  - MapReduce emphasizes: Components can't share data arbitrarily, and data
    elements are immutable.
  - The MapReduce combiner is run: Between the mapper and the shuffle.
  - There is one map task per: Input split.
  - Hadoop restarts task when: JobTracker loses contact with a task's
    TaskTracker.
  - Problem that does not benefit from use of combiners: Selection.
  - Cross-correlation can be solved more efficiently using stripes than pairs
    because: Mapper output can be combined more effectively. (Shorter key)
  - Spark: Difference between cache() and persist() is: persist() uses one of
    several storage levels, while cache() only uses the default storage level
    (memory).
  - Spark: Recording lineage of RDDs allows: Reconstruction of RDD following
    failure, AND analysis to optimize execution.
  - Spark: Most useful function for word counting: reduceByKey
  - Scala lambdas are things
  - Performance difference between Spark and MapReduce is in: iterative
    computations.
    - Spark is actually slower for very simple things.
    - But Spark is faster for iterative computations.
  - ["A B C", "B C D"] to ["A", "B", "C", "B", "C", "D"] via: flatMap
  - [("A", (1,2)), ("B", (2,3))] to [("A",1), ("B",2), ...] via: flatMapValues

- Use the cluster to test.
  - Grading will run on the cluster.
  - Some things may work differently on the cluster.
  - NO JOBS require multiple mapreduce jobs.
  - For Task 4, make the ENTIRE FILE available to ALL tasks. Use distributed
    cache in Hadoop, broadcast in Spark.
    - Google: Map-side join.

<!-- 2018 Jun 19 -->

## 9a. Consistency and replication

- Consistency models
  - Sequential consistency
  - Causal consistency
  - Linearizability
  - Eventual consistency
  - Session guarantees

- Reasons to replicate
  - Increase reliability
  - Increase throughput
  - Decrease latency

- Examples of replicators
  - GFS, HDFS: Replicate data (harder)
  - MapReduce: Replicate math (easier)
  - MySQL: Replicate data (very hard! shared mutable state)

- Why replicating data is hard
  - Hard to define correctness (lol I troll u)

- Sequential consistency
  - Operations: read, write
  - "The result is the same as if all operations were executed in some order,
    and the operations of an individual process aren't reordered."

- Causal consistency
  - Weaker than sequential consistency.
  - "A write that causally precedes another must be seen by all processes in the
    same order. Concurrent writes may be seen in a different order on different
    machines."
    - Op1 causally precedes Op2 if Op1 occurs before Op2 in a process.
    - Op2 causally precedes Op2 if Op2 reads a value written by Op1.
    - Op1 precedes Op2 precedes Op3 -> Op1 precedes Op3.

<!-- 2018 Jun 20 -->

- Causal-but-not-sequential consistency is possible if the data is stored
  across multiple machines, any of which can handle a query.

- Linearizability
  - Stronger than sequential consistency.
  - Originally a correctness property for concurrent data structures
  - Every operation additionally has a well-defined start and finish time
  - "The result is the same as if all operations were executed in some order
    that extends Lamport's 'happens before' relation."
  - Happens before: "If Op1 finishes before Op2 begins, then Op1 must precede
    Op2 in the sequential order."
  - Linearizability algorithm:
    - (will be used to test the correctness of our A3 solution!!)
    - Something something make sure there are no cycles (TODO)

- Eventual consistency
  - Textbook:   With no updates,    all replicas will gradually  become consistent.
  - Doug Terry: With no NEW WRITES, all SERVERS  will EVENTUALLY HOLD THE SAME DATA.

<!-- 2018 Jun 21 -->

- Session guarantees
  - Used to augment eventual consistency
  - Monotonic reads: Successive reads never return an older value.

## 9b. Consistency and replication (part 2)

- Primary-based replication protocols
  - All updates are executed by a designated primary replica, and pushed to
    backup replicas
  - Classifications:
    - Remote-write: Primary replica is stationary, must be updated remotely by other servers
    - Local-write: Primary replica migrates from server to server, allowing local updates
    - TODO: Review examples
  - Failure detection needed to prevent split brain

- How do you tell if a server is down?
  - You can't! Any absence of communication could be due to the network.
  - However, you can typically assume that the network is reliable enough.

- Quorum-based replication protocols
  - Primary replica is a perfomrance bottleneck
  - Assume that e.g. 3/5 of the servers will always be up
  - Write quorum and read quorum
  - Partial quorums:
    - e.g. Dynamo, Cassandra, Voldemort, Riak
    - NR + NW > N: Strong consistency
    - NR + NW <= N: Weak consistency

---

- Eventually-consistent replication protocols

- Anti-entropy: MERKLE TREES, or HASH TREES

---

<!-- 2018 Jun 26 -->

## 10a. Fault tolerance

- Dependability is:
  - Availability (percentage of time up)
  - Reliability (high MTBF: mean time between failures)
  - Safety (things don't hurt people ever)
  - Maintainability (you can fix things, low MTTR)

- Fault: Physical underlying cause.
  - e.g. bugs, misconfiguration, natural disaster, failing hardware
  - Three types:
    - Transient. They happen and they disappear. e.g. bird flying in path of
      your communication laser
    - Intermittent. Happens randomly every once in a while. e.g. concurrency
    - Permanent. Happens consistently forever until fixed. e.g. power supply
      burned out
- Error: Bad state.
- Failure: System does not fulfill its promises.
  - Five types:
    - Crash
    - Omission
      - Receive omission
      - Send omission
    - Timing
    - Response
      - Value failure
      - State transition failure
    - Arbitrary (Byzantine failure, not covered)

- Masking failures
  - The solution is redundant "identical" programs.
  - Hardware example: Triple Modular Redundancy
  - Software example
    - Flat group: all N choose 2 connections exist.
    - Hierarchical group: Leader at the top.

- Consensus problem
  - Each process proposes a value, then learns what value was determined
  - Safety property 1: Agreement. Two calls to decide() never return different values.
  - Safety property 2: Validity. decide() has to return something someone propose()d.
  - Liveness property: propose() and decide() eventually terminate if they do not fail.

<!-- 2018 Jun 27 -->

- Variations on the consensus problem
  - Sync vs. async processes
  - Communication delays
  - Message delivery order
  - Unicast vs. multicast messaging

- Possibilities and impossibilities
  - Given process behavior, communication delay, message ordering, message
    transmission, the only things that are possible (sum of minterms) are
    - Synchronous, bounded
    - Synchronous, ordered
    - Multicast
    - All others are impossible.

- TODO: rest of these slides

## 10b. Zookeeper

- Google Chubby: Uses Paxos, invented by Leslie Lamport.
  - Solves consensus in an ok way.
  - Best with few writes, MANY reads
  - Zookeeper is a rip of this.

- Theorems
  - FLP impossibility result: consensus never certain.
    - By Fischer, Lynch, Paterson
  - CAP: consistency, availability, partition tolerance, pick 2
    - By Eric Brewer

- Applications
  - Group membership
  - Master election
  - Dynamic configuration
  - Status monitoring
  - Queueing
  - Barriers, critical sections

- Consistency guarantees
  - Linearizable writes
  - Sequentially consistent reads
  - Client FIFO ordering

- Change events
  - Clients can subscribe to changes

- ZNodes
  - Ephemeral: Deleted automatically when creator fails
  - Sequence: Monotonically increasing counter

- Joke
  - How many people does it take to run a data center?
  - It takes a person and a dog. The dog makes sure no one gets in, and the
    person feeds the dog.

<!-- 2018 Jun 28 -->

- Example: Configuration
  - Workers get the config, either on startup or when notified of change:
    - getData(".../config/settings", true)
  - Administrators set the config:
    - setData(".../config/settings", newConf, -1)

- Example: Group membership
  - Workers register themselves on startup:
    - create(".../workers/workerName", hostInfo, EPHEMERAL)
  - Workers list group members
    - listChildren(".../workers", true)

- Example: Leader election done right (for all of us)
  - Step 1: Check if there is already a leader.
    - getData(".../workers/leader", true)
    - Step 2: If successful, follow the leader and exit.
  - Step 3: Try to become leader.
    - create(".../workers/leader", hostname, EPHEMERAL)
    - Step 4: If successful, then lead and exit.
  - Step 5: Go to Step 1.
  - Can theoretically loop for a very long time
  - In practice, usually only takes a few loops

- TODO: FINAL EXAM: You need to be able to write ZooKeeper recipe for solving
  the consensus problem. Not in the slides. i.e. How to implement propose() and
  decide().
  - No timeouts, quorums

- Example: Queue lock
  - Fair: Everyone eventually enters the critical section
  - Step 1: Get in line.
    - id = create(".../locks/x-", SEQUENCE | EPHEMERAL)
  - Step 2: Check if you're the first person in line.
    - getChildren(".../locks", false)
    - Step 3: If your id is the first child, then exit.
  - Step 4: Check if the person before you exists.
    - exists(name of znode of last child before id, true)
    - Step 5: If it doesn't exist, go to Step 2.
  - Step 6: Wait for notification.
  - Step 7: Go to Step 2.

- Example: Shared lock
  - Slide 20

<!-- 2018 Jul 3 -->

- Assignment 3.
  - You don't have to write much code... but it has to be damn careful code.
  - In past years, A3 grades have been quite a bit lower than A1 and A2.
  - Setup:
    - You are building a distributed key-value store using Zookeeper.
    - Gets and puts do NOT go through ZK; would be very bad performance. ZK
      only for coordination.
    - Client get data:
      - Goes to master, goes to backup, only returns to client after backup
        confirms data (???)
    - Grading script will kill -9 your servers every single-digit-seconds
  - You also need concurrency control:
    - Two clients can be doing simultaneous puts.
    - Not enough to use ConcurrentHashMap in the nodes individually. Must
      implement locking per key-value pair ON TOP OF your hashmaps. Can do
      reader-writer locks or exclusive locks; R-W probably faster.
    - HINT: Do lock-free gets via clever optimization
  - You also need to implement SOMETHING ELSE:
    - Primary goes down
    - Client goes back to ZK to get the new primary
    - New primary is running without a secondary.
    - Your code should not break. Should continue to work.
    - New backup node comes up.
    - Primary must discover there is a new backup.
      - Otherwise, you will lose your only copy of the hashtable.
    - New backup must receive a copy of the entire hashtable.
      - Cannot just wait for new updates.
      - May run into Thrift message size limits. Must change some settings
        to get around this.
    - Your code should not deadlock. It will probably deadlock.
  - Your solution should do 100k gets and puts.

- ZooKeeper servers.
  - Use Paxos or Raft or something.
  - 2f+1 servers can handle f failures.

- ZooKeeper crossing over.
  - With a pure-write workload, adding servers reduces throughput because each
    update has to reach a majority of servers.
  - With a pure-read workload, adding servers increases throughput because
    reads can be done in parallel.
  - The result is that a graph of %reads vs. ops/sec with various numbers of
    servers has crossover.

- Composition
  - People don't compose: two happy people can become unhappy when they enter a relationship.
  - Two fault-tolerant systems may no longer be fault-tolerant when put together.
  - Engineering: Spot welding.

- Five failure scenarios for RPCs
  - 1. Client cannot locate server
  - 2. Request is lost
  - 3. Server crashes during request
  - 4. Reply is lost
  - 5. Client crashes after sending request
  - Client cannot tell the difference between 2,3,4

- Actions and acknowledgements
  - M: Server replies to client with acknowledgement
  - P: Server prints the text
  - C: Server crashes
  - Any ordering of MP has a sour position for C.
  - Client reissue strategies:
    - 
  - What is the solution? Make sure your RPC actions are idempotent.
    - e.g. Mr. Smith suffers from amnesia. Can't just keep calling him to take
      pills; must number pills, and call Mr. Smith and tell him to take a
      particular number.

<!-- 2018 Jul 4 -->

## 9b. Consistency and replication part 2, part 2

- Primary-backup replication
  - Updates:
    - 1. Client submits update to primary
    - 2. Primary takes exclusive lock on data
    - 3. Primary writes to backup (no lock at backup)
    - 4. Backup ACKs the primary
    - 5. Primary releases the lock
    - 6. Primary ACKs the client
    - Total: 2 round-trips
  - Reads:
    - 1. Client submits request to primary
    - 2. Primary takes shared lock on data, reads it
    - 3. Primary releases shared lock on data
    - 4. Primary returns result to client
    - Total: 1 round-trip

- Quorum (ACID database)
  - ACID: You care very much about resolving read/write conflicts.
  - Updates:
    - 1. Client locks a majority of servers
    - 2. Servers take a lock on the requested data
    - 3. Servers send ack to the client
    - 4. Client completes transaction by sending data to all servers
    - 5. Servers update the data and release the lock
    - 6. Servers send ack to the client
    - Total: 2 round-trips
  - In order to be linearizable, we need a majority quorum:
    - 1. NR + NW > N
    - 2. NW + NW > N

- Quorum (NoSQL key-value store)
  - Puts:
    - 1. The client sends the data + timestamp to all servers
    - 2. Servers uses the latest update as authoritative
      - Called "last writer wins"
    - 3. Servers ack
    - 4. Client waits for NW acks
  - Gets:
    - 1. Client reads from NR servers, uses value with latest timestamp
    - NOT linearizable
  - Problems:
    - If a write fails part-way through, some servers won't be updated

- Anti-entropy
  - Uses Merkle Trees (hash trees)
    - How Mercurical works
    - How Bitcoin works

- Assignment

<!-- 2018 Jul 5 -->

- So far, we've been building fault-tolerant systems using other fault-tolerant
  systems.
- How do we build a fault-tolerant system from scratch?
  - Recall ACID. For C, I, and D, we can use locks. for Atomicity, we need
    something else.

- Caymen Islands
  - How to get the money from Canada to Caymen Islands?
    - Option 1:
      - 1. Withdraw
      - 2. Deposit
      - If you fail halfway through, the money disappears
    - Option 2:
      - 1. Deposit
      - 2. Withdraw
      - If you fail halfway through, money is created
    - So, the withdrawal and deposit together must be atomic.

- 2PC: Two-phase commit
  - Consists of a Coordinator and multiple Participants
  - Steps:
    - 1. Vote-Request to all participants
    - 2. Vote-Commit comes back from all participants
    - 3. Global-Commit to all participants
      - Or, if any participant does Vote-Abort, Global-Abort
    - 4. ACK comes from all participants

- 2PC state diagrams
  - Coordinator
    - 1. INIT: Upon receiving Commit, send Vote-Request and enter state WAIT.
    - 2. WAIT:
      - Upon receiving Vote-Abort, send Global-Abort and enter state ABORT.
      - Upon receiving Vote-Commit, send Global-Commit and enter state COMMIT.
    - 3. ABORT
    - 4. COMMIT
  - Participant
    - 1. INIT: Upon receiving Vote-Request, either
      - Send Vote-Abort and enter state ABORT, or
      - Send Vote-Commit and enter state READY
    - 2. READY:
      - Upon receiving Global-Abort, send ACK and enter state ABORT
      - Upon receiving Global-Commit, send ACK and enter state COMMIT
    - 3. ABORT
    - 4. COMMIT

- 2PC errors
  - Participant failure
    - A participant may not respond to the Vote-Request.
    - Coordinator times out and does a Global-Abort.
  - Coordinator failure
    - Suppose the coordinator fails after participants cast their votes.
    - Silly version (textbook):
      - Wait forever. Protocol gets stuck
    - Smart version:
      - Participants do a Decision-Request amongst'd've themselves
      - If all other participants voted for commit, unstuck yourself
      - (Seems bad???)
  - FINAL: Just know silly textbook version of the 2PC protocol.
  - Spanner uses 2PC.

- Jim Gray
  - Did 2PC
  - Disappeared after a short sailing trip

- Checkpointing
  - Each participant takes checkpoints every once in a while (e.g. after every message received)
  - You can only recover if the collection of checkpoints taken by processes
    form a distributed snapshot
  - Recovery line: Most recent distributed snapshot

<!-- 2018 Jul 10 -->

- Distributed snapshots are ordered by "every checkpoint is at least as recent
  in one snapshot as the other".

- Coordinated checkpointing algorithm
  - Takes a distributed snapshot with a correct recovery line
  - Phase 1: Coordinator sends `CHECKPOINT_REQUEST` to all processes
    - Processes stop sending messages, and take a local checkpoint, upon
      receiving this
  - Phase 2: Coordinator sends `CHECKPOINT_DONE` to all processes
    - Processes resume processing incoming messages

- Quiz!
  - Sequential consistency ensures all write operations observed in the same
    order. → True
  - P1 W(x)a, P2 W(x) b, P3 R(x)b, P3 R(x)a sequentially consistent. → True
  - It is causally consistent. → True. Causal is weaker than sequential.
  - If W1 is before W2, causal consistency requires that nobody observe the
    other order. → False. "Happens before" is a linearizability thing.
  - P1 W(x)a, P2 R(x)a W(x)b, P3 R(x)b R(x)a is sequentially consistent. →
    False.
  - It is causally consistent. → False.
  - To be continued tomorrow.

<!-- 2018 Jul 11 -->

## Raft

- Consensus protocol built at Stanford
- Designed for understandability

- Naive consensus:
  - Just use majority vote!
    - Quorum of 3
    - 1 server votes 1, 1 server votes 0, 1 server... crashes?!
    - no majority here
    - Can't just assume 1. Crashed server may have already intended to vote 0.

- User study results of Raft vs. Paxos
  - Raft PROVEN easier to implement and explain with DATA

- Raft
  - Used to be an assignment; has been replaced with Kafka
  - Modelled as a replicated state machine
  - A consensus module controls a replicated log, which are events in the state
    machine

- Raft decomposition
  - 1. Leader election
    - Choose leader
    - Choose new leader on crashes
  - 2. Log replication
    - Leader accepts commands from clients
    - Leader replicates its log to other servers, overwriting inconsistencies
  - 3. Safety
    - Keep logs consistent
    - Server can become leader only if it have up-to-date logs

- Server states and RPCs
  - Follower (start)
    - No heartbeat -> Candidate
    - Passive, expects regular heartbeats from leader
  - Candidate
    - win election -> Leader
    - else -> Follower
    - RPCs: RequestVote
  - Leader
    - discover higher term -> Follower
    - RPCs: AppendEntries

- Terms
  - At most 1 leader per term
  - May have no leader in a term, if election fails
  - Each server maintains the current term value
    - Exchanged in every RPC
  - Terms identify obsolete information
  - Each term starts with "overhead" (leader election), followed by normal
    operation

- Leader election
  - 1. Become candidate
  - 2. currentTerm++, vote for self
  - 3. Send RequestVote RPCs to other servers
  - 4. If vote form majority:
    - Become leader and send heartbeats
  - 5. If RPC from leader:
    - Become follower
  - 6. If timeout:
    - Go back to 2.

- Election correctness
  - Safety: Only one winner per term, since majority is required to win
    election.
  - Liveness: Some candidate must eventually win, since election timeouts are
    randomly chosen from [T, 2T]; one server usually times out and wins
    election before others time out.

- Normal operation
  - Client sends command to leader
  - Leader appends command to its log
  - Leader sends AppendEntries RPCs to all followers
  - Once new entry committed:
    - Leader executes command in state machine
    - Leader notifies followers of committed entries in subsequent AppendEntries
    - Followers execute committed commands in their state machines
  - With crashed/slow followers:
    - Retry AppendEntries until success
  - Optimal performance: One successful RPC to any majority of servers

- Log structure
  - Must survive crashes (store on disk)
  - Entries are committed iff they are safe to execute in state machines
    - i.e. are replicated on a majority of servers by the leader of its term

- Log inconsistencies
  - Crashes can result in log inconsistencies
  - Raft minimizes special code for repairing inconsistencies:
    - Leader assumes its log is correct
    - Normal operation will repair all inconsistencies

- Log matching property
  - If log entries on different servers have the same index and term, then they
    store the same command and the logs are identical in all preceding entries.
  - If a given entry is committed, all preceding entries are also committed.
  
- AppendEntries consistency check
  - AppendEntries RPCs include the index and term of entries preceding the new
    ones.
  - If a follower thinks it doesn't match, it rejects the request and the
    leader retries with a lower log index.
  - This is inductive.

- Safety: Leader completeness.
  - Candidates include the index and term of their last log entry in their RequestVote RPCs.
  - Voters deny vote if its log is more up-to-date.
  - Logs are lexico ranked by (lastTerm, lastIndex) tuple.

- Raft evaluation
  - Formal proof of safety
    - Ongaro dissertation
    - UW mechanically checked proof (50 klines)

<!-- 2018 Jul 12 -->
<!-- Missed first 40m of class. -->

## 11. Kafka

- Named after Franz Kafka
- Pubsub system

- Topic: Stream of records, stored as a partitioned log

- Producer:
  - Pushes records
  - Picks the partition to contact for a given topic
  - Can batch records, send to broker asynchronously
    - Better throughput
    - Slight latency cost
  - Can do idempotent delivery

- Consumer
  - Pulls records from Kafka broker
  - Can do "exactly once" semantics

- Producer and consumer code examples
- Streams example
- Word count example

- Record stream vs. changelog stream
  - Two broad ways to interpret a stream
  - Record stream: Every record is a state transition
  - Changelog stream: Every record is a state
  - In Kafka Streams API:
    - KStream: Record stream
    - KTable: Changelog stream

- Stream-table duality
  - A changelog stream record overwrites a table row
  - A table is a snapshot for the latest value of each key in a changelog
    stream

- Converting between KStream and KTable
  - KStream
    - groupBy, groupByKey -> KGroupedStream
  - KGroupedStream
    - count, reduce, aggregate -> KTable
    - aggregate is a generalization of reduce, apparently
  - KTable
    - toStream -> KStream

- Windowed streams
  - Hopping time windows
    - Every *hop*, compute over the last *size*
    - Hop windows may overlap, may also have gaps
  - Tumbling time windows
    - Hopping window with hop = size
  - Sliding windows
    - Slide continuously over time axis
  - Session windows
    - Aggregate by period of activity, thresholding on period of inactivity

<!-- 2018 Jul 17 -->
<!-- Missed first 17m of class. -->
<!-- TODO: Review Kafka stuff. -->

## 12. Clocks

- Clocks should be synchronized, otherwise weird things can happen (e.g.
  compiled object file "created before" source code)

- Calendars
  - Roman Calendar
    - Based on moon phases, so months of 29-30 days
    - 10 months per year = 304 days. 61 winter days unaccounted for
    - Reformed by adding two more months per year, occasional intercalary month
    - Still difficult to align to seasons
  - Julian Calendar
    - Since 45 BCE
    - Solar calendar, based on Earth's rotation around the sun
    - Leap year every three years, then every four
  - Gregorian Calendar
    - Since 1582
    - Leap years calculated more carefully (every 4 years, except every 100,
      except every 400)
    - Drift of 1 day per 7700 years (wrt vernal equinox)

- Timekeeping standards
  - Solar day: Not constant
  - TAI (Temps Atomique International): Based on average of multiple Cesium 133
    atomic clocks
  - UTC: Based on TAI, adjusted using leap seconds whenever discrepancy gets to
    800ms

- Demonstration of relativistic time dilation
  - "I'm moving so it either goes faster or slower, I forget which"

- Limitations of atomic clocks
  - Hafele-Keating experiment: 1971, confirmed Einstein's relativity wrt time
    dilation. Four atomics clocks flown around in the wolrd in opposite
    directions, and compared against clocks that stayed on the ground. Both
    clocks lost time due to gravitational time dilation. Eastbound clock lost
    time, westbound clock gained time. Difference more than 200ns.

- Textbook definitions of clocks
  - Let C(t) be the value for time recorded by clock C.
  - Clock skew: dC/dt - 1
  - Offset: C(t) - t
  - Maximum drift rate: ρ such that 1-ρ ≤ dC/dT ≤ 1+ρ

- NTP: Network Time Protocol
  - Client at host A (T₁) polls server (T₂) at host B, then (T₃) receives a
    response (T₄), with one-way delay δ.
  - We have:
    - T₂ = T₁ + δ + θ
    - T₄ = T₃ + δ - θ
  - Solving for θ:
    - θ = ((T₂ - T₁) + (T₃ - T₄)) / 2
  - Estimate δ as:
    - δ = ((T₄ - T₁) - (T₃ - T₂)) / 2
  - NTP collects multiple (θ, δ) pairs and uses the minimum value of δ as the
    estimate of the delay, and the corresponding θ as the most reliable offset
    estimate.
  - Failure mode: Asymmetric network delays
  - Typical performance: within 10s of ms from stratum 0
  - PTP: Precision Time Protocol. 100ns accuracy using hardware timestamping.

- Assignment 4
  - The idea: Real-time processing system for classroom occupancy
    - Trying to find classrooms whose occupancy exceeds their limit, which is a
      fire code violation

<!-- 2018 Jul 18 -->

- Lamport clocks
  - Define "happens before" relation (→) on pairs of events
  - a→b if
    - a occurs before b in the same process, or
    - a is the send event and b is the receive event for some message
    - ∃ c . a→c and c→b
  - Provides a partial order of events
  - a and b are concurrent if neither a→b nor b→a
  - Allows the generation of logical timestamps (t: event → timestamp) for
    events by updating counters at each process.
  - If a→b, then t(a) < t(b). If t(a) < t(b), then a→b or a and b are
    concurrent.

- Vector clocks
  - Each process Pi has a vector VCi such that:
    - VCi[i] is the number of events that have happened at Pi.
      - i.e. the local logical clock at Pi
    - If VCi[j] = k, then Pi knows that k events have occurred at Pj.
      - i.e. Pi's knowledge of local time at Pj
  - Algorithm for updating clock
    - When Pi sends message m to Pj:
      - VCi[i]++
      - ts(m) = VCi
    - When Pj receives message m:
      - VCj[k] := max(VCj[k], ts(m)[k]) for each k
      - VCj[j]++
      - Deliver message to application
  - Partial order
    - i happens before j if ∀ k | VCi[k] ≤ VCj[k], and strict for some k
    - i and j are concurrent if neither happens before the other

<!-- 2018 Jul 19 -->

## 13. CAP principle

- Myth 1: Conventional DBs don't scale.
  - You can scale up or out with enough money. (Just not write scale lol)
- Myth 2: Transactions don't scale.
  - No need for single bottleneck. Can use multi-master replication, partitioning.
  - Example: Spanner, F1.
- Myth 3: Scalability means high latency.

- Brewer's conjecture
  - It's impossible to have all three of:
    - Consistency
      - Clients agree on latest state of data
    - Availability
      - Clients can execute queries and updates
    - Partition tolerance
      - System continues to function if network fails and nodes are separated
        into disjoint sets
  - Naive interpretation: "Pick two of the three, and you're good to go!"
  - In reality, partitions can happen any time!! So it really means:
  - "If a partition happens, you have to choose between C or A."

- CP consistency models:
  - Serializability
  - Linearizability
  - Sequential consistency
  - NR + NW > N

- AP consistency models:
  - Eventual consistency
  - Causal consistency

- AP systems
  - Useful for latency-sensitive, inconsistency-tolerant apps
  - Mostly get/put, no transactions
  - Used by Amazon (Dynamo)

- PACELC:
  - By Dr. Daniel Abadi of Yale
  - "If there is P, choose between A and C. Else, choose between Latency and
    Consistency"

- How to build an AP system
  - Wrong way 1: Take a CP system and remove C.
    - You might just end up with a P system
  - Wrong way 2: Use quorums with NR=1, NW=1, N=3
    - May still not have availability, since a partition can be created where
      you have no access to the owner of a key
  - Right way: Hinted handoff and sloppy quorums
    - Use NR=1, NW=1, N=3+?

- Cassandra
  - Made at Facebook

- On Tuesday: Final exam takeup.
  - TA will tell you everything except how to solve consensus using ZooKeeper.

<!-- 2018 Jul 24 -->

- Tutorial TA covers lecture.
- Taking up practice final.

1.
a.) Layer 7.
b.) i32. fooproc(1: list<string> arg1) throws (1: IllegalArgument ia)
c.) TNonblockingServer
d.) Hopping and tumbling (sliding, session)
e.) `set my_key 0 60 6\nmy_val`

2.
a.) theta = 1ms
b.) It depends; they can be concurrent or e1 before e2.
c.) t1 = [A:1, B:2, C:3] t2 = [A:3, B:2, C:x], x ≥ 3
d.) Gravitational and kinematic
e.) Universal Coordinated Time; Temps Universel Coordonné

3.
a.) Yes. Total order: P1.w(x)a P2w(y)a P3R(y)a P3R(x)a P2W(x)b P4R(x)b P4R(y)a
b.) No. They are incomparable. One is safety property; other is liveness property.
c.) Monotonic reads
d.) Session semantics.

4.
One possible solution is:
```scala
val records = textFile.map(line=>line.split(","))
records.cartesian(records) // compute pairs
  .filter(x => x._1(0) < x._2(0)) // eliminate duplicates
  .map(x => (x._1(0), x._2(0), x._1.drop(1), x._2.drop(1))) // extract friend list
  .map(x => (x._1, x._2, x._3.intersect(x._4).length))
  .map(x => x._1 + "," + x._2 + "," + x._3)
  .saveAsTextFile(outpath)
```

5.
a.)
Formula: 0.5 × 1/4 + 0.5 × sum of incoming contributions
V1: 0.31, V2: 0.17, V3: 0.29, V4: 0.23
b.)
Initial state:
  - V4 has value 0
  - All other vertices have value infinity
Messages:
  - V4 sends "1" to V1 and V3
  - Optional: Other vertices send infinity
End of superstep 1:
  - V4 has value 0
  - V1,3 have value 1
c.) When it receives a message.

6.
a.) Recovery line is the *most recent* distributed snapshot.
b.) TODO. States are Init, Wait, Abort, Commit. Transitions are
Commit/Vote-Request, Vote-Abort/Global-Abort, Vote-Commit/Global-Commit.
c.) Yes. Upon recv vote-request, one participant can transition directly to
Abort while a second participant has not yet received a vote request and
remains in Init.

7.
a.) No. Server requires 3 votes to become a leader, but only 2 servers remain.
b.) 0 or 1. 0 if election fails; no more than 1 leader possible.
c.) Reliable/Replicated/Redundant and Fault-Tolerant

8.
a.) Take a specific numbered pill.
b.) Linearizable writes and serializable reads
c.) THIS WILL BE ON THE EXAM AND IS HOMEWORK AND TOP SECRET

9.
a.)
No. Implementation can violate linearizability as follows:
- Primary receives input, updates local map
- Primary receives get on the same key, returns new value
- Primary crashes
- Backup becomes new primary, receives get on key, returns stale value
b.) WRITE ANY (ANY)
c.) It can be CP or AP depending on the choice of client-side consistency level
d.) PACELC: if Partition, then choose Availability or Consistency. else, choose
Latency or Consistency.

10.
a.)
ONE for put, ALL for get: Avg latency = 0.9×0ms + 0.1×40ms = 4ms
b.) At most 10 RPCs/sec
