# CS 486: Intro to AI

Professor: Peter van Beek

## 2017 May 02

- What is intelligence?
  - Remembering, reasoning, planning, solving, learning, adapting
  - Understanding other intelligences and beat them
  - Understanding oneself deeply?

- Thinking is symbolic reasoning.
  - Recall Turing machines, lambda calculus
  - Can humans compute functions that are not Turing computable? >> open
    question, probably not

- Newell-Simon hypothesis/conjecture:
  - A physical symbol system suffices for intelligence.

- Four categories of AI definitions:
  - Systems that (think | act) (like humans | rationally)

- Thinking rationally
  - "Laws of thought" approach: logic and probability as normative theories.
  - "Rational agent" approach: agent does best thing to achieve its goals.

- Lots of AI applications

## 2018 May 07

- Problem solving using search
  - Find a goal state given constraints on the goal.
  - Find a sequence of actions that leads to the goal state.

- Example: n queens on an nxn board; no threats
- Example: crossword puzzles
- Example: sliding puzzles
- Example: river crossing puzzle with 100kg capacity boat, 100kg parent, two
  50kg children
- Example: propositional satisfiability (NP complete)
- Example: partition problem (equalize weights) (NP complete)
- Example: travelling saleswoman problem
- Example: set covering (minimum size committee with all skills)

- Formulate problem solving as graph search.
  - Nodes are states.
  - Edges are actions.
  - Solution is a path from initial state to goal state.
  - May have cost on edges.

- For river crossing puzzle:
  - States are assignments of {boat, parent, child1, child2} to {L, R}
    - 16 states
  - Edges are valid river crossings

- For sliding puzzles:
  - States are permutations of {1, ..., 8, empty}
    - 9! states
  - Edges are valid slides

- For n-queens:
  - States are assignments of {Q1, ..., Qn} to row index (assume Qi is in
    column i)
  - Initial state is assignment of Qi to i
  - Edges are swapping two queens

## 2018 May 09

- Skipped in favour of mental health seminar

- General search algorithm is
  - Start with list of start nodes
  - Until it's empty:
    - Pop a node, and if it isn't the goal, push successor states

- Effect of different queues
  - LIFO queue: Breadth first search
  - FIFO queue: Depth-first search
  - Priority queue: Informed search

- Distance function: 
  - g(n) is cost from initial state to n that was found
  - h(n) is heuristic estimate of cost from n to a goal
  - f(n) = g(n) + h(n) is the heuristic estimate of the total cost of the
    solution obtained down this path
  - if `h(n) <= h*(n)` for all n, then h(n) is admissible
    - with admissible h(n), `A*` will find an optimal path
  - if `h(n) <= cost(n, n') + h(n') for all n, n' a successor to n`, then h(n)
    is consistent
    - with consistent h(n), `A*` will find an optimal path
  - dominating heuristics: h2(n) dominates h1(n) if for all nodes n, h2(n) >=
    h1(n), and there's some node n' where h2(n) > h1(n').
    - thm: If h2(n) dominates h1(n), then `A*` with h1(n) expands at least as
      many node as with h2(n)

- Greedy search: Informed search with f(n) = h(n)
- Dijkstra: Informed search with f(n) = g(n)
- `A*`: Informed search with f(n) + h(n) + g(n)

- Facts about `A*`
  - Is complete, optimal, optimally efficient
    - No algorithm with the same information can do better!
  - Assumes a single goal
  - Has exponential time complexity `O(b^(εd))`, where
    - ε is the maximum relative error `1 - h(n) / h*(n)`
    - b is the branching factor
    - d is the depth of the goal node

- Iterative-deepening `A*`
  - No priority queue; instead, each iteration is a complete DFS that is cut
    off if f(n) exceeds some threshold

## Constraint satisfaction problems <!-- 2018 May 14 -->

- You have
  - A set of variables { x1, ..., xn }
  - A set of values for each variable dom(x1), ..., dom(xn)
  - A set of constraints { C1, ..., Cm }
- And want
  - An assignment to variables that satisfies the constraints
    - ... whether or not there is one?
    - ... any one?
    - ... all of them?
    - ... the optimal one, given some cost function?

- Example domains and constraints
  - Reals -- linear constraints: Gaussian elimination, linear programming
  - Integers -- linear constraints: Integer linear programming,
    branch-and-bound
  - Booleans -- propositional statements
  - Here, we will use finite domains with expressive constraint languages

- Constraint languages
  - Arithmetic operators
  - Logical operators
  - Global constraints: over an arbitrary number of variables
  - Table constraints: enumerate satisfied assignments

- Alldifferent
  - A global constraint over a set of variables that is satisfied iff all
    variables are assigned a different value

- Examples
  - Sudoku:
    - 81 variables x1, ..., x81 for each grid square
    - Rows: alldifferent(x1, ..., x9)
    - Columns: alldifferent(x1, x10, ..., x73)
    - Squares: alldifferent(x1, x2, x3, x10, ..., x21)
    - Given values
  - n-queens: Variables are x1, ..., x4; xi is the row of the queen in column i
  - Crosswords: Variables are squares, domain is latin alphabet

### Constraint propagation

- Detail on constraints
  - An assignment is x = a, where a in dom
  - A tuple t over a list of variables {x1, ..., xk} is a list of values (a1,
    ..., ak)
    - like a set of assignments
  - In a tuple t, t[xi] is the value for variable xi.
  - A constraint C defined over vars(C) specifies the allowed combination of
    values for the variables in vars(C).
  - vars(C) is the **scope** or **scheme** of the constraint
  - len(vars(C)) is the **arity** of the constraint
    - Unary constraint: 1
    - Binary constraint: 2
    - Non-binary: > 2
  - In a binary CSP, all constraints are binary.

- Intensional vs. extensional
  - Intensional is implicit. Like "x1 != x2 and |x1 - x2| != 1"
  - Extensional is explicit. Like {(1,3), (1,4), (2,4), (3,1), (4,1), (4,2)}.
    - AKA the table constraint.

- Local consistency: Arc consistency
  - Given a constraint, remove a value from the domain of a variable if it
    cannot be part of a solution according to that constriant.
  - Formally:
    - Let C be a constraint
    - Domain support:
      - Let x in vars(C)
      - Let a in dom(x)
      - x has a domain support in C if there exists t in C such that t[x] = a
        and t[y] in dom(y) for every y in vars(C).
    - C is arc consistent iff for all x in vars(C), every value a in dom(x) has
      a domain support in C.
      - i.e. if it's possible that x = a can solve the CSP.
  - A CSP is arc consistent if every constraint is arc consistent
  - You can make a CSP arc consistent by repeatedly removing unsupported values
    from the domains.

- Arc consistency algorithm
  - ac: Q:(variable * constraint) list -> boolean
    - while Q is not empty:
      - pop (x, C) from Q
      - if revise(x, C):
        - if dom(x) is empty: return false
        - else: add pairs to Q
    - return true
  - revise: x:variable -> C:constraint -> boolean
    - change = false
    - for a:value in dom(x):
      - if there's no domain support for a in C:
        - remove a from dom(x)
        - change = true
    - return change

<!-- 2018 May 16 -->

### Backtracking search

- Backtracking search: DFS of search tree

- Algorithm template:
  - backtrack(assignment, csp):
    - if assignment is complete then solution found
    - var <- select next variable
    - for val in dom(var):
      - save(csp)
      - add var = val to assignment
      - if propagate(assignment, csp):
        - backtrack(assignment, csp)
      - restore(csp)

- Backtracking algorithms:
  - Naive backtracking (BT)
    - No constraint propagation
    - Backtrack in time
  - Forward checking (FC)
    - Maintain arc consistency on all constraints with exactly one
      uninstantiated variable
    - Backtrack in time
  - Maintaining arc consistency (MAC):
    - Maintain arc consistency on all constraints with at least one
      uninstantiated variable
    - Backtrack in time
  - MAC is the best. Only use FC if MAC is guaranteed to not help at all.

### Local search

- Local search
  - Does both satisfaction and optimization problems
  - No guaranteed that a solution will be found, even if it exists
  - Cannot find a provably optimal solution
  - Finds locally optimal solutions, not necessarily globally optimal

- Notation
  - S := set of states
  - c : S -> R := cost function
  - N : S -> 2^S := neighborhood function

- Optimality
  - Globally optimal: solution `s*` in S with `c(s*) <= c(s)` for all s in S.
  - Locally optimal: solution `s+` in S with `c(s+) <= c(s)` for all s in
    `N(s+)`.

- For CSPs
  - Consider some constraints hard (must be satisfied); others soft (cost
    function +1 for each unsatisfied constraint)

- Algorithm template:
  - s = some initial complete assignment
  - k = 0
  - do
    - r = neighbor of s
    - if c(r) - c(s) < tk then
      - s = r
    - k += 1
  - until stopping criteria satisfied
  - return best s

- Stopping criteria
  - Maximum # of iterations
  - Solution of low enough cost
  - Number of iterations since last (big enough) improvement too large

- Choices
  - Initial feasible solution
    - Can be random, or "good" according to some heuristic
  - Neighborhood function
    - Small neighborhood: Easily explored, low quality solutions
    - Large neighborhood: Expensive
  - Selecting r
    - First improvement (pick first improving neighbor)
    - Best improvement (go through all neighbors and pick best one)

- Thresholds
  - Iterative improvement
  - Threshold accepting
    - Accept worse cost neighbors with diminishing threshold
    - Variation: simulated annealing. (accept worse neighbors with a gradually
      decreasing probability)
    - Variation: tabu search. (accept worse neighbors based on a list of legal
      neighbors)

- Improvements
  - Multi-starts: Try multiple initial solutions
  - Multi-level: Use different neighborhoods throughout

- Example: Neighborhoods for 8-queens
  - Transpose: swap adjacent queens. O(n) neighbors
  - Insert: move a queen, displacing queens in between. O(n^2) neighbors
  - Swap: swap two queens. O(n^2) neighbors
  - Block insert: move a subsequence of queens. O(n^3) neighbors.

<!-- 2018 May 22 --> <!-- Missed a bit due to photo session. -->

- Example: Results for TSP
  - Theoretical results
    - Exact neighborhood: When every local optimum is also a global optimum
      - Must be exponential in size, unless P = NP
    - Non-exact neighborhoods
      - Cost of local optimum can be arbitrarily far from global optimum :(
      - Local search can still take exponential number of steps to reach a
        local optimum
  - Empirical results
    - Best local search algorithms get within 1.5-2.5% of optimal
    - Can do a million cities in under an hour

- Example: Local search for TSP
  - Nodes are permutations of cities
  - Cost is cost of tour
  - Neighborhood function 2-opt:
    - Delete two edges from the tour to break it into two paths, then reconnect
      in all possible ways

- Example: Local search for SAT
  - GSAT: Keeping flipping the boolean variable that minimizes the number of
    unsatisfied clauses, do so maxTries times

- Genetic algorithms
  - Create a population of solutions, assign fitness according to how good the
    solution is, breed solutions randomly weighted by goodness, possibly mutate
    their children, and pick the fittest individual

<!-- 2018 May 23 -->

## Reasoning under uncertainty

- Types of uncertainty:
  - Partial observability: incomplete knowledge of world
  - Nondeterminism: actions have random consequences

- Random variable: is a thing

- Joint probability distribution
  - Assignment of a probability to each atomic event for a set of random
    variables
  - Atomic event is cross product of domains of your random variables

- Holmes scenario:  
  - A: Alarm goes off
  - B: Burglary in progress
  - R: Radio runs report
  - W: Watson calls saying alarm is going
  - G: Gibson calls saying alarm is going

- Important rules:
  - Product rule: P(X,Y) = P(X|Y)P(Y) = P(Y|X)P(X)
  - Sum rule: P(X=a) = sum over b in dom(Y) of P(X=a|Y=b)P(Y=b)
  - Bayes rule: P(Y|X) = P(Y)P(X|Y)/P(X)
  - Chain rule: P(X1,...,Xn) = product for i from 1 to n of P(Xi|Xi-1,...,X1)

- Independence:
  - X is ind. of Y if P(X|Y) = P(X).
  - Implies P(Y|X) = P(Y).

- Examples of (non)independence
  - Watson is independent of Gibbon? No

<!-- 2018 May 28 -->

- Conditional independence:
  - X is cond. ind. of Y given Z if P(X|Y,Z) = P(X|Z).
  - Can swap X and Y.
  - Allows simplification of chain rule:
    - P(X1, ..., Xn) = ... = P(X1), if X2, ..., Xn cond. ind. of X1.

- Bayesian network
  - DAG
  - Nodes are random variables
  - Directed edges are "influence"
  - Each node has a conditional probability table of the effect of parents on
    the node
  - Two ways to understand them semantically:
    - As a representation of the entire joint probability distribution
    - As an encoding of conditional independence assumptions
  - Correct only if each node is conditionally independent of its predecessors
    in the node ordering, given its parents
    - In practice, you'll need to look at many orderings
    - For us, any ordering works

- Example: Bayesian network for burglary scenario
  - B, E -> A
  - E -> R
  - A -> G, W
  - Assumptions made: P(W|A) = P(W|A,B) = P(W|A,...)
  - From "disease" to "symptom"
  - 16 probabilities

- Another one (drop R)
  - W -> G, A
  - G -> A
  - A -> B, E
  - B -> E
  - Using formula for network:
    - P(W,G,A,B,E) = P(W)P(G|W)P(A|W,G)P(B|A)P(E|B,A)
  - Using chain rule:
    - P(W)P(G|W)P(A|W,G)P(B|W,G,A)P(E|W,G,A,B)
  - Also correct
  - Comparison with previous one:
    - Number of probabilities: 2 + 4 + 8 + 4 + 8 = 26
  - From "symptom" to "disease"

- Another one
  - W, G -> A
  - A -> B, E
  - Using formula for network:
    - P(W,G,A,B,E) = P(W)P(G|W)P(A|W,G)P(B|W,G,A)P(E|W,G,A,B)
  - Using chain rule:
    - P(W,G,A,B,E) = P(W)P(G|W)P(A|W,G)P(B|A)P(E|A)
  - Assumptions made:
    - P(G|W) = P(G)? NOPE. Incorrect network.

- Another one (no conditional independence assumptions)
  - W -> G,A,B,E
  - G -> A,B,E
  - A -> B,E
  - B -> E
  - Correct
  - But, you need 2 + 4 + 8 + 16 + 32 = 62 probabilities

<!-- 2018 May 30 -->

- After this lecture, you will be able to do all of A2
- A2 groups: Aim for at least two people. Reduces marking load.

- Inference in Bayesian networks
  - P(Query | Evidence)

- Kinds of probabilistic inference
  - Diagnostic
    - P(cause | effect), P(disease | symptom)
    - e.g. B -> A -> W. P(B | W) = ?
  - Causal inferences
    - P(effect | cause), P(symptom | disease)
    - e.g. B -> A -> W. P(W | B) = ?
  - Intercausal inferences
    - Between causes of a common effect
    - P(cause1 | cause2, effect), P(disease1 | disease2, symptom)
    - e.g. B, E -> A. P(B | E, A) = ?
    - Note: If P(B | A) >> P(B | E, A), then a bit of A is explained away
  - Mixed inferences
    - Combining diagnostic, causal, and intercausal
    - e.g. E -> A -> W. P(A | W, !E) = ?

- Examples of probabilistic inference
  - If Watson calls:
    - +E, +B, +R, 1W, +G
    - Note that Watson doesn't actually give us all that much information about
      whether a burglary is happening, because he's so unreliable. Chance goes
      from .0001 to .0002
  - If radio reports earthquake:
    - ++E, =B, 1R, +W, +G
  - Consider E and B.
    - If Watson calls, +E and +B.
    - If, in addition to that, R, then ++E and -B.
      - Wait, -B? Yes. R "explains away" some of the W.

- Example query using our Bayesian network
  -   P(!B | W,G) = P(!B,W,G) / P(W,G) = P(!B,W,G) / sum over dom(B) of
      P(B=b,W,G) = P(!B,W,G) / (P(!B,W,G) + P(B,W,G))
  - Where:
    -   P(!B,W,G) = sum over e, a, r of P(!B,E=e,A=a,R=r,G,W) = sum over e, a,
        r of P(!B)P(E=e)P(A=a|!B,E=e)P(R=r|E=e)P(W|A=a)P(G|A=a) (using formula
        for Bayesian network)
    - Can somewhat simplify by factoring

- Examples of Bayesian networks
  - In general: What's observable? What's latent?
    - Top layer: Situations and root causes
    - Mid layer: Events
    - Bot layer: Sensor outputs and reports
  - Example: Nuclear power plant
  - Example: Fire alarms
  - Example: Diagnosing diabetes
    - "One gender lives longer; one gender dies earlier with a surprising death
      peak in early 20s. I think it's called testosterone?"
  - Example: User needs assistance
  
## Slides 8, 9?

- Dynamic systems
  - What if we could reason over time?
  - Have a set of states S
  - Have cond. prob. tables P(St+1 | St, ..., S0)
  - Like a Bayesian network with one RV per time slice

- Markov chain
  - Based on the Markov assumption: That P(St+1 | St, ..., S0) = P(St+1 | St)
  - Stationary: If P(St+1 | St) is the same for all t

- Hidden Markov model
  - Like a Markov chain, but also have observations Ot: P(Ot | St)
  - We don't actually know what state we're in; we have to infer it via
    observations!!
  - holy shiiiiit

- Inference in temporal models
  - Monitoring: P(St | O1, ..., Ot)
  - Prediction: P(St+k | O1, ..., Ot)
    - Example: Weather, stocks
  - Hindsight:  P(Sk | O1, ..., Ot)
    - Example: Crime scene
  - Most probable explanation: argmax over S0, ..., St of P(S0, ..., St | O1,
    ..., Ot)
    - Example: Speech recognition

<!-- 2018 Jun 4 -->

## Slides 10

- Decision theory
  - Probability theory + utility theory

- Notation
  - wi, o <= i < n: state of the world
  - A, B: actions
  - U(wi): utility of state i (from point of view of making decision)

- Maximize Expected Utility (MEU) principle:
  - A rational agent should do what maximizes its expected utility.
  - EU(A|E) = sum over i=0..n-1 of P(wi|E,A)U(wi)
    - A: action
    - E: available evidence

- Trolley problem
  - MEU says pull the lever
  - Fat man variation: Suddenly it seems that action is bad
  - Fat villain variation: Suddenly it seems maybe we should follow MEU

- Decision networks
  - DAG
  - Three kinds of nodes: Chance, Decision, Utility
  - Chance nodes (): Parents are Chance and Decision nodes that influence it
  - Decision nodes []: Parents are Chance and Decision variables whose values
    will be known when the decision is made.
  - Utility nodes <>: Parents are Chance and Decision nodes that affect the
    utility

- Example: Robot that delivers mail
  - Two paths: Short route and long route
  - Short route is shorter, but might slip down stairs
  - Robot can put on pads and helmet, which reduce damage on falling down
    stairs (but make robot slower)
  - Decision network
    - [Pads, Short] -> (Accident), `<U>`
    - (Accident) -> `<U>`
  - Eight states of the world:
    - PSA: State
    - 000: w0: Slow, noweight:      6
    - 001: w1: impossible:          NaN
    - 010: w2: Quick, noweight:     10
    - 011: w3: Severe damage:       0
    - 100: w4: Slow, extra weight:  4
    - 010: w5: Impossible:          NaN
    - 110: w6: Quick, extra weight: 8
    - 111: w7: Moderate damage:     2
  - Want to calculate:
    - EU(P, S) for P in t/f for S in t/f
  - Variation: Oil slick at top of stairs
    - New node (Oil) -> (Accident)
    - No arc from (Oil) to [Pads, Short] because we don't know whether there's
      an oil slick at the time of making that decision
    - No arc from (Oil) to `<U>` because oil in wheels is not enough of a big
      deal. Accident/no accident is already accounted for.

<!-- 2018 Jun 6 -->

- Should also encode probability of various states actually being the case
- Have table of P(wi | evidence, action)

- What if we have a sensor?
  - Value of information is:
    - EU of optimal action using new information, minus
    - EU of optimal action without new information

- Flipped classroom: Lectures are posted online; class is for group work
  - Am not doing

- Basis of utility theory
  - Notation
    - A,B,C: states of the world
    - [p, A; 1-p, B]: A lottery with p chance to resolve to A
    - TODO

- Axioms of utility theory
  - Orderability: `(A>B)v(A<B)v(A~B)`
  - Transitivity: `(A>B)^(B>C) => (A>C)`
    - If your utilities aren't transitive, you become a money pump
  - Continuity: `A>B>C => ∃p[p,A;1-p,C]~B`
  - Substitutability: `A~B => [p,A;1-p,C] ~ [p,B;1-p,C]`
  - Monotonicity: `(A>B) => (p>q <=> [1,A;1-p,B]>[q,A;1-q,B]`
  - Decomposability: TODO

- Consequences of axioms
  - If an agent's preferences obey the axioms, then there is a real-valued
    function U such that
    - `U(A) > U(B) <=> U > B`
    - `U(A) = U(B) <=> A ~ B`
  - Scaling is arbitrary, since it doesn't affect decision made

- Example:
  - Choose between:
    - A: 80% of winning $4000:  [0.8,$4000; 0.2,$0]
    - B: 100% of winning $3000: [1.0,$3000; 0.0,$0]
  - Choose between:
    - C: 20% of winning $4000:  [0.20,$4000; 0.80,$0]
    - D: 25% of winning $3000:  [0.25,$3000; 0.75,$0]
  - If you didn't pick both A and C, or both B and D, then you are a money pump

- The utility of money
  - Not linear
  - In NA, tapers off at about $75k.
  - The pursuit of money is not going to bring you happiness (after a point)

<!-- 2018 Jun 13 -->

## Slides 13

- Multiagent systems
  - Fully cooperative: agents have same utility function
  - Fully competitive: win-lose (zero-sum games)

- Game theory framework
  - Finite set of agents I={1,...,n}
  - Set of actions Ai for each agent
  - Utility function ui for each agent
  - Each agent chooses an action without knowing what other agents will choose
  - The joint action of all the agents produces the outcome

- Strategies
  - Pure: one action. Stochastic (mixed): Probability distribution over
    actions.
  - Strategy profile: Assignment of a strategy to each agent.

- Nash equilibrium
  - Let utility(σ,i) be the expected utility of strategy profile σ for agent i
  - Best response for agent i to the strategies σ-i of the other agents is a
    strategy σi that has maximal utility for agent i
  - Strategy profile is a Nash equilibrium if σi is a best response to σ-i for
    all agents i.
  - Every finite game has at least one Nash equilibrium.
  - Nash equilibrium strategies are often not pure.
  - Finding a Nash equilibrium is NP complete in the number of players.

- Pareto optimal
  - An outcome is Pareto optimal if no other outcome makes every player at
    least as well off and at least one player strictly better off.
  - Every finite game has at least one Pareto optimal outcome.
  - It is not necessarily a Nash equilibrium (??)

- Example: Fred and Barney go to the beach.
  - [Barney, Fred] x [Home, Beach] = (0,0), (0,1), (1,0), (2,2)
  - Intuitively: They should both go to the beach.
  - (Beach,Beach) is a Nash equilibrium and a Pareto optimal.

- Example: Fred and Wilma go to the pool.
  -            Fred
  -           H     P
  - Wilma H (2,0) (2,1)
  -       P (3,0) (1,2)
  - (H,P) is a Nash equilibrium.
  - All but (H,H) are Pareto optimal.

- Example: Fred and Barenty go swimming or hiking with some probability.
  - (2,1), (0,0), (0,0), (1,2).
  - With strategies of [p,swim;1-p,hike] and [q,swim;1-q,hike], expected
    utility is
    - 2pq+(1-p)(1-q), pq+2(1-p)(1-q)
    - Want to pick p and q such that ^ is a Nash equilibrium
    - Result: p=2/3, q=1/3

- Example: Prisoner's dilemma
  - Fred v. Barney, Solidarity v. Defection
  - (3,3), (1,4), (4,1), (2,2).
  - Both defect is the only Nash equilibrium, and the only non-Pareto optimum.

- Example: Divorce
  - Jack v. Jill, Conciliatory v. Aggressive
  - (50,50), (20,80), (80,20), (30,30).
  - Divorce lawyers can get real expensive. Take 40% of total assets.
  - Same as prisoner's dilemma.

- Walmart.
  - Walmart destroys downtown.
  - Why? Everybody defects. I like the lower prices, so I defect. And you end
    up with Walmart.

- Dominated strategies
  - TODO

<!-- 2018 Jun 18 -->

## 14: Unsupervised and supervised learning.

- It's time for MUH SHEEEEN LURNING

- Agents that learn by themselves and improve their performance over time.

- What is learning?
  - "An increase in knowledge" -- but what kind of knowledge? That depends on
    your definition of knowledge.

- Types of learning systems
  - Rote learning: Memorizing facts.
    - Evaluate by querying facts
  - Speedup learning: Practice.
    - Evaluate by measuring increase in efficiency
  - Inductive learning: Generalizing.
    - Evaluate by measuring correctness of the new knowledge

- Supervised learning
  - Given training set {(xi,yi)} for some unknown function f(xi) = yi, produce
    a hypothesis function h from the hypothesis space H that is kinda sorta
    close to f
  - Should separate test set and training set

- Outcomes can be real-valued or discrete
  - Learning for real-valued outcomes: Regression
  - Learning for discrete outcomes: Classification

- Choosing the training data
  - Should be representative
  - Should have a good choice of features/attributes
  - Should have a good balance of positive and negative examples

- Choosing the hypothesis space
  - Consider cost of learning a hypothesis
  - Consider amount of training data needed
  - Consider whether the representation makes sense

- Choosing the hypothesis
  - Ockham's razor: You want a simple h
  - Performance on training set
  - Performance on test set
  - Measuring errors

- Overfitting
  - As h gets more complex, the error on the training set decays down to
    zero...
  - but the error on the test set rebounds after a certain point of complexity!
  - h in H overfits the training data if there exists some h' in H that
    performs worse on the training set but better on the test set than h.

- Unsupervised learning
  - We don't know the categories... we just have the data.
  - !!! In A1P3, we were unknowingly learning the STRUCTURE of a BAYESIAN
    NETWORK

- Assignment 4 will have transfer learning.
  - IN MATLAB! ayyyy
  - You want Python, but I'm in love with Matlab

<!-- 2018 Jun 20 -->

- Hard clustering
  - Let:
    - ci be a cluster
    - k be the number of clusters
    - μi be the mean of the examples in ci
  - We want to minimize the distance to the mean within the clusters
    - sum for i=1..k of sum for x in ci of d(x, μi)
  - Very hard to minimize exactly
    - Approximation: k-means
  - As number of clusters increases, error goes down
    - You need to pick k

- Distance/similarity measures
  - General distance: d(A,B) = (sum for j=1..d of |aj-bj|^q)^(-1/q)
  - Manhattan distance: General distance w/ q=1
  - Euclidean distance: General distance w/ q=2
  - Cosine similarity: s(A,B) = cos α = transpose(A)B / ||A|| ||B||
    - using ||A|| = d(A,A) w/ Euclidean distance

- k-means
  - Set μi to random values for all i = 1..k
  - Until no changes:
    - Produce k clusters by assigning each example to its nearest mean μi,
      breaking ties randomly
    - Update μi's using the k clusters
  - May restart several times (like local search)

- Example: Medical image.
  - Clustering in RGB is kinda bad.
  - Clustering in Lab results in smoother clusters.

- "Matlab, Python... what else is there, right?"

- Learning Bayesian networks for classification
  - You can learn Bayesian networks in a supervised or unsupervised way
  - Versions (simplest to hardest)
    - Given data, naive Bayes (fixed structure) of network
      - Learn probabilities
    - Given data and structure of the network (that we come up with)
      - Learn probabilities
    - Given data and nodes for each attribute and class variable
      - Learn arcs and probabilities
    - Given data and nodes for each attribute and class variable
      - Learn hidden nodes, arcs, and probabilities

- Tennis example: Day, Outlook, Temp, Humidity, Wind, Tennis

- Naive Bayes
  - Tennis example: Tennis node goes out into Outlook, Temp, Humidity, Wind
  - Conditional independence assumptions: Given Tennis, (Outlook, Temp,
    Humidity, Wind) are all mutually independent.
  - Correct? No, by our knowledge of weather.
  - What about the data? Also no.
