# CS 486: Intro to AI

Professor: Peter van Beek

## 2017 May 02

- What is intelligence?
  - Remembering, reasoning, planning, solving, learning, adapting
  - Understanding other intelligences and beat them
  - Understanding oneself deeply?

- Thinking is symbolic reasoning.
  - Recall Turing machines, lambda calculus
  - Can humans compute functions that are not Turing computable? >> open
    question, probably not

- Newell-Simon hypothesis/conjecture:
  - A physical symbol system suffices for intelligence.

- Four categories of AI definitions:
  - Systems that (think | act) (like humans | rationally)

- Thinking rationally
  - "Laws of thought" approach: logic and probability as normative theories.
  - "Rational agent" approach: agent does best thing to achieve its goals.

- Lots of AI applications

## 2018 May 07

- Problem solving using search
  - Find a goal state given constraints on the goal.
  - Find a sequence of actions that leads to the goal state.

- Example: n queens on an nxn board; no threats
- Example: crossword puzzles
- Example: sliding puzzles
- Example: river crossing puzzle with 100kg capacity boat, 100kg parent, two
  50kg children
- Example: propositional satisfiability (NP complete)
- Example: partition problem (equalize weights) (NP complete)
- Example: travelling saleswoman problem
- Example: set covering (minimum size committee with all skills)

- Formulate problem solving as graph search.
  - Nodes are states.
  - Edges are actions.
  - Solution is a path from initial state to goal state.
  - May have cost on edges.

- For river crossing puzzle:
  - States are assignments of {boat, parent, child1, child2} to {L, R}
    - 16 states
  - Edges are valid river crossings

- For sliding puzzles:
  - States are permutations of {1, ..., 8, empty}
    - 9! states
  - Edges are valid slides

- For n-queens:
  - States are assignments of {Q1, ..., Qn} to row index (assume Qi is in
    column i)
  - Initial state is assignment of Qi to i
  - Edges are swapping two queens

## 2018 May 09

- Skipped in favour of mental health seminar

- General search algorithm is
  - Start with list of start nodes
  - Until it's empty:
    - Pop a node, and if it isn't the goal, push successor states

- Effect of different queues
  - LIFO queue: Breadth first search
  - FIFO queue: Depth-first search
  - Priority queue: Informed search

- Distance function: 
  - g(n) is cost from initial state to n that was found
  - h(n) is heuristic estimate of cost from n to a goal
  - f(n) = g(n) + h(n) is the heuristic estimate of the total cost of the
    solution obtained down this path
  - if `h(n) <= h*(n)` for all n, then h(n) is admissible
    - with admissible h(n), `A*` will find an optimal path
  - if `h(n) <= cost(n, n') + h(n') for all n, n' a successor to n`, then h(n)
    is consistent
    - with consistent h(n), `A*` will find an optimal path
  - dominating heuristics: h2(n) dominates h1(n) if for all nodes n, h2(n) >=
    h1(n), and there's some node n' where h2(n) > h1(n').
    - thm: If h2(n) dominates h1(n), then `A*` with h1(n) expands at least as
      many node as with h2(n)

- Greedy search: Informed search with f(n) = h(n)
- Dijkstra: Informed search with f(n) = g(n)
- `A*`: Informed search with f(n) + h(n) + g(n)

- Facts about `A*`
  - Is complete, optimal, optimally efficient
    - No algorithm with the same information can do better!
  - Assumes a single goal
  - Has exponential time complexity `O(b^(εd))`, where
    - ε is the maximum relative error `1 - h(n) / h*(n)`
    - b is the branching factor
    - d is the depth of the goal node

- Iterative-deepening `A*`
  - No priority queue; instead, each iteration is a complete DFS that is cut
    off if f(n) exceeds some threshold

## Constraint satisfaction problems <!-- 2018 May 14 -->

- You have
  - A set of variables { x1, ..., xn }
  - A set of values for each variable dom(x1), ..., dom(xn)
  - A set of constraints { C1, ..., Cm }
- And want
  - An assignment to variables that satisfies the constraints
    - ... whether or not there is one?
    - ... any one?
    - ... all of them?
    - ... the optimal one, given some cost function?

- Example domains and constraints
  - Reals -- linear constraints: Gaussian elimination, linear programming
  - Integers -- linear constraints: Integer linear programming,
    branch-and-bound
  - Booleans -- propositional statements
  - Here, we will use finite domains with expressive constraint languages

- Constraint languages
  - Arithmetic operators
  - Logical operators
  - Global constraints: over an arbitrary number of variables
  - Table constraints: enumerate satisfied assignments

- Alldifferent
  - A global constraint over a set of variables that is satisfied iff all
    variables are assigned a different value

- Examples
  - Sudoku:
    - 81 variables x1, ..., x81 for each grid square
    - Rows: alldifferent(x1, ..., x9)
    - Columns: alldifferent(x1, x10, ..., x73)
    - Squares: alldifferent(x1, x2, x3, x10, ..., x21)
    - Given values
  - n-queens: Variables are x1, ..., x4; xi is the row of the queen in column i
  - Crosswords: Variables are squares, domain is latin alphabet

### Constraint propagation

- Detail on constraints
  - An assignment is x = a, where a in dom
  - A tuple t over a list of variables {x1, ..., xk} is a list of values (a1,
    ..., ak)
    - like a set of assignments
  - In a tuple t, t[xi] is the value for variable xi.
  - A constraint C defined over vars(C) specifies the allowed combination of
    values for the variables in vars(C).
  - vars(C) is the **scope** or **scheme** of the constraint
  - len(vars(C)) is the **arity** of the constraint
    - Unary constraint: 1
    - Binary constraint: 2
    - Non-binary: > 2
  - In a binary CSP, all constraints are binary.

- Intensional vs. extensional
  - Intensional is implicit. Like "x1 != x2 and |x1 - x2| != 1"
  - Extensional is explicit. Like {(1,3), (1,4), (2,4), (3,1), (4,1), (4,2)}.
    - AKA the table constraint.

- Local consistency: Arc consistency
  - Given a constraint, remove a value from the domain of a variable if it
    cannot be part of a solution according to that constriant.
  - Formally:
    - Let C be a constraint
    - Domain support:
      - Let x in vars(C)
      - Let a in dom(x)
      - x has a domain support in C if there exists t in C such that t[x] = a
        and t[y] in dom(y) for every y in vars(C).
    - C is arc consistent iff for all x in vars(C), every value a in dom(x) has
      a domain support in C.
      - i.e. if it's possible that x = a can solve the CSP.
  - A CSP is arc consistent if every constraint is arc consistent
  - You can make a CSP arc consistent by repeatedly removing unsupported values
    from the domains.

- Arc consistency algorithm
  - ac: Q:(variable * constraint) list -> boolean
    - while Q is not empty:
      - pop (x, C) from Q
      - if revise(x, C):
        - if dom(x) is empty: return false
        - else: add pairs to Q
    - return true
  - revise: x:variable -> C:constraint -> boolean
    - change = false
    - for a:value in dom(x):
      - if there's no domain support for a in C:
        - remove a from dom(x)
        - change = true
    - return change

<!-- 2018 May 16 -->

### Backtracking search

- Backtracking search: DFS of search tree

- Algorithm template:
  - backtrack(assignment, csp):
    - if assignment is complete then solution found
    - var <- select next variable
    - for val in dom(var):
      - save(csp)
      - add var = val to assignment
      - if propagate(assignment, csp):
        - backtrack(assignment, csp)
      - restore(csp)

- Backtracking algorithms:
  - Naive backtracking (BT)
    - No constraint propagation
    - Backtrack in time
  - Forward checking (FC)
    - Maintain arc consistency on all constraints with exactly one
      uninstantiated variable
    - Backtrack in time
  - Maintaining arc consistency (MAC):
    - Maintain arc consistency on all constraints with at least one
      uninstantiated variable
    - Backtrack in time
  - MAC is the best. Only use FC if MAC is guaranteed to not help at all.

### Local search

- Local search
  - Does both satisfaction and optimization problems
  - No guaranteed that a solution will be found, even if it exists
  - Cannot find a provably optimal solution
  - Finds locally optimal solutions, not necessarily globally optimal

- Notation
  - S := set of states
  - c : S -> R := cost function
  - N : S -> 2^S := neighborhood function

- Optimality
  - Globally optimal: solution `s*` in S with `c(s*) <= c(s)` for all s in S.
  - Locally optimal: solution `s+` in S with `c(s+) <= c(s)` for all s in
    `N(s+)`.

- For CSPs
  - Consider some constraints hard (must be satisfied); others soft (cost
    function +1 for each unsatisfied constraint)

- Algorithm template:
  - s = some initial complete assignment
  - k = 0
  - do
    - r = neighbor of s
    - if c(r) - c(s) < tk then
      - s = r
    - k += 1
  - until stopping criteria satisfied
  - return best s

- Stopping criteria
  - Maximum # of iterations
  - Solution of low enough cost
  - Number of iterations since last (big enough) improvement too large

- Choices
  - Initial feasible solution
    - Can be random, or "good" according to some heuristic
  - Neighborhood function
    - Small neighborhood: Easily explored, low quality solutions
    - Large neighborhood: Expensive
  - Selecting r
    - First improvement (pick first improving neighbor)
    - Best improvement (go through all neighbors and pick best one)

- Thresholds
  - Iterative improvement
  - Threshold accepting
    - Accept worse cost neighbors with diminishing threshold
    - Variation: simulated annealing. (accept worse neighbors with a gradually
      decreasing probability)
    - Variation: tabu search. (accept worse neighbors based on a list of legal
      neighbors)

- Improvements
  - Multi-starts: Try multiple initial solutions
  - Multi-level: Use different neighborhoods throughout

- Example: Neighborhoods for 8-queens
  - Transpose: swap adjacent queens. O(n) neighbors
  - Insert: move a queen, displacing queens in between. O(n^2) neighbors
  - Swap: swap two queens. O(n^2) neighbors
  - Block insert: move a subsequence of queens. O(n^3) neighbors.

<!-- 2018 May 22 --> <!-- Missed a bit due to photo session. -->

- Example: Results for TSP
  - Theoretical results
    - Exact neighborhood: When every local optimum is also a global optimum
      - Must be exponential in size, unless P = NP
    - Non-exact neighborhoods
      - Cost of local optimum can be arbitrarily far from global optimum :(
      - Local search can still take exponential number of steps to reach a
        local optimum
  - Empirical results
    - Best local search algorithms get within 1.5-2.5% of optimal
    - Can do a million cities in under an hour

- Example: Local search for TSP
  - Nodes are permutations of cities
  - Cost is cost of tour
  - Neighborhood function 2-opt:
    - Delete two edges from the tour to break it into two paths, then reconnect
      in all possible ways

- Example: Local search for SAT
  - GSAT: Keeping flipping the boolean variable that minimizes the number of
    unsatisfied clauses, do so maxTries times

- Genetic algorithms
  - Create a population of solutions, assign fitness according to how good the
    solution is, breed solutions randomly weighted by goodness, possibly mutate
    their children, and pick the fittest individual

<!-- 2018 May 23 -->

## Reasoning under uncertainty

- Types of uncertainty:
  - Partial observability: incomplete knowledge of world
  - Nondeterminism: actions have random consequences

- Random variable: is a thing

- Joint probability distribution
  - Assignment of a probability to each atomic event for a set of random
    variables
  - Atomic event is cross product of domains of your random variables

- Holmes scenario:  
  - A: Alarm goes off
  - B: Burglary in progress
  - R: Radio runs report
  - W: Watson calls saying alarm is going
  - G: Gibson calls saying alarm is going

- Important rules:
  - Product rule: P(X,Y) = P(X|Y)P(Y) = P(Y|X)P(X)
  - Sum rule: P(X=a) = sum over b in dom(Y) of P(X=a|Y=b)P(Y=b)
  - Bayes rule: P(Y|X) = P(Y)P(X|Y)/P(X)
  - Chain rule: P(X1,...,Xn) = product for i from 1 to n of P(Xi|Xi-1,...,X1)

- Independence:
  - X is ind. of Y if P(X|Y) = P(X).
  - Implies P(Y|X) = P(Y).

- Examples of (non)independence
  - Watson is independent of Gibbon? No

<!-- 2018 May 28 -->

- Conditional independence:
  - X is cond. ind. of Y given Z if P(X|Y,Z) = P(X|Z).
  - Can swap X and Y.
  - Allows simplification of chain rule:
    - P(X1, ..., Xn) = ... = P(X1), if X2, ..., Xn cond. ind. of X1.

- Bayesian network
  - DAG
  - Nodes are random variables
  - Directed edges are "influence"
  - Each node has a conditional probability table of the effect of parents on
    the node
  - Two ways to understand them semantically:
    - As a representation of the entire joint probability distribution
    - As an encoding of conditional independence assumptions
  - Correct only if each node is conditionally independent of its predecessors
    in the node ordering, given its parents
    - In practice, you'll need to look at many orderings
    - For us, any ordering works

- Example: Bayesian network for burglary scenario
  - B, E -> A
  - E -> R
  - A -> G, W
  - Assumptions made: P(W|A) = P(W|A,B) = P(W|A,...)
  - From "disease" to "symptom"
  - 16 probabilities

- Another one (drop R)
  - W -> G, A
  - G -> A
  - A -> B, E
  - B -> E
  - Using formula for network:
    - P(W,G,A,B,E) = P(W)P(G|W)P(A|W,G)P(B|A)P(E|B,A)
  - Using chain rule:
    - P(W)P(G|W)P(A|W,G)P(B|W,G,A)P(E|W,G,A,B)
  - Also correct
  - Comparison with previous one:
    - Number of probabilities: 2 + 4 + 8 + 4 + 8 = 26
  - From "symptom" to "disease"

- Another one
  - W, G -> A
  - A -> B, E
  - Using formula for network:
    - P(W,G,A,B,E) = P(W)P(G|W)P(A|W,G)P(B|W,G,A)P(E|W,G,A,B)
  - Using chain rule:
    - P(W,G,A,B,E) = P(W)P(G|W)P(A|W,G)P(B|A)P(E|A)
  - Assumptions made:
    - P(G|W) = P(G)? NOPE. Incorrect network.

- Another one (no conditional independence assumptions)
  - W -> G,A,B,E
  - G -> A,B,E
  - A -> B,E
  - B -> E
  - Correct
  - But, you need 2 + 4 + 8 + 16 + 32 = 62 probabilities

<!-- 2018 May 30 -->

- After this lecture, you will be able to do all of A2
- A2 groups: Aim for at least two people. Reduces marking load.

- Inference in Bayesian networks
  - P(Query | Evidence)

- Kinds of probabilistic inference
  - Diagnostic
    - P(cause | effect), P(disease | symptom)
    - e.g. B -> A -> W. P(B | W) = ?
  - Causal inferences
    - P(effect | cause), P(symptom | disease)
    - e.g. B -> A -> W. P(W | B) = ?
  - Intercausal inferences
    - Between causes of a common effect
    - P(cause1 | cause2, effect), P(disease1 | disease2, symptom)
    - e.g. B, E -> A. P(B | E, A) = ?
    - Note: If P(B | A) >> P(B | E, A), then a bit of A is explained away
  - Mixed inferences
    - Combining diagnostic, causal, and intercausal
    - e.g. E -> A -> W. P(A | W, !E) = ?

- Examples of probabilistic inference
  - If Watson calls:
    - +E, +B, +R, 1W, +G
    - Note that Watson doesn't actually give us all that much information about
      whether a burglary is happening, because he's so unreliable. Chance goes
      from .0001 to .0002
  - If radio reports earthquake:
    - ++E, =B, 1R, +W, +G
  - Consider E and B.
    - If Watson calls, +E and +B.
    - If, in addition to that, R, then ++E and -B.
      - Wait, -B? Yes. R "explains away" some of the W.

- Example query using our Bayesian network
  -   P(!B | W,G) = P(!B,W,G) / P(W,G) = P(!B,W,G) / sum over dom(B) of
      P(B=b,W,G) = P(!B,W,G) / (P(!B,W,G) + P(B,W,G))
  - Where:
    -   P(!B,W,G) = sum over e, a, r of P(!B,E=e,A=a,R=r,G,W) = sum over e, a,
        r of P(!B)P(E=e)P(A=a|!B,E=e)P(R=r|E=e)P(W|A=a)P(G|A=a) (using formula
        for Bayesian network)
    - Can somewhat simplify by factoring

- Examples of Bayesian networks
  - In general: What's observable? What's latent?
    - Top layer: Situations and root causes
    - Mid layer: Events
    - Bot layer: Sensor outputs and reports
  - Example: Nuclear power plant
  - Example: Fire alarms
  - Example: Diagnosing diabetes
    - "One gender lives longer; one gender dies earlier with a surprising death
      peak in early 20s. I think it's called testosterone?"
  - Example: User needs assistance
  
## Slides 8, 9?

- Dynamic systems
  - What if we could reason over time?
  - Have a set of states S
  - Have cond. prob. tables P(St+1 | St, ..., S0)
  - Like a Bayesian network with one RV per time slice

- Markov chain
  - Based on the Markov assumption: That P(St+1 | St, ..., S0) = P(St+1 | St)
  - Stationary: If P(St+1 | St) is the same for all t

- Hidden Markov model
  - Like a Markov chain, but also have observations Ot: P(Ot | St)
  - We don't actually know what state we're in; we have to infer it via
    observations!!
  - holy shiiiiit

- Inference in temporal models
  - Monitoring: P(St | O1, ..., Ot)
  - Prediction: P(St+k | O1, ..., Ot)
    - Example: Weather, stocks
  - Hindsight:  P(Sk | O1, ..., Ot)
    - Example: Crime scene
  - Most probable explanation: argmax over S0, ..., St of P(S0, ..., St | O1,
    ..., Ot)
    - Example: Speech recognition

<!-- 2018 Jun 4 -->

## Slides 10

- Decision theory
  - Probability theory + utility theory

- Notation
  - wi, o <= i < n: state of the world
  - A, B: actions
  - U(wi): utility of state i (from point of view of making decision)

- Maximize Expected Utility (MEU) principle:
  - A rational agent should do what maximizes its expected utility.
  - EU(A|E) = sum over i=0..n-1 of P(wi|E,A)U(wi)
    - A: action
    - E: available evidence

- Trolley problem
  - MEU says pull the lever
  - Fat man variation: Suddenly it seems that action is bad
  - Fat villain variation: Suddenly it seems maybe we should follow MEU

- Decision networks
  - DAG
  - Three kinds of nodes: Chance, Decision, Utility
  - Chance nodes (): Parents are Chance and Decision nodes that influence it
  - Decision nodes []: Parents are Chance and Decision variables whose values
    will be known when the decision is made.
  - Utility nodes <>: Parents are Chance and Decision nodes that affect the
    utility

- Example: Robot that delivers mail
  - Two paths: Short route and long route
  - Short route is shorter, but might slip down stairs
  - Robot can put on pads and helmet, which reduce damage on falling down
    stairs (but make robot slower)
  - Decision network
    - [Pads, Short] -> (Accident), `<U>`
    - (Accident) -> `<U>`
  - Eight states of the world:
    - PSA: State
    - 000: w0: Slow, noweight:      6
    - 001: w1: impossible:          NaN
    - 010: w2: Quick, noweight:     10
    - 011: w3: Severe damage:       0
    - 100: w4: Slow, extra weight:  4
    - 010: w5: Impossible:          NaN
    - 110: w6: Quick, extra weight: 8
    - 111: w7: Moderate damage:     2
  - Want to calculate:
    - EU(P, S) for P in t/f for S in t/f
  - Variation: Oil slick at top of stairs
    - New node (Oil) -> (Accident)
    - No arc from (Oil) to [Pads, Short] because we don't know whether there's
      an oil slick at the time of making that decision
    - No arc from (Oil) to `<U>` because oil in wheels is not enough of a big
      deal. Accident/no accident is already accounted for.

<!-- 2018 Jun 6 -->

- Should also encode probability of various states actually being the case
- Have table of P(wi | evidence, action)

- What if we have a sensor?
  - Value of information is:
    - EU of optimal action using new information, minus
    - EU of optimal action without new information

- Flipped classroom: Lectures are posted online; class is for group work
  - Am not doing

- Basis of utility theory
  - Notation
    - A,B,C: states of the world
    - [p, A; 1-p, B]: A lottery with p chance to resolve to A
    - TODO

- Axioms of utility theory
  - Orderability: `(A>B)v(A<B)v(A~B)`
  - Transitivity: `(A>B)^(B>C) => (A>C)`
    - If your utilities aren't transitive, you become a money pump
  - Continuity: `A>B>C => ∃p[p,A;1-p,C]~B`
  - Substitutability: `A~B => [p,A;1-p,C] ~ [p,B;1-p,C]`
  - Monotonicity: `(A>B) => (p>q <=> [1,A;1-p,B]>[q,A;1-q,B]`
  - Decomposability: TODO

- Consequences of axioms
  - If an agent's preferences obey the axioms, then there is a real-valued
    function U such that
    - `U(A) > U(B) <=> U > B`
    - `U(A) = U(B) <=> A ~ B`
  - Scaling is arbitrary, since it doesn't affect decision made

- Example:
  - Choose between:
    - A: 80% of winning $4000:  [0.8,$4000; 0.2,$0]
    - B: 100% of winning $3000: [1.0,$3000; 0.0,$0]
  - Choose between:
    - C: 20% of winning $4000:  [0.20,$4000; 0.80,$0]
    - D: 25% of winning $3000:  [0.25,$3000; 0.75,$0]
  - If you didn't pick both A and C, or both B and D, then you are a money pump

- The utility of money
  - Not linear
  - In NA, tapers off at about $75k.
  - The pursuit of money is not going to bring you happiness (after a point)

<!-- 2018 Jun 13 -->

## Slides 13

- Multiagent systems
  - Fully cooperative: agents have same utility function
  - Fully competitive: win-lose (zero-sum games)

- Game theory framework
  - Finite set of agents I={1,...,n}
  - Set of actions Ai for each agent
  - Utility function ui for each agent
  - Each agent chooses an action without knowing what other agents will choose
  - The joint action of all the agents produces the outcome

- Strategies
  - Pure: one action. Stochastic (mixed): Probability distribution over
    actions.
  - Strategy profile: Assignment of a strategy to each agent.

- Nash equilibrium
  - Let utility(σ,i) be the expected utility of strategy profile σ for agent i
  - Best response for agent i to the strategies σ-i of the other agents is a
    strategy σi that has maximal utility for agent i
  - Strategy profile is a Nash equilibrium if σi is a best response to σ-i for
    all agents i.
  - Every finite game has at least one Nash equilibrium.
  - Nash equilibrium strategies are often not pure.
  - Finding a Nash equilibrium is NP complete in the number of players.

- Pareto optimal
  - An outcome is Pareto optimal if no other outcome makes every player at
    least as well off and at least one player strictly better off.
  - Every finite game has at least one Pareto optimal outcome.
  - It is not necessarily a Nash equilibrium (??)

- Example: Fred and Barney go to the beach.
  - [Barney, Fred] x [Home, Beach] = (0,0), (0,1), (1,0), (2,2)
  - Intuitively: They should both go to the beach.
  - (Beach,Beach) is a Nash equilibrium and a Pareto optimal.

- Example: Fred and Wilma go to the pool.
  -            Fred
  -           H     P
  - Wilma H (2,0) (2,1)
  -       P (3,0) (1,2)
  - (H,P) is a Nash equilibrium.
  - All but (H,H) are Pareto optimal.

- Example: Fred and Barenty go swimming or hiking with some probability.
  - (2,1), (0,0), (0,0), (1,2).
  - With strategies of [p,swim;1-p,hike] and [q,swim;1-q,hike], expected
    utility is
    - 2pq+(1-p)(1-q), pq+2(1-p)(1-q)
    - Want to pick p and q such that ^ is a Nash equilibrium
    - Result: p=2/3, q=1/3

- Example: Prisoner's dilemma
  - Fred v. Barney, Solidarity v. Defection
  - (3,3), (1,4), (4,1), (2,2).
  - Both defect is the only Nash equilibrium, and the only non-Pareto optimum.

- Example: Divorce
  - Jack v. Jill, Conciliatory v. Aggressive
  - (50,50), (20,80), (80,20), (30,30).
  - Divorce lawyers can get real expensive. Take 40% of total assets.
  - Same as prisoner's dilemma.

- Walmart.
  - Walmart destroys downtown.
  - Why? Everybody defects. I like the lower prices, so I defect. And you end
    up with Walmart.

- Dominated strategies
  - TODO

<!-- 2018 Jun 18 -->

## 14: Unsupervised and supervised learning.

- It's time for MUH SHEEEEN LURNING

- Agents that learn by themselves and improve their performance over time.

- What is learning?
  - "An increase in knowledge" -- but what kind of knowledge? That depends on
    your definition of knowledge.

- Types of learning systems
  - Rote learning: Memorizing facts.
    - Evaluate by querying facts
  - Speedup learning: Practice.
    - Evaluate by measuring increase in efficiency
  - Inductive learning: Generalizing.
    - Evaluate by measuring correctness of the new knowledge

- Supervised learning
  - Given training set {(xi,yi)} for some unknown function f(xi) = yi, produce
    a hypothesis function h from the hypothesis space H that is kinda sorta
    close to f
  - Should separate test set and training set

- Outcomes can be real-valued or discrete
  - Learning for real-valued outcomes: Regression
  - Learning for discrete outcomes: Classification

- Choosing the training data
  - Should be representative
  - Should have a good choice of features/attributes
  - Should have a good balance of positive and negative examples

- Choosing the hypothesis space
  - Consider cost of learning a hypothesis
  - Consider amount of training data needed
  - Consider whether the representation makes sense

- Choosing the hypothesis
  - Ockham's razor: You want a simple h
  - Performance on training set
  - Performance on test set
  - Measuring errors

- Overfitting
  - As h gets more complex, the error on the training set decays down to
    zero...
  - but the error on the test set rebounds after a certain point of complexity!
  - h in H overfits the training data if there exists some h' in H that
    performs worse on the training set but better on the test set than h.

- Unsupervised learning
  - We don't know the categories... we just have the data.
  - !!! In A1P3, we were unknowingly learning the STRUCTURE of a BAYESIAN
    NETWORK

- Assignment 4 will have transfer learning.
  - IN MATLAB! ayyyy
  - You want Python, but I'm in love with Matlab

<!-- 2018 Jun 20 -->

- Hard clustering
  - Let:
    - ci be a cluster
    - k be the number of clusters
    - μi be the mean of the examples in ci
  - We want to minimize the distance to the mean within the clusters
    - sum for i=1..k of sum for x in ci of d(x, μi)
  - Very hard to minimize exactly
    - Approximation: k-means
  - As number of clusters increases, error goes down
    - You need to pick k

- Distance/similarity measures
  - General distance: d(A,B) = (sum for j=1..d of |aj-bj|^q)^(-1/q)
  - Manhattan distance: General distance w/ q=1
  - Euclidean distance: General distance w/ q=2
  - Cosine similarity: s(A,B) = cos α = transpose(A)B / ||A|| ||B||
    - using ||A|| = d(A,A) w/ Euclidean distance

- k-means
  - Set μi to random values for all i = 1..k
  - Until no changes:
    - Produce k clusters by assigning each example to its nearest mean μi,
      breaking ties randomly
    - Update μi's using the k clusters
  - May restart several times (like local search)

- Example: Medical image.
  - Clustering in RGB is kinda bad.
  - Clustering in Lab results in smoother clusters.

- "Matlab, Python... what else is there, right?"

- Learning Bayesian networks for classification
  - You can learn Bayesian networks in a supervised or unsupervised way
  - Versions (simplest to hardest)
    - Given data, naive Bayes (fixed structure) of network
      - Learn probabilities
    - Given data and structure of the network (that we come up with)
      - Learn probabilities
    - Given data and nodes for each attribute and class variable
      - Learn arcs and probabilities
    - Given data and nodes for each attribute and class variable
      - Learn hidden nodes, arcs, and probabilities

- Tennis example: Day, Outlook, Temp, Humidity, Wind, Tennis

- Naive Bayes
  - Tennis example: Tennis node goes out into Outlook, Temp, Humidity, Wind
    - Conditional independence assumptions: Given Tennis, (Outlook, Temp,
      Humidity, Wind) are all mutually independent.
    - Correct? No, by our knowledge of weather.
    - What about the data? Also no.
  - At least it's efficient.
  - Often used as a baseline. If something does worse than NB, it's literally garbage.

<!-- 2018 Jun 25 -->

## 15: More learning

### Sructure learning from data

- Measure fit to data
  - First attempt: Maximize probability of observing data, given model G
    - P(data | G)
    - This overfits, since a complete network can get this perfect
  - Second attempt: Add a penalty for the complexity of the model
    - Score(G) = likelihood + (penalty for complexity)
    - e.g. BIC(G) = -log₂P(data | G) + ½ log₂(N) || G ||

- Score-and-search
  - Use the scoring function to determine the goodness of a list of possible parent sets

- Learning arcs and probabilities
  - Algorithm
    - 1. For each variable/feature, for each possible parent set, calculate
      - score(xi, parent set for xi)
    - 2. Pruning rule.
      - For parent sets p, p' of x,
      - If p ⊂ p' and score(x,p) ≤ score(x,p'), then p' does not need to be
        considered.
    - 3. For each variable, pick a parent set that
      - a. total score is minimized
      - b. graph must be acyclic
      Can solve with A*, local search (we did it)

- Example: Jeeves
  - Step 1,2:
    - Tennis. {}: 10.44, {Humidity}: 10.29
    - Temperature. {}: 17.74, {Humidity}: 16.75
    - Humidity: {}: 11.02, {Tennis}: 10.87, {Temperature}: 10.03
    - Outlook: {}: 17.95
    - Wind: {}: 10.88
  - Your program can do step 3! Here's one optimal solution:
    - Wind, Outlook, Humidity, Temperature, Tennis
    - Total cost: 66.89

## 16. Decision trees

- Decision trees
  - Start at the root
  - At each node, test a feature
  - Leaves are labels
  - Easy to overfit, by accidentally propositional-logicking or
    sum-of-minterming your entire training set

- ID3
  - Algorithm to learn decision trees by perfectly fitting your training set
  - Let S: examples, F: features
  - 1. If S is only positive examples, return "yes"
  - 2. If S is only negative examples, return "no"
  - 3. Else:
    - Choose best feature f in F
    - For v in f
      - Add arc to tree with label v
      - Add subtree ID3(F - {f}, {s in S if f(s)=v})
  - Where the "best" feature is the one that divides the data into roughly
    equal chunks.

- How to quantify this? Use information theory:
  - Information content I(P(c1), ..., P(cl)) of l outcomes c1..cl with probabilities P(c1)..P(cl) is:
    - sum for i=1..l of -P(ci)log₂(P(ci)) (???)
    - measured in bits
  - e.g. Fair coin gives you 1 bit of information, 99% biased coin gives you
    0.08 bits
  - So information gain of a feature f is
    - I(p/(p+n), n/(p+n)) - sum for i=1..k of ((pi + ni) / (p + n)) * I(pi / (pi+ni), ni/(pi+ni))

<!-- 2018 Jun 27 -->

## 17. Extending decision trees

- Numeric attributes
  - Soln 1: Discretize them into buckets
  - Soln 2: Dynamically pick a split point
    - Pick split point with the largest information gain
    - Caveat: Real-valued attributes can be tested many times on a path from
      root to leaf

- Missing attribute values
  - 1. When constructing decision tree
    - Use other instances to estimate missing attribute (e.g. majority)
    - Divide example into fractional examples weighted by frequency of missing
      attribute values
  - 2. When using decision tree
    - Try all possible values of attribute and weight results by prior
      probability of that value; return most probable classification

- Discrete attributes with many values
  - Pick attribute that maximizes GainRatio(A) = Gain(A) / I(|s1|/|s|,..,|sk|/|s|)

- Attributes with costs
  - e.g. blood tests are more costly and invasive than checking temperature
  - Pick attribute that maximizes GainCost(A) = Gain(A)² / Cost(A)
    - Why do we square the gain?
    - I don't know. It's just what people have found that works.

- Multiclass (non-binary) class variables
  - Must change ID3 to check if S contains only examples in each class
  - Also, must generalize information gain calculation to multiple classes:
    - Gain(f) = I(k1/(k1+..+kn),..,kn/(k1+..+kn)) - sum of TODO: will be posted online

- Noise and overfitting
  - Stop growing the tree early (use Chi-squared test)
    - You all remember the Chi-squared test from your stats course so I won't
      elaborate on that
  - Prune the tree early

<!-- Missed 2018 Jul 09 -->

## 18. Neural networks

- Neural Networks
- Just make those DAGs boy make those dags
- Backprop is a thing and is how you train them

<!-- 2018 Jul 11 -->

- Maluuba donated $500k to AI group at UWaterloo
- When you start another Maluuba... don't forget from whence you came

## 19. Backpropagation

- The Great Glorious Backprop Learning Algorithm
  - 1. Initialize weights and thresholds to small random values.
  - 2. Choose an input/output pair from the training set. Assign activation levels to input units.
  - 3. Determine activation levels of hidden units.
  - 4. Determine activation levels of output units.
  - 5. Determine how to adjust weights between hidden and output layer to reduce error.
  - 6. Determine how to adjust weights between input and hidden layer to reduce error.
  - 7. Adjust weights between hidden and output layer.
  - 8. Adjust weights between input and hidden layer.
  - 9. If stopping criteria not met, go to step 2 and repeat.

- Preventing overfitting:
  - Stop early
  - Regularize (penalty for large weights)

- NETtalk:
  - Window 7 characters wide
  - Network learns to pronounce the middle letter
  - Input layer of one-hot letters (a-z, )
  - Hidden layer: 80 units
  - Output layer: 26 units, one for each phoneme
  - Achieves 90% correct in training, 80-87% for new data

- Paint-quality inspection:
  - Reflect laser beam off panel onto projection screen
  - Poor paint job will give a diffuse image
  - Neural network used to look at reflected pattern
  - 900 input units (30x30 pixels), 256 color greyscale
  - 50 hidden units
  - 1 output unit (floating 0.0 to 1.0)
  - 130,000 training patterns
  - Results consistent with human experts

- Vision-based autonomous driving
  - "You can drive with very few neurons"
  - 30x32 video input
  - 9 hidden units
  - 45 output units, from "sharp left" to "sharp right"
  - To train, just put a reliable driver in the car and record! Bam, tons of
    training data
  - If you only need 54 neurons to drive on the freeway, why so many bad
    drivers

- AlexNet
  - For ImageNet
  - Deep convnets
  - Deep, not so wide
  - 650k neurons
  - "It's a lion"

<!-- 2018 Jul 16 -->

- About A4.
  - Problem 3 is neural nets. C code is provided.
  - "you may wish to change this" means you may wish to change this.
  - Very simple feedforward one hidden layer.
  - Input is a flat vector.
  - Output is one-hot.

## 20. Deep neural networks

- Deep neural networks
  - What is deep? Anything with more than one hidden layer.
  - Andrew Ng has good content.
  - deeplearningbook.org is free.

- Activation/threshold functions
  - Sigmoid: f(z) = 1/(1+e^-z)
  - Tanh: f(z) = (e^z - e^-z) / (e^z + e^-z)
  - ReLU: f(z) = max(0, z)
  - Softplus: f(z) = ln(1+e^z)

- Example: Parity function
  - With n inputs, output 1 if odd active, 0 if even active
  - With three layers:
    - Sum of minterms requires O(2^n) hidden nodes
  - With 2n-2 layers:
    - Linear number of hidden nodes

- Support vector machines
  - All the range until 2012, then nets came

- Vanishing gradients
  - is a problem with sigmoid and hyperbolic units, because the slope is never
    steeper than 1
  - Solutions:
    - pre-training
    - ReLU and maxout units

- Overfitting
  - Also a problem
  - Solutions:
    - Regularization
    - Dropout
    - Data augmentation

- TODO: Make good notes for these slides

<!-- 2018 Jul 18 -->

## 21. Evaluating supervised machine learning methods

- Split your labelled data set into a training set and a test set.
- Test set leakage: Test set labels influence the learned model.
- Best conferences: 80% of papers rejected. Cruel business.
- Preprocessing done over the entire data set must not use labels.

- Learning curves
  - Accuracy typically increases about logarithmically in the size of the
    training set
  - A learning curve is a plot of training set size vs. accuracy

- Validation sets
  - You can further break your training set into training and validation sets
  - Use the validation set to select a model, select parameters, and pick when
    to stop learning

- Random resampling
  - When you don't have much data, randomly parititon data set into training
    and test sets

- Cross validation
  - Partition data into k groups; one group is the test set and the rest make
    up the training set
  - 10-fold is common

- Alternatives to accuracy
  - Motivation: In medicine, false negatives can be more costly than false
    positives
  - Dual dichotomy of actual class vs. predicted class results in four total
    counts: TP, FP, FN, TN
  - Recall = TP / (TP + FN)
  - Precision = TP / (TP + FP)

- Evaluating hypotheses / confidence intervals
  - Confidence interval: With approximately N% probability, the true error rate
    lies in the interval r/n ± ZN √((r/n)(1-r/n)/n)
  - Where ZN is 1.64, 1.96, 2.58
  - For example, with a decision tree of error rate 3/14 = 0.214,
  - we get confidence intervals of:
    - 90%: [0.034, 0.394]
    - 95%: [0, 0.429]
    - 99%: [0, 0.497]

- Ensembles
  - You just boost a single classifier up by making several classifiers vote
  - Ensembles are more accurate if errors are uncorrelated, and the error rate
    is less than 1/2

- Constructing ensembles
  - Run learning algorithm multiple times with different subsets of the
    training examples
  - Techniques
    - Bagging:
      - Draw examples randomly with replacement.
      - Take 63.2% of the data on average.
    - Cross-validated committees:
      - Divide original training set into k disjoint subsets.
      - Construct k training sets by dropping out a different one of the k
        subsets.
    - Boosting:
      - Use different probability distributions over the training examples
      - TODO: learn this in more detail
    - Injecting randomness:
      - Neural networks: multiple runs with different initial random weights
      - Decision trees: Randomly choose from among top few features
  - This gives you many classifiers. Do this e.g. 31, 101 times. Odd is good so
    that you have a majority.

- Combining classifiers
  - Unweighted voting
  - Weighted voting: Better classifiers are worth more
  - Stacking: Use a classifier on your classifiers

<!-- 2018 Jul 23 -->

- Steps for supervised ML classification
  - 1. Choose features
  - 2. Collect data
  - 3. Filter features
    - Prune features that don't contribute to the model
    - Helps with efficiency and performance
  - 4. Select classifier
  - 5. Evaluate classifier
    - Cross-validation can be used
    - Bootstrapping can be used
    - Cost of classification errors
      - Weigh FN and FP results differently

- Assignment: `ACCEPT_TRAIN` and `ACCEPT_TEST` don't really need to be played
  with once you've figured out a good value for them

## 22. Reinforcement learning

- Reinforcement learning: Separate from supervised and unsupervised. The
  environment provides hints to the learner.
  - Not as big as unsupervised or supervised, but growing.
  - Learning how to maximize a numerical reward signal.
  - Trade-off between maximizing reward with known techniques vs. exploring
    things that may result in a bigger reward.

- Examples
  - Games: Playing games
  - Operations: Pricing and routing vehicles
  - Control: Elevator scheduling, helicopter control
  - Robotics

- Reinforcement learning
  - A Markov decision process with unknown transition and reward models.
  - Consists of:
    - S: Set of states
    - A: Set of actions; may be stochastic
    - Rewards: Set of reinforcement signals; may be delayed

- Markov decision process
  - Consists of:
    - S: Set of states
    - A: Set of actions
    - P(St | At-1, St-1): Transition model
    - R(St, At-1, St-1): Reward model
    - 0 ≤ γ ≤ 1: Discount factor
    - h: Horizon (# of time steps)
  - Goal: Find optimal policy π:S→A

- Example: Slot machine
  - State: configuration of slots
  - Action: stopping time
  - Reward: $$

- Example: Inverted pendulum
  - State: (x(t), x'(t), θ(t), θ'(t))
  - Action: Force F
  - Reward: +1 for any step where the pole is balanced

- RL characteristics
  - Reinforcements: rewards
  - Temporal credit assignment: what action should be credited with a reward?
  - Exploration/exploitation tradeoff
  - Lifelong learning

- Passive vs. active learning
  - Passive learning: Agent executes fixed policy
  - Active learning: Agent must learn a good policy

- Passive learning
  - Adaptive dynamic programming
  - Temporal difference

- Active learning
  - You uh just improve π guy
  - MDP
  - RL
  - Q-learning

- Explore vs. exploit
  - Gotta balance

- Common exploration methods
  - ε-greedy:
    - With probability ε, execute a random action
    - Otherwise, execute `a* = argmaxa Q(s,a)`
  - Boltzmann exploration: P(a) = `e^(Q(s,a)/T) / sum over a of e^(Q(s,a)/T)

- Success example: TD-Gammon
  - Backgammon player

- Main takeaways:
  - Three types of machine learning
  - Markov Decision Process
  - Learn transition model, reward model

## 23. Natural language for communication

- Many applications for language understanding:
  - summarization
  - computer-aided instruction
  - machine translation
  - sentiment analysis
  - speech understanding
  - interface search languages

- Levels of analysis
  - (Speech recognition)
    - Prosody: Rhythm and intonation
    - Phonology: Basic sounds and how they form words
  - (Natural language understanding)
    - (Surface form)
      - Morphology: Rules for forming words
      - Syntax: Rules for forming phrases and sentences
    - (Meaning)
      - Semantics: Meanings of words and hwo they combine
      - Pragmatics: Meanings in context

- Language is hard because
  - whereas formal languages have no ambiguity and compositional semantics,
  - natural languages are highly ambiguous, with evolving and fluid grammar,
    and non-compositional context-sensitive semantics

- Examples of ambiguity
  - Lexical (word sense)
    - "The man went to the **bank** [to get some cash | and jumped in]."
  - Syntactic
    - "John saw the Rockies flying to Vancouver."
      - (Is John or the Rockies flying?)
    - "He saw her duck."
      - (Is duck a verb or a noun?)
    - "Salespeople sold the dog biscuits."
      - Interesting ones: The dog's name is biscuits, biscuits made of dog
  - Referential ambiguity
    - "I took the cake from the table and ate it."
      - (What did you eat? Try s/ate/cleaned)
  - Intersentencial referential ambiguity
    - "After John proposed to Alex, they found a justice of the peace and got
      married. For the honeymoon, they went to hawaii."
      - (Who went to hawaii? Three people? Two people? Need to know what a
        honeymoon is)
  - Metonymy
    - "Let me have your ear."
  - Metaphor
    - "The inside of a car was a refrigerator."
  - Pragmatic ambiguity
    - "I'll meet you at the coffee shop next Friday."
      - (If today is Thursday, next Friday is 8 days from now. If today is
        Saturday, when is next Friday?)

- Examples of pragmatics
  - Conventions, cultural
    - "Are you sure you don't mind?"
      - "Oh, no." means yes, I don't mind.
      - "Well, yes." means no, I might mind!
      - "Yes." probably means yes
  - Shared background knowledge
    - "Who do you like tonight, Toronto or Montreal?"
      - "Leafs. You?"
        - The plural of leaf is leaves though
  - "NOW OPEN" sign vs. "OPEN" sign
    - "NOW OPEN": Store recently became a store
    - "OPEN": Open for business hours
  - Cooperative answers
    - Q: "Can I switch to the other section of the course?"
    - A (malicious): "YUP!"
    - A (good): Sure, but you should know it's taught by the same guy
    - Q: "How many students failed CS 586 last term?"
    - A (malicious): "None!", or "All of them!"
    - A (good): It wasn't offered last term.
  - Situation, knowledge of knowledge
    - Q. "Can you open the door?"
    - A. Yup.
      - You might need help opening the door
      - You might want to check whether the other person has the keys
  - Goals, beliefs, shared knowledge
    - Q. "Do you know what time it is?"
      - A. Yup. (failed to be cooperative)
      - e.g. teenager comes home at 2am
        - Real question: "Where were you? You safe? You being bad?"
      - AAAH we're late for a party!

- Final exam
  - Study sample solutions to assignments
  - You won't be surprised!
